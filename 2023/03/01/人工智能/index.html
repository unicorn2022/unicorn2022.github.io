<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/30-unicorn.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/30-unicorn.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="人工智能学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="人工智能">
<meta property="og:url" content="http://example.com/2023/03/01/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.html">
<meta property="og:site_name" content="华风夏韵">
<meta property="og:description" content="人工智能学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308101352133.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308103020259.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308103137719.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308103501121.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308104140298.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308104247799.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308105126317.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308105600298.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308105751140.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308110626463.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308111153776.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308111508713.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308111625830.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308112024333.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308112056373.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308112437690.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308113530071.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308114245048.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308114752378.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308121022320.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308121146128.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308115343756.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308115624013.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308115903721.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308120227606.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308120240453.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308120259169.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308120313852.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308120348725.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308120452139.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230308120757276.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315102434738.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315102756553.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315102819660.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315103929751.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315104033233.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315105108127.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315105456694.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315110036244.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315105725408.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315110335591.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315110202244.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315110432875.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315110902668.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315111057252.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315111105731.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315111519449.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315111527551.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315112208471.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315112216700.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315112519863.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315112809443.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315113312157.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315113138273.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315113700564.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315113745900.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114236896.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114453721.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114501042.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114507047.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114515725.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114522281.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114533679.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114651422.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114705326.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315114716032.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230619174409045.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315120043120.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315120320565.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315120803013.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230315121011296.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322100826151.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322101907861.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322102849176.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322103132962.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322103300921.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322104535118.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322104917057.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322105746718.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322105943064.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322110053522.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230619180342828.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322111238257.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322111253530.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322111434761.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322111745307.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230619180431404.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230619180437563.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322112244614.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114448673.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322112850905.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114532149.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114605324.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114641449.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114709158.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114742922.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114332302.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114838614.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322114946852.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322115203632.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322115316924.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322115348387.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322115358630.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322115410222.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230322115632518.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329104355044.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329104913964.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329104945313.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329105537971.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329105621040.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329110247031.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329110443829.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329114630713.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329114704437.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329114740563.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329112909723.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329113152461.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329112837126.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329113104137.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329113409355.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329113443038.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329120546389.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329115050889.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230329115330878.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412100433480.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412101818991.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412102039855.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412102938493.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412104306809.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412104553768.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412104858207.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412105046102.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412105534388.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412110526430.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412110537158.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412110824855.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412112007331.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412112101726.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412114227541.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412114209654.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412114237511.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412114246140.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412114318932.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412114340961.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412115552314.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412120217404.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412120649871.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412120933462.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412121034169.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412121242624.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412121251130.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412121440464.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412121602099.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230412121629302.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419101607549.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419101552222.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419101626670.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419102048725.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419112131702.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419113127137.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419113624996.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419113632079.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419115115486.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419115230356.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419115421635.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419120306729.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230419120942125.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426101927415.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426101941266.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426102742675.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426103032436.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426103044250.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426103537877.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426103752281.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426104845910.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426104854324.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426110545341.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426105838487.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426110335936.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426110349882.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426110256967.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426110404057.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426112028228.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426112208421.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426112602109.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426112910620.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426113059647.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426113104146.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426115207956.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426115150060.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426115238544.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426115528872.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514150442883.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230426121027846.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514151508626.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514151849630.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514152116968.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514153303851.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514194138200.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514194151894.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514194235428.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514153853900.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514154041975.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514154308251.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514191448977.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514191608315.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514194017518.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514194854168.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514195631109.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514195930797.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514200028614.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514200107035.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514200437118.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514200544741.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514214259608.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514220258231.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514220315167.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514221952266.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230514223119712.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515001733288.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515191651335.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515193712715.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515193901099.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515194317721.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515195126133.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515195158097.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515195221719.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515200909879.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515200951193.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515201117406.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515201230336.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230515201257374.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517103207132.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517103826163.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517104754931.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517105910270.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517110515225.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517110654230.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517110917886.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517114027577.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517114407250.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517115250091.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517115306429.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517115337147.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517115738367.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517120713430.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517120730339.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517120756408.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230517120829356.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524103752806.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524104106939.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524104237120.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524104717419.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524111034692.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524111010163.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524110051895.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524110407594.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524111115648.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524111206856.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524111559060.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524111640888.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524111811375.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524112203863.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524112703162.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230619221300001.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524113241486.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524113539570.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524120505284.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524120622673.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121005545.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121431806.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121443343.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121517685.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121527223.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121625616.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121642810.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121731339.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121902690.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524121930690.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230524122000911.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531105937279.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531111906949.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531111834818.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531112006083.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531113435683.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531115940689.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531120014258.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531120224753.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607111149556.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230531121756780.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607104906067.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607110005718.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607105608061.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607105558855.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607105808959.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607110243336.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607110537655.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607111305301.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112146858.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607111451720.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607111528939.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607111755649.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112416470.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112509623.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112551559.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112747539.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112801250.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112837994.png">
<meta property="og:image" content="http://example.com/images/AssetMarkdown/image-20230607112848255.png">
<meta property="article:published_time" content="2023-03-01T02:00:00.000Z">
<meta property="article:modified_time" content="2023-07-13T14:14:01.704Z">
<meta property="article:author" content="华丰夏">
<meta property="article:tag" content="专业课">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/AssetMarkdown/image-20230308101352133.png">

<link rel="canonical" href="http://example.com/2023/03/01/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>人工智能 | 华风夏韵</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband">
      <a target="_blank" rel="noopener" href="https://github.com/unicorn2022" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    </div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">华风夏韵</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/01/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="华丰夏">
      <meta itemprop="description" content="一切都是上天最好的安排">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="华风夏韵">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          人工智能
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-01 10:00:00" itemprop="dateCreated datePublished" datetime="2023-03-01T10:00:00+08:00">2023-03-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-13 22:14:01" itemprop="dateModified" datetime="2023-07-13T22:14:01+08:00">2023-07-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%B8%93%E4%B8%9A%E8%AF%BE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">专业课学习笔记</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>40k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>37 分钟</span>
            </span>
            <div class="post-description">人工智能学习笔记</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<h1 id="一逻辑与推理">一、逻辑与推理</h1>
<ol type="1">
<li><strong>符号主义</strong>人工智能中，所有概念均可通过人类可理解的“符号”及符号之间的关系来表示</li>
<li><strong>符号主义</strong>人工智能方法基于如下假设：
<ol type="1">
<li>可通过逻辑方法来对符号及其关系进行计算，实现逻辑推理，辨析符号所描述内容是否正确</li>
</ol></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230308101352133.png" alt="image-20230308101352133" style="zoom:80%;" /></p>
<h2 id="命题逻辑-proposition-logic">1.1 命题逻辑 proposition logic</h2>
<h3 id="定义">1.1.1 定义</h3>
<ol type="1">
<li><p><strong>命题逻辑</strong>：是应用一套形式化规则对以符号表示的描述性陈述进行推理的系统</p></li>
<li><p><strong>原子命题</strong>：一个或真或假的描述性陈述被称为原子命题</p>
<ol type="1">
<li>对原子命题的内部结构不做任何解析</li>
</ol></li>
<li><p><strong>复合命题(compound
proposition)</strong>：若干原子命题可通过逻辑运算符来构成复合命题</p></li>
<li><p><strong>命题联结词(connectives)</strong>：通过命题联结词对已有命题进行组合，得到新命题</p>
<ol type="1">
<li>通过命题联结词得到的命题被称为复合命题</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230308103020259.png" alt="image-20230308103020259" style="zoom:80%;" /></p></li>
<li><p>通过<strong>真值表</strong>来计算复合命题的真假</p>
<ol type="1">
<li><strong>p=&gt;q</strong>是一个蕴含关系，表示<strong>p∈q</strong></li>
<li>如果<strong>p=False</strong>，则<strong>p=&gt;q</strong>恒为<strong>True</strong></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230308103137719.png" alt="image-20230308103137719" style="zoom:80%;" /></p></li>
</ol>
<h3 id="逻辑等价">1.1.2 逻辑等价</h3>
<ol type="1">
<li><strong>逻辑等价：</strong>给定命题p和命题q，如果p和q在所有情况下都具有同样真假结果，那么p和q在逻辑上等价，即<strong>p≡q</strong></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230308103501121.png" alt="image-20230308103501121" style="zoom:80%;" /></p>
<h3 id="推理规则">1.1.3 推理规则</h3>
<p><img src="/images/AssetMarkdown/image-20230308104140298.png" alt="image-20230308104140298" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308104247799.png" alt="image-20230308104247799" style="zoom:80%;" /></p>
<blockquote>
<p>示例：</p>
<p><img src="/images/AssetMarkdown/image-20230308105126317.png" alt="image-20230308105126317" style="zoom:80%;" /></p>
</blockquote>
<h3 id="命题范式">1.1.4 命题范式</h3>
<p><strong>范式（normal
form)</strong>：把命题公式化归为一种标准的形式</p>
<ol type="1">
<li><p>范式最大的作用是可以进行两个命题的等价判定</p></li>
<li><p><strong>析取范式</strong>：有限个简单合取式构成的析取式</p></li>
<li><p><strong>合取范式</strong>：有限个简单析取式构成的合取式</p>
<p><img src="/images/AssetMarkdown/image-20230308105600298.png" alt="image-20230308105600298" style="zoom:80%;" /></p></li>
<li><p>一个析取范式是不成立的，当且仅当：它的每个简单合取式都不成立</p></li>
<li><p>一个合取范式是成立的，当且仅当：它的每个简单析取式都是成立的</p></li>
<li><p>任一命题公式都存在着与之等值的析取范式与合取范式</p>
<ol type="1">
<li>注意：命题公式的析取范式与合取范式都不是唯一的</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230308105751140.png" alt="image-20230308105751140" style="zoom:80%;" /></p></li>
</ol>
<h2 id="谓词逻辑">1.2 谓词逻辑</h2>
<h3 id="个体与谓词">1.2.1 个体与谓词</h3>
<ol type="1">
<li><p><strong>个体</strong>：个体是指所研究领域中可以独立存在的具体或抽象的概念</p></li>
<li><p><strong>谓词</strong>：谓词是用来刻画个体属性或者描述个体之间关系存在性的元素，其值为真或为假</p>
<ol type="1">
<li>包含一个参数的谓词称为<strong>一元谓词</strong>，表示一元关系</li>
<li>包含多个参数的谓词称为<strong>多元谓词</strong>，表示个体之间的多元关系</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230308110626463.png" alt="image-20230308110626463" style="zoom:80%;" /></p></li>
<li><p>函数与谓词的区别：</p>
<p><img src="/images/AssetMarkdown/image-20230308111153776.png" alt="image-20230308111153776" style="zoom:80%;" /></p></li>
</ol>
<h3 id="量词">1.2.2 量词</h3>
<ol type="1">
<li><strong>全称量词</strong>(universal quantifier,
<strong>∀</strong>)：<strong>∀xP(x)</strong>表示定义域中的所有个体具有性质P</li>
<li><strong>存在量词</strong>(existential quantifier,
<strong>∃</strong>)：<strong>∃xP(x)</strong>表示定义域中存在一个个体或若干个体具有性质P</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230308111508713.png" alt="image-20230308111508713" style="zoom:80%;" /></p>
<h3 id="变元">1.2.3 变元</h3>
<ol type="1">
<li><p><strong>约束变元</strong>：在全称量词或存在量词的约束条件下的变量符号称为约束变元</p></li>
<li><p><strong>自由变元</strong>：不受全称量词或存在量词约束的变量符号称为自由变元</p>
<p><img src="/images/AssetMarkdown/image-20230308111625830.png" alt="image-20230308111625830" style="zoom:80%;" /></p></li>
<li><p>在约束变元相同的情况下，量词的运算满足<strong>分配律</strong></p>
<p><img src="/images/AssetMarkdown/image-20230308112024333.png" alt="image-20230308112024333" style="zoom:80%;" /></p></li>
<li><p>当公式中存在多个量词时，若多个量词都是全称量词或者都是存在量词，则量词的位置可以互换；若多个量词中既有全称量词又有存在量词，则量词的位置不可以随意互换</p>
<p><img src="/images/AssetMarkdown/image-20230308112056373.png" alt="image-20230308112056373" style="zoom:80%;" /></p></li>
</ol>
<h3 id="项与原子谓词公式">1.2.4 项与原子谓词公式</h3>
<ol type="1">
<li>项：项是描述对象的逻辑表达式，被递归地定义为：
<ol type="1">
<li>常量符号和变量符号是项；</li>
<li>若<span class="math inline">\(f(x_1,x_2,⋯,x_n)\)</span>是<span
class="math inline">\(n\)</span>元函数符号，<span
class="math inline">\(t_1,t_2,⋯,t_n\)</span>是项，则<span
class="math inline">\(f(t_1,t_2,⋯,t_n)\)</span>是项；</li>
<li>有限次数地使用上述规则产生的符号串是项。</li>
</ol></li>
<li>原子谓词公式：
<ol type="1">
<li>若<span class="math inline">\(P(x_1,x_2,⋯,x_n)\)</span>是<span
class="math inline">\(n\)</span>元谓词，<span
class="math inline">\(t_1,t_2,⋯,t_n\)</span>是项，则称<span
class="math inline">\(P(t_1,t_2,⋯,t_n)\)</span>是原子谓词公式，简称原子公式</li>
</ol></li>
</ol>
<h3 id="合式公式">1.2.5 合式公式</h3>
<p>合式公式是由<strong>逻辑联结词</strong>和<strong>原子公式</strong>构成的用于陈述事实的复杂语句，又称<strong>谓词公式</strong>，由以下规则定义：</p>
<ol type="1">
<li>命题常项、命题变项、原子谓词公式是合式公式</li>
<li>如果A是合式公式，则¬A也是合式公式</li>
<li>如果A和B是合式公式，则A∧B、A∨B、A→B 、B→A、A⟷B 都是合式公式</li>
<li>如果A是合式公式，x是个体变项，则(∃x)A(x) 和(∀x)A(x)也是合式公式</li>
<li>有限次数地使用上述规则构成的表达式是合式公式</li>
</ol>
<h3 id="推理规则-1">1.2.6 推理规则</h3>
<p><img src="/images/AssetMarkdown/image-20230308112437690.png" alt="image-20230308112437690" style="zoom:80%;" /></p>
<h3 id="专家系统的构成">1.2.7 专家系统的构成</h3>
<p><img src="/images/AssetMarkdown/image-20230308113530071.png" alt="image-20230308113530071" style="zoom:80%;" /></p>
<h2 id="知识图谱">1.3 知识图谱</h2>
<h3 id="基本概念">1.3.1 基本概念</h3>
<p><img src="/images/AssetMarkdown/image-20230308114245048.png" alt="image-20230308114245048" style="zoom: 80%;" /></p>
<ol type="1">
<li><strong>知识图谱</strong>可视为包含多种关系的图
<ol type="1">
<li>在图中，每个<strong>节点</strong>是一个实体（如人名、地名、事件和活动等）</li>
<li>任意两个节点之间的<strong>边</strong>表示这两个节点之间存在的关系</li>
<li>一般而言，可将知识图谱中<strong>任意两个相连节点及其连接边</strong>表示成一个三元组（triplet）,
即 <strong>(left_node, relation, right_node)</strong></li>
</ol></li>
<li>知识图谱中存在连线的两个实体可表达为形如<strong>&lt;left_node,
relation, right_node &gt;</strong>的三元组形式
<ol type="1">
<li>这种三元组也可以表示为一阶逻辑<strong>(first order logic,
FOL)</strong>的形式，从而为基于知识图谱的推理创造了条件</li>
<li>例如从&lt;奥巴马，出生地，夏威夷&gt;和&lt;夏威夷，属于，美国&gt;两个三元组，可推理得到&lt;奥巴马，国籍，美国&gt;</li>
</ol></li>
</ol>
<h3 id="知识图谱推理">1.3.2 知识图谱推理</h3>
<p><img src="/images/AssetMarkdown/image-20230308114752378.png" alt="image-20230308114752378" style="zoom:80%;" /></p>
<ol type="1">
<li>可利用<strong>一阶谓词</strong>来表达刻画知识图谱中节点之间存在的关系
<ol type="1">
<li>如图中形如&lt;James,Couple,David&gt;的关系可用一阶逻辑的形式来描述，即Couple(James,
David)</li>
<li>Couple(x,
y)是一阶谓词，Couple是图中实体之间具有的关系，x和y是谓词变量</li>
<li>从图中已有关系可推知David和Ann具有父女关系，但这一关系在图中初始图(无红线)中并不存在，是需要推理的目标</li>
</ol></li>
</ol>
<h3 id="归纳学习">1.3.3 归纳学习</h3>
<ol type="1">
<li><strong>归纳逻辑程序设计ILP</strong>(Inductive Logic
Programming)：是机器学习和逻辑程序设计交叉领域的研究内容</li>
<li>ILP使用一阶谓词逻辑进行知识表示，通过修改和扩充逻辑表达式对现有知识归纳，完成推理任务</li>
<li>作为ILP的代表性方法，<strong>FOIL</strong>(First Order Inductive
Learner)通过<strong>序贯覆盖</strong>实现规则推理</li>
</ol>
<h3 id="一阶推导学习-foil">1.3.4 一阶推导学习 FOIL</h3>
<blockquote>
<p><strong>FOIL</strong>：First Order Inductive Learner</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230308121022320.png" alt="image-20230308121022320" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308121146128.png" alt="image-20230308121146128" style="zoom:80%;" /></p>
<ol type="1">
<li><p><strong>目标谓词：</strong><code>Father(x, y)</code></p></li>
<li><p><strong>正例</strong>：目标谓词只有一个正例<code>Father(David, Mike)</code>。</p></li>
<li><p><strong>反例</strong>：在知识图谱中一般不会显式给出，但可从知识图谱中构造出来</p>
<ol type="1">
<li>如从知识图谱中已经知道<code>Couple(David, James)</code>成立，则<code>Father(David, James)</code>可作为目标谓词P的一个反例，记为<code>¬Father (David, James)</code>。</li>
<li>只能在已知两个实体的关系且确定其关系与目标谓词相悖时，才能将这两个实体用于构建目标谓词的反例</li>
<li>而不能在不知两个实体是否满足目标谓词前提下将它们来构造目标谓词的反例</li>
</ol></li>
<li><p><strong>背景知识</strong>：知识图谱中目标谓词以外的其他谓词实例化结果，如<code>Sibling(Ann, Mike)</code></p></li>
<li><p><strong>推理思路：</strong>从一般到特殊，逐步给目标谓词添加前提约束谓词，直到所构成的推理规则<strong>不覆盖任何反例</strong></p>
<ol type="1">
<li><strong>从一般到特殊</strong>：对目标谓词或前提约束谓词中的变量赋予具体值</li>
<li>如将<code>(∀x)(∀y)(∀z)(Mother(z, y)∧Couple(x,z) → Father(x, y))</code>这一推理规则所包含的目标谓词<code>Father(x, y)</code>中<code>x</code>和<code>y</code>分别赋值为<code>David</code>和<code>Ann</code>，进而进行推理</li>
</ol></li>
<li><p>如何选择约束谓词：<strong>信息增益值最大</strong></p>
<ol type="1">
<li>要求添加该谓词后，可以覆盖的正例更多，负例更少</li>
<li>直到只覆盖正例，不覆盖负例</li>
</ol></li>
<li><p><strong>信息增益值：描述了添加某个谓词后，<span
class="math inline">\(\frac{正例}{正例+反例}\)</span>的比例变化</strong>
<span class="math display">\[
FOIL\_Gain=\hat{m_+}(\log_2\frac{\hat{m_+}}{\hat{m_+}+\hat{m_-}}-\log_2\frac{m_+}{m_++m_-})
\]</span></p>
<p><img src="/images/AssetMarkdown/image-20230308115343756.png" alt="image-20230308115343756" style="zoom:80%;" /></p></li>
</ol>
<blockquote>
<p>示例：</p>
<p><img src="/images/AssetMarkdown/image-20230308115624013.png" alt="image-20230308115624013" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308115903721.png" alt="image-20230308115903721" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308120227606.png" alt="image-20230308120227606" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308120240453.png" alt="image-20230308120240453" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308120259169.png" alt="image-20230308120259169" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308120313852.png" alt="image-20230308120313852" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308120348725.png" alt="image-20230308120348725" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308120452139.png" alt="image-20230308120452139" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230308120757276.png" alt="image-20230308120757276" style="zoom:80%;" /></p>
</blockquote>
<h3 id="路径排序推理-pra">1.3.5 路径排序推理 PRA</h3>
<blockquote>
<p><strong>Path Ranking Algorithm</strong></p>
</blockquote>
<ol type="1">
<li><strong>基本思想：将实体之间的关联路径作为特征，来学习目标关系的分类器</strong></li>
<li>工作流程主要分为三步：
<ol type="1">
<li><strong>特征抽取</strong>：生成并选择路径特征集合
<ol type="1">
<li>生成路径的方式有：随机游走、广度优先搜索、深度优先搜索等</li>
</ol></li>
<li><strong>特征计算</strong>：计算每个训练样例的特征值<span
class="math inline">\(𝑃(𝑠→𝑡;𝜋_𝑗)\)</span>
<ol type="1">
<li>该特征值可以是：从实体节点<span
class="math inline">\(𝑠\)</span>出发，通过关系路径<span
class="math inline">\(𝜋_𝑗\)</span>到达实体节点<span
class="math inline">\(𝑡\)</span>的概率；</li>
<li>也可以是：<strong>布尔值，表示实体<span
class="math inline">\(𝑠\)</span>到实体<span
class="math inline">\(𝑡\)</span>之间是否存在路径<span
class="math inline">\(𝜋_𝑗\)</span>；</strong></li>
<li>还可以是：实体<span class="math inline">\(𝑠\)</span>和实体<span
class="math inline">\(𝑡\)</span>之间路径出现频次、频率等</li>
</ol></li>
<li><strong>分类器训练</strong>：根据训练样例的特征值，为目标关系训练分类器
<ol type="1">
<li>当训练好分类器后，即可将该分类器用于推理两个实体之间是否存在目标关系</li>
</ol></li>
</ol></li>
</ol>
<blockquote>
<p>特征向量的含义：布尔值，表示实体<span
class="math inline">\(𝑠\)</span>到实体<span
class="math inline">\(𝑡\)</span>之间是否存在路径<span
class="math inline">\(𝜋_𝑗\)</span>；</p>
<ol type="1">
<li>[Couple→Mother，Father→Mother<sup>-1</sup>，Mother→Sibling，Couple→Father]</li>
</ol>
<p>如(David,
Ann)：只可以通过Couple→Mother这条路径链接，因此特征向量是[1, 0, 0,
0]</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230315102434738.png" alt="image-20230315102434738" style="zoom:80%;" /></p>
<h3 id="基于分布式的知识推理">1.3.6 基于分布式的知识推理</h3>
<p><img src="/images/AssetMarkdown/image-20230315102756553.png" alt="image-20230315102756553" style="zoom:80%;" /></p>
<h3 id="马尔可夫逻辑网络">1.3.7 马尔可夫逻辑网络</h3>
<p><img src="/images/AssetMarkdown/image-20230315102819660.png" alt="image-20230315102819660" style="zoom:80%;" /></p>
<h2 id="不考1.4-因果推理">(不考)1.4 因果推理</h2>
<ol type="1">
<li><p>传统以统计建模为核心的推理手段：AI学习联合分布的概率</p>
<p><img src="/images/AssetMarkdown/image-20230315103929751.png" alt="image-20230315103929751" style="zoom:80%;" /></p></li>
<li><p>因果推理：改变控制变量的取值后，会导致结果如何变化</p>
<p><img src="/images/AssetMarkdown/image-20230315104033233.png" alt="image-20230315104033233" style="zoom:80%;" /></p></li>
</ol>
<h3 id="不考1.4.1-三种因果推理">(不考)1.4.1 三种因果推理</h3>
<p><img src="/images/AssetMarkdown/image-20230315105108127.png" alt="image-20230315105108127" style="zoom:80%;" /></p>
<h3 id="不考1.4.2-因果推理的主要模型">(不考)1.4.2
因果推理的主要模型</h3>
<p><img src="/images/AssetMarkdown/image-20230315105456694.png" alt="image-20230315105456694" style="zoom:80%;" /></p>
<h3 id="不考1.4.3-结构因果模型">(不考)1.4.3 结构因果模型</h3>
<p><img src="/images/AssetMarkdown/image-20230315110036244.png" alt="image-20230315110036244" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315105725408.png" alt="image-20230315105725408" style="zoom:80%;" /></p>
<h2 id="不考1.5-因果图模型">(不考)1.5 因果图模型</h2>
<p><img src="/images/AssetMarkdown/image-20230315110335591.png" alt="image-20230315110335591" style="zoom:80%;" /></p>
<h3 id="不考1.5.1-联合概率分布">(不考)1.5.1 联合概率分布</h3>
<p><img src="/images/AssetMarkdown/image-20230315110202244.png" alt="image-20230315110202244" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315110432875.png" alt="image-20230315110432875" style="zoom:80%;" /></p>
<h3 id="不考1.5.2-链">(不考)1.5.2 链</h3>
<blockquote>
<p><strong>中间节点<span
class="math inline">\(Z\)</span>一旦给定，两端的节点<span
class="math inline">\(X,Y\)</span>条件独立</strong></p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230315110902668.png" alt="image-20230315110902668" style="zoom:80%;" /></p>
<h3 id="不考1.5.3-分连">(不考)1.5.3 分连</h3>
<blockquote>
<p><strong>中间节点<span
class="math inline">\(Z\)</span>一旦给定，两端的节点<span
class="math inline">\(X,Y\)</span>条件独立</strong></p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230315111057252.png" alt="image-20230315111057252" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315111105731.png" alt="image-20230315111105731" style="zoom:80%;" /></p>
<h3 id="不考1.5.4-汇连">(不考)1.5.4 汇连</h3>
<blockquote>
<p><strong>中间节点<span
class="math inline">\(Z\)</span>给定时，两端的节点<span
class="math inline">\(X,Y\)</span>条件相关</strong></p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230315111519449.png" alt="image-20230315111519449" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315111527551.png" alt="image-20230315111527551" style="zoom:80%;" /></p>
<h3 id="不考1.5.5-d-分离">(不考)1.5.5 D-分离</h3>
<p><img src="/images/AssetMarkdown/image-20230315112208471.png" alt="image-20230315112208471" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315112216700.png" alt="image-20230315112216700" style="zoom:80%;" /></p>
<h3 id="不考1.5.6-干预的因果效应">(不考)1.5.6 干预的因果效应</h3>
<p><img src="/images/AssetMarkdown/image-20230315112519863.png" alt="image-20230315112519863" style="zoom:80%;" /></p>
<h3 id="不考1.5.7-因果效应差">(不考)1.5.7 因果效应差</h3>
<blockquote>
<p>因果效应差越大，表示该变量对结果的因果效应越大</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230315112809443.png" alt="image-20230315112809443" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315113312157.png" alt="image-20230315113312157" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315113138273.png" alt="image-20230315113138273" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315113700564.png" alt="image-20230315113700564" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315113745900.png" alt="image-20230315113745900" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315114236896.png" alt="image-20230315114236896" style="zoom:80%;" /></p>
<h3 id="不考1.5.8-反事实模型">(不考)1.5.8 反事实模型</h3>
<p><img src="/images/AssetMarkdown/image-20230315114453721.png" alt="image-20230315114453721" style="zoom:80%;" /></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230315114501042.png" alt="image-20230315114501042" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315114507047.png" alt="image-20230315114507047" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315114515725.png" alt="image-20230315114515725" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315114522281.png" alt="image-20230315114522281" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315114533679.png" alt="image-20230315114533679" style="zoom:80%;" /></p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230315114651422.png" alt="image-20230315114651422" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315114705326.png" alt="image-20230315114705326" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230315114716032.png" alt="image-20230315114716032" style="zoom:80%;" /></p>
<h1 id="二搜索与求解">二、搜索与求解</h1>
<h2 id="搜索算法基础">2.1 搜索算法基础</h2>
<h3 id="搜索算法的形式化描述">2.1.1 搜索算法的形式化描述</h3>
<p><img src="/images/AssetMarkdown/image-20230619174409045.png" alt="image-20230619174409045" style="zoom:80%;" /></p>
<ol type="1">
<li><strong>状态</strong>：对智能体和环境当前情形的描述
<ol type="1">
<li>例如，在最短路径问题中，城市可作为状态</li>
<li>将原问题城市对应的状态称为初始状态</li>
</ol></li>
<li><strong>动作</strong>：从当前时刻所处状态转移到下一时刻所处状态所进行操作
<ol type="1">
<li>一般而言这些操作都是离散的</li>
</ol></li>
<li><strong>状态转移</strong>：智能体选择了一个动作之后，其所处状态的相应变化</li>
<li><strong>路径/代价</strong>：一个状态序列。该状态序列被一系列操作所连接
<ol type="1">
<li>如从A到K所形成的路径。</li>
</ol></li>
<li><strong>目标测试</strong>：评估当前状态是否为所求解的目标状态</li>
</ol>
<h3 id="评价指标">2.1.2 评价指标</h3>
<p><img src="/images/AssetMarkdown/image-20230315120043120.png" alt="image-20230315120043120" style="zoom:80%;" /></p>
<h3 id="树搜索">2.1.3 树搜索</h3>
<p><img src="/images/AssetMarkdown/image-20230315120320565.png" alt="image-20230315120320565" style="zoom:80%;" /></p>
<h3 id="剪枝搜索">2.1.4 剪枝搜索</h3>
<ol type="1">
<li><p>主动放弃一些后继节点，可以提高搜索效率，而不会影响最终的搜索效果</p>
<ol type="1">
<li>如删除已访问节点</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230315120803013.png" alt="image-20230315120803013" style="zoom:80%;" /></p></li>
</ol>
<h2 id="启发式搜索">2.2 启发式搜索</h2>
<h3 id="贪婪最佳优先搜索-greedy-best-first-search">2.2.1
贪婪最佳优先搜索 Greedy best-first search</h3>
<p><strong>评价函数<code>f(n)</code> =
启发函数<code>h(n)</code></strong></p>
<ol type="1">
<li><code>f(n)</code>：评价函数，判定下一个节点是谁</li>
<li><code>h(n)</code>：启发函数，预估完成任务的最小代价</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230315121011296.png" alt="image-20230315121011296" style="zoom:80%;" /></p>
<h3 id="a算法">2.2.2 A*算法</h3>
<h4 id="a算法的定义">2.2.2.1 A*算法的定义</h4>
<p><strong>评价函数<code>f(n) = g(n) + h(n)</code></strong></p>
<ol type="1">
<li><code>f(n)</code>：评价函数，判定下一个节点是谁</li>
<li><code>h(n)</code>：启发函数，预估完成任务的最小代价</li>
<li><code>g(n)</code>：从起始节点到当前节点的代价</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230322100826151.png" alt="image-20230322100826151" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322101907861.png" alt="image-20230322101907861" style="zoom:80%;" /></p>
<h4 id="不考2.2.2.2-a算法的性能分析">(不考)2.2.2.2 A*算法的性能分析</h4>
<ol type="1">
<li><p>A*算法的完备性、最优性，取决于搜索问题和启发函数的性质</p>
<ol type="1">
<li>完备性：一定能找到解</li>
<li>最优性：解最优</li>
</ol></li>
<li><p>相关定义</p>
<ol type="1">
<li><code>f(n)</code>：评价函数，判定下一个节点是谁</li>
<li><code>h(n)</code>：启发函数，预估完成任务的最小代价</li>
<li><code>g(n)</code>：从起始节点到当前节点的代价</li>
<li><code>c(n,a,n')</code>：从节点n，执行动作a，到达节点n'的单步代价</li>
<li><code>h*(n)</code>：从节点n到终点的实际最小代价</li>
</ol></li>
<li><p>启发函数<code>h(n)</code>需要满足的性质：</p>
<ol type="1">
<li><p><strong>可容性admissible</strong>：<code>h(n) ≤ h*(n)</code>，预估代价不超过实际代价</p></li>
<li><p><strong>一致性consistency</strong>：<code>h(n) ≤ c(n,a,n') + h(n')</code>，从当前节点直接到达终点的代价，不超过到达下一节点，然后再到达终点的代价</p></li>
<li><p><strong>满足一致性，一定满足可容性</strong>：</p>
<p><img src="/images/AssetMarkdown/image-20230322102849176.png" alt="image-20230322102849176" style="zoom:80%;" /></p></li>
</ol></li>
</ol>
<h4 id="不考2.2.2.3-a算法的完备性">(不考)2.2.2.3 A*算法的完备性</h4>
<ol type="1">
<li><strong>完备性</strong>：如果在起始节点和终止节点之间有路径存在，那么一定可以得到解。得不到解一定说明没有解存在</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230322103132962.png" alt="image-20230322103132962" style="zoom:80%;" /></p>
<h4 id="不考2.2.2.4-a算法的最优性">(不考)2.2.2.4 A*算法的最优性</h4>
<ol type="1">
<li>树搜索算法：如果启发函数满足<strong>可容性</strong>，则A*算法满足<strong>最优性</strong></li>
<li>图搜索算法：如果启发函数满足<strong>一致性</strong>，则A*算法满足<strong>最优性</strong></li>
<li>对于任意一个状态<strong>t</strong>，它第一次被加入搜索树时的路径必然是最短路径</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230322103300921.png" alt="image-20230322103300921" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322104535118.png" alt="image-20230322104535118" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322104917057.png" alt="image-20230322104917057" style="zoom:80%;" /></p>
<h2 id="对抗搜索博弈搜索">2.3 对抗搜索/博弈搜索</h2>
<blockquote>
<p>Adversarial / Game Search</p>
</blockquote>
<ol type="1">
<li><p>在一个竞争的环境中，智能体(agents)之间通过竞争实现相反的利益，一方<strong>最大化</strong>这个利益，另外一方<strong>最小化</strong>这个利益</p>
<p><img src="/images/AssetMarkdown/image-20230322105746718.png" alt="image-20230322105746718" style="zoom:80%;" /></p></li>
<li><p>分为三种搜索策略</p>
<ol type="1">
<li><strong>最小最大搜索(Minimax
Search)</strong>：最小最大搜索是在对抗搜索中最为基本的一种让玩家来计算最优策略的方法</li>
<li><strong>Alpha-Beta剪枝搜索(Pruning
Search)</strong>：一种对最小最大搜索进行改进的算法，即在搜索过程中可剪除无需搜索的分支节点，且不影响搜索结果。</li>
<li><strong>蒙特卡洛树搜索(Monte-Carlo Tree
Search)</strong>：通过采样而非穷举方法来实现搜索</li>
</ol></li>
<li><p>本书的讨论范围：</p>
<ol type="1">
<li>确定的、全局可观察的、竞争对手轮流行动、零和游戏</li>
</ol></li>
<li><p>形式化描述：</p>
<p><img src="/images/AssetMarkdown/image-20230322105943064.png" alt="image-20230322105943064" style="zoom:80%;" /></p></li>
</ol>
<h3 id="最小最大搜索-minimax">2.3.1 最小最大搜索 Minimax</h3>
<blockquote>
<ol type="1">
<li>当前收益：<code>minimax(s)</code></li>
<li>选择动作<code>a</code>后的可能收益：<code>minimax(result(s,a))</code></li>
</ol>
</blockquote>
<ol type="1">
<li>如果为终止状态：返回当前状态的得分<code>utility(s)</code></li>
<li>如果当前玩家为MAX：选择可能收益最大的动作，返回执行该动作后的可能收益</li>
<li>如果当前玩家为MIN：选择可能收益最小的动作，返回执行该动作后的可能收益</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230322110053522.png" alt="image-20230322110053522" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230619180342828.png" alt="image-20230619180342828" style="zoom:80%;" /></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 玩家MAX行动下, 当前的最优动作ans</span></span><br><span class="line"><span class="function">Action <span class="title">MinimaxDecision</span><span class="params">(State s)</span></span>&#123;</span><br><span class="line">    Action ans = null;</span><br><span class="line">    Value maxValue = -INF;</span><br><span class="line">    <span class="keyword">for</span>(a in <span class="built_in">Action</span>(s))&#123;</span><br><span class="line">        State nextState = <span class="built_in">result</span>(s,a);</span><br><span class="line">        Value nextValue = <span class="built_in">MinValue</span>(nextState);</span><br><span class="line">        <span class="keyword">if</span>(maxValue &lt; nextValue)&#123;</span><br><span class="line">            ans = a;</span><br><span class="line">            maxValue = nexValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 玩家MAX行动下, 当前状态的得分 v = minimax(s, MAX)</span></span><br><span class="line"><span class="function">Value <span class="title">MaxValue</span><span class="params">(State s)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(s == terminal_state) <span class="keyword">return</span> <span class="built_in">utility</span>(s);</span><br><span class="line">    Value v = -INF;</span><br><span class="line">    <span class="keyword">for</span>(a in <span class="built_in">Action</span>(s))&#123;</span><br><span class="line">        State nextState = <span class="built_in">result</span>(s,a);</span><br><span class="line">        Value nextValue = <span class="built_in">MinValue</span>(nextState);</span><br><span class="line">        v = <span class="built_in">max</span>(v, nextValue);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> v;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 玩家MIN行动下, 当前状态的得分 v = minimax(s, MINs)</span></span><br><span class="line"><span class="function">Value <span class="title">MinValue</span><span class="params">(State s)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(s == terminal_state) <span class="keyword">return</span> <span class="built_in">utility</span>(s);</span><br><span class="line">    Value v = INF;</span><br><span class="line">    <span class="keyword">for</span>(a in <span class="built_in">Action</span>(s))&#123;</span><br><span class="line">        State nextState = <span class="built_in">result</span>(s,a);</span><br><span class="line">        Value nextValue = <span class="built_in">MaxValue</span>(nextState);</span><br><span class="line">        v = <span class="built_in">min</span>(v, nextValue);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> v;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="alpha-beta剪枝搜索">2.3.2 Alpha-Beta剪枝搜索</h3>
<p><img src="/images/AssetMarkdown/image-20230322111238257.png" alt="image-20230322111238257" style="zoom:80%;" /></p>
<blockquote>
<p>MIN节点<span class="math inline">\(m\)</span>：alpha剪枝</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322111253530.png" alt="image-20230322111253530" style="zoom:80%;" /></p>
<blockquote>
<p>MAX节点<span class="math inline">\(m\)</span>：beta剪枝</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322111434761.png" alt="image-20230322111434761" style="zoom: 80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322111745307.png" alt="image-20230322111745307" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230619180431404.png" alt="image-20230619180431404" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230619180437563.png" alt="image-20230619180437563" style="zoom:80%;" /></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 玩家MAX行动下, 当前的最优动作ans</span></span><br><span class="line">&#123;Value,Action&#125; <span class="built_in">AlphaBetaDecision</span>(State s)&#123;</span><br><span class="line">    Action ans;</span><br><span class="line">    Value v;</span><br><span class="line">    &#123;v, ans&#125; = <span class="built_in">MaxValue</span>(s, -INF, INF);</span><br><span class="line">    <span class="keyword">return</span> &#123;v, ans&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 玩家MIN行动下, 当前状态的得分 v = minimax(s, MIN)和最优动作 ans</span></span><br><span class="line">&#123;Value,Action&#125; <span class="built_in">MuValue</span>(State s, Value α, Value β)&#123;</span><br><span class="line">    <span class="keyword">if</span>(s == terminal_state) <span class="keyword">return</span> &#123;<span class="built_in">utility</span>(s), null&#125;;</span><br><span class="line">    Value v = INF;</span><br><span class="line">    Action ans = null;</span><br><span class="line">    <span class="keyword">for</span>(a in <span class="built_in">Action</span>(s))&#123;</span><br><span class="line">        State nextState = <span class="built_in">result</span>(s,a);</span><br><span class="line">        &#123;vv, aa&#125; = <span class="built_in">MaxValue</span>(nextState, α, β);</span><br><span class="line">       	<span class="keyword">if</span>(vv &lt; v)&#123;</span><br><span class="line">            v = vv; </span><br><span class="line">            ans = aa;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        β = <span class="built_in">min</span>(β, v);</span><br><span class="line">        <span class="keyword">if</span>(α &gt; β) <span class="keyword">return</span> &#123;v, ans&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>算法性能：</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322112244614.png" alt="image-20230322112244614" style="zoom:80%;" /></p>
<h2 id="蒙特卡洛树搜索">2.4 蒙特卡洛树搜索</h2>
<h3 id="问题定义">2.4.1 问题定义</h3>
<p><img src="/images/AssetMarkdown/image-20230322114448673.png" alt="image-20230322114448673" style="zoom:80%;" /></p>
<h3 id="相关概念">2.4.2 相关概念</h3>
<blockquote>
<p>核心：降低悔值函数的取值</p>
<p>悔值函数：期望T次操作的得分 - 实际T次操作的得分</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322112850905.png" alt="image-20230322112850905" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322114532149.png" alt="image-20230322114532149" style="zoom:80%;" /></p>
<h3 id="贪心算法策略">2.4.3 贪心算法策略</h3>
<blockquote>
<p>在第t步，选择过去t-1步，平均的得分最高的赌博机</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322114605324.png" alt="image-20230322114605324" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322114641449.png" alt="image-20230322114641449" style="zoom:80%;" /></p>
<h3 id="ε-贪心算法">2.4.4 ε-贪心算法</h3>
<blockquote>
<p>在第t步，1-ε的概率取平均分最高，ε的概率取一个随机的赌博机</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322114709158.png" alt="image-20230322114709158" style="zoom:80%;" /></p>
<h3 id="上限置信区间算法ucd1upper-confidence-bounds">2.4.5
上限置信区间算法UCD1：Upper Confidence Bounds</h3>
<blockquote>
<p>估计每一个动作的奖励区间，优先选取上限高的动作</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322114742922.png" alt="image-20230322114742922" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322114332302.png" alt="image-20230322114332302" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322114838614.png" alt="image-20230322114838614" style="zoom:80%;" /></p>
<h3 id="对抗搜索蒙特卡洛树搜索">2.4.6 对抗搜索：蒙特卡洛树搜索</h3>
<blockquote>
<p>使用UCB1算法，预估下一步操作的收益</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322114946852.png" alt="image-20230322114946852" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322115203632.png" alt="image-20230322115203632" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322115316924.png" alt="image-20230322115316924" style="zoom:80%;" /></p>
<blockquote>
<p>示例</p>
<p><img src="/images/AssetMarkdown/image-20230322115348387.png" alt="image-20230322115348387" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322115358630.png" alt="image-20230322115358630" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230322115410222.png" alt="image-20230322115410222" style="zoom:80%;" /></p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230322115632518.png" alt="image-20230322115632518" style="zoom:80%;" /></p>
<h1 id="三监督学习">三、监督学习</h1>
<h2 id="机器学习的基本概念">3.1 机器学习的基本概念</h2>
<ol type="1">
<li>从原始数据中提取<strong>特征</strong></li>
<li>学习<strong>映射函数𝑓</strong></li>
<li>通过映射函数𝑓将<strong>原始数据</strong>映射到<strong>语义任务空间</strong>，即寻找数据和任务目标之间的关系</li>
</ol>
<h3 id="机器学习的分类">3.1.1 机器学习的分类</h3>
<ol type="1">
<li><strong>监督学习</strong>：数据有标签，一般为回归/分类等任务</li>
<li><strong>无监督学习</strong>：数据无标签，一般为聚类/若干降维任务
<ol type="1">
<li>同一类物体之间总有一定的相似性</li>
</ol></li>
<li><strong>半监督学习</strong>：有的数据有标签，有的数据无标签</li>
<li><strong>强化学习</strong>：序列数据决策学习，一般为从环境交互中学习</li>
</ol>
<h3 id="监督学习的重要元素">3.1.2 监督学习的重要元素</h3>
<ol type="1">
<li><strong>标注数据</strong>：标识了类别信息的数据，学什么</li>
<li><strong>学习模型</strong>：如何学习到映射模型，如何学</li>
<li><strong>损失函数</strong>：如何对学习结果进行度量，是否学到</li>
</ol>
<blockquote>
<p>没有免费午餐定理NFL：任何机器学习模型在<strong>所有问题</strong>上的<strong>性能都是相同</strong>的，其总误差和模型本身是没有关系的。一种算法（算法A）在特定数据集上的表现优于另一种算法（算法B）的同时，一定伴随着算法A在另外某一个特定的数据集上有着不如算法B的表现</p>
</blockquote>
<h3 id="监督学习">3.1.3 监督学习</h3>
<h4 id="损失函数">3.1.3.1 损失函数</h4>
<ol type="1">
<li>设共有n个标注数据，第i个标记数据为<span
class="math inline">\((x_i,y_i)\)</span>，其中<span
class="math inline">\(x_i\)</span>为样本数据，<span
class="math inline">\(y_i\)</span>是标注信息</li>
<li>设学习到的映射函数为<span class="math inline">\(f\)</span>，对<span
class="math inline">\(x_i\)</span>的预测结果为<span
class="math inline">\(f(x_i)\)</span></li>
<li>损失函数即为计算<span class="math inline">\(y_i\)</span>和<span
class="math inline">\(f(x_i)\)</span>之间的差值的函数</li>
<li>训练的目标是：在训练数据集上得到的损失之和最小，即<span
class="math inline">\(min\sum_{i=1}^nLoss(f(x_i),y_i)\)</span></li>
<li>典型损失函数：
<ol type="1">
<li><strong>0-1损失函数</strong>：<span
class="math inline">\(Loss(y_i,f(x_i))=\{1, f(x_i)\ne y_i;\ \
0,f(x_i)=y_i\}\)</span></li>
<li><strong>平方损失函数</strong>：<span
class="math inline">\(Loss(y_i,f(x_i))=(y_i-f(x_i))^2\)</span></li>
<li><strong>绝对损失函数</strong>：<span
class="math inline">\(Loss(y_i,f(x_i))=|y_i-f(x_i)|\)</span></li>
<li><strong>对数损失函数/对数似然损失函数</strong>：<span
class="math inline">\(Loss(y_i,P(y_i|x_i))=-\log
P(y_i|x_i)\)</span></li>
</ol></li>
</ol>
<h4 id="训练数据与测试数据">3.1.3.2 训练数据与测试数据</h4>
<ol type="1">
<li><strong>训练精度</strong>：在<strong>测试数据集</strong>上的精度，因为在训练集上一定是100%通过的</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230329104355044.png" alt="image-20230329104355044" style="zoom:80%;" /></p>
<h4 id="经验风险期望风险">3.1.3.3 经验风险、期望风险</h4>
<p><strong>经验风险 empirical
risk</strong>：<strong>训练集</strong>中数据产生的损失</p>
<ol type="1">
<li><p>经验风险越小说明学习模型对训练数据拟合程度越好</p></li>
<li><p>经验风险最小化<strong>ERM</strong></p>
<p><img src="/images/AssetMarkdown/image-20230329104913964.png" alt="image-20230329104913964" style="zoom:80%;" /></p></li>
</ol>
<p><strong>期望风险 expected
risk</strong>：当测试集中存在<strong>无穷多数据</strong>时产生的损失</p>
<ol type="1">
<li><p>期望风险越小，学习所得模型越好</p></li>
<li><p>期望风险最小化<strong>ERM</strong></p>
<p><img src="/images/AssetMarkdown/image-20230329104945313.png" alt="image-20230329104945313" style="zoom:80%;" /></p></li>
</ol>
<h4 id="过学习-欠学习">3.1.3.4 过学习 &amp; 欠学习</h4>
<p><img src="/images/AssetMarkdown/image-20230329105537971.png" alt="image-20230329105537971" style="zoom:80%;" /></p>
<h4 id="结构风险最小">3.1.3.5 结构风险最小</h4>
<p><img src="/images/AssetMarkdown/image-20230329105621040.png" alt="image-20230329105621040" style="zoom:80%;" /></p>
<h4 id="判别模型-生成模型">3.1.3.6 判别模型 &amp; 生成模型</h4>
<ol type="1">
<li><p><strong>判别模型</strong>：</p>
<ol type="1">
<li>建立输入到输出的映射函数：<span
class="math inline">\(f(输入)=人脸\)</span></li>
<li>判断输入属于输出空间的概率有多大：<span
class="math inline">\(P(人脸|输入)=0.99\)</span></li>
</ol></li>
<li><p><strong>生成模型</strong>：</p>
<ol type="1">
<li><p>学习联合概率分布<span
class="math inline">\(P(X,Y)\)</span>（通过似然概率<span
class="math inline">\(P(X|Y)\)</span>和类概率<span
class="math inline">\(P(Y)\)</span>的乘积来求取）</p>
<p><img src="/images/AssetMarkdown/image-20230329110247031.png" alt="image-20230329110247031" style="zoom: 80%;" /></p></li>
<li><p>典型方法：贝叶斯方法、隐马尔科夫链</p></li>
<li><p>似然概率：计算导致样本<span
class="math inline">\(X\)</span>出现的模型参数值</p>
<p><img src="/images/AssetMarkdown/image-20230329110443829.png" alt="image-20230329110443829" style="zoom:80%;" /></p></li>
<li><p>联合概率分布<span
class="math inline">\(P(X,Y)\)</span>、似然概率<span
class="math inline">\(P(X|Y)\)</span>求取很困难</p></li>
</ol></li>
</ol>
<h2 id="回归分析">3.2 回归分析</h2>
<h3 id="线性回归">3.2.1 线性回归</h3>
<ol type="1">
<li><strong>回归分析</strong>：分析不同变量之间存在的关系</li>
<li><strong>回归模型</strong>：刻画不同变量之间关系的模型</li>
<li><strong>线性回归模型</strong>：回归模型是线性的</li>
</ol>
<h4 id="一元线性回归">3.2.1.1 一元线性回归</h4>
<ol type="1">
<li><p>一个变量，对另一个变量的影响</p></li>
<li><p>回归模型：<span
class="math inline">\(f(x_i)=ax_i+b\)</span></p></li>
<li><p>要求：<span class="math inline">\(\frac{1}{N}\sum(y-\hat
y)^2\)</span>最小</p></li>
<li><p>计算结果： <span class="math display">\[
b=\overline{y}-a\overline{x}\\
a=\frac{\sum_{i=1}^{n}x_iy_i-n\overline{x}·\overline{y}}{\sum_{i=1}^{n}x_i^2-n\overline{x}^2}
\]</span></p></li>
</ol>
<blockquote>
<p>计算过程：</p>
<p><img src="/images/AssetMarkdown/image-20230329114630713.png" alt="image-20230329114630713" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230329114704437.png" alt="image-20230329114704437" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230329114740563.png" alt="image-20230329114740563" style="zoom:80%;" /></p>
</blockquote>
<h4 id="多元线性回归">3.2.1.2 多元线性回归</h4>
<ol type="1">
<li>多个变量，对一个变量产生影响</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230329112909723.png" alt="image-20230329112909723" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230329113152461.png" alt="image-20230329113152461" style="zoom:80%;" /></p>
<h4 id="logistics-回归对数几率回归">3.2.1.3 logistics
回归/对数几率回归</h4>
<p><img src="/images/AssetMarkdown/image-20230329112837126.png" alt="image-20230329112837126" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230329113104137.png" alt="image-20230329113104137" style="zoom:80%;" /></p>
<h4 id="二分类问题">3.2.1.4 二分类问题</h4>
<p><img src="/images/AssetMarkdown/image-20230329113409355.png" alt="image-20230329113409355" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230329113443038.png" alt="image-20230329113443038" style="zoom:80%;" /></p>
<h4 id="基于似然函数的参数优化">3.2.1.5 基于似然函数的参数优化</h4>
<p><img src="/images/AssetMarkdown/image-20230329120546389.png" alt="image-20230329120546389" style="zoom:80%;" /></p>
<blockquote>
<p>使用梯度下降公式，快速到达导数接近0的位置，出口为：</p>
<ol type="1">
<li>迭代次数到达预定次数</li>
<li>相邻两次差值小于某个值</li>
</ol>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230329115050889.png" alt="image-20230329115050889" style="zoom:80%;" /></p>
<h4 id="mle最大似然估计-map最大后验概率估计">3.2.1.6 MLE最大似然估计
&amp; MAP最大后验概率估计</h4>
<p><img src="/images/AssetMarkdown/image-20230329115330878.png" alt="image-20230329115330878" style="zoom:80%;" /></p>
<h2 id="决策树">3.3 决策树</h2>
<p>决策树是一种通过树形结构来进行分类的方法。</p>
<ol type="1">
<li>在决策树中，树形结构中每个非叶子节点表示对分类目标在某个属性上的一个判断，每个分支代表基于该属性做出的一个判断，最后树形结构中每个叶子节点代表一种分类结果</li>
<li>所以决策树可以看作是一系列以叶子节点为输出的决策规则</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230412100433480.png" alt="image-20230412100433480" style="zoom:80%;" /></p>
</blockquote>
<h3 id="信息熵-ed">3.3.1 信息熵 E(D)</h3>
<blockquote>
<p>构建决策树时<strong>划分属性的顺序</strong>选择是重要的。</p>
<p>性能好的决策树随着划分不断进行，决策树分支结点样本集的“纯度”会越来越高，即其所包含样本尽可能属于相同类别，即<strong>E(D)尽可能小</strong></p>
</blockquote>
<ol type="1">
<li><p>假设有<span
class="math inline">\(K\)</span>个信息，组成了集合样本<span
class="math inline">\(D\)</span>，第<span
class="math inline">\(k\)</span>个信息发生的概览为<span
class="math inline">\(p_k(1\le k\le K)\)</span>，则这<span
class="math inline">\(K\)</span>个信息的<strong>信息熵</strong>为：
<span class="math display">\[
E(D)=-\sum_{k=1}^Kp_k\log_2\ p_k
\]</span></p>
<ol type="1">
<li>其中，<span class="math inline">\(\sum_{k=1}^{K}p_k=1\)</span></li>
</ol></li>
<li><p><span class="math inline">\(E(D)\)</span>值越小，表示<span
class="math inline">\(D\)</span>包含的信息越确定，即<span
class="math inline">\(D\)</span>的纯度越高</p>
<ol type="1">
<li>当<span class="math inline">\(p_k=1\)</span>时，<span
class="math inline">\(E(D)=0\)</span>，信息熵为0，即包含的信息量很小</li>
</ol></li>
</ol>
<blockquote>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">年龄属性取值 a<sub>i</sub></th>
<th style="text-align: center;">“&gt;30”</th>
<th style="text-align: center;">“20~30”</th>
<th style="text-align: center;">“&lt;20”</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">对应样本数 |D<sub>i</sub>|</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td style="text-align: center;">正负样本数量</td>
<td style="text-align: center;">(2+, 3-)</td>
<td style="text-align: center;">(4+，0-)</td>
<td style="text-align: center;">(3+，2-)</td>
</tr>
</tbody>
</table>
<p>计算E(D)：</p>
<p><img src="/images/AssetMarkdown/image-20230412101818991.png" alt="image-20230412101818991" style="zoom:80%;" /></p>
</blockquote>
<h3 id="信息增益-gainda">3.3.2 信息增益 Gain(D,A)</h3>
<ol type="1">
<li><p>得到信息熵后，可进一步计算使用某个特定属性<strong>A</strong>对原样本集<strong>D</strong>进行划分后的<strong>信息增益</strong>，计算公式为：
<span class="math display">\[
Gain(D,A)=E(D)-\sum_{i=1}^{n}\frac{|D_i|}{|D|}E(D_i)
\]</span></p>
<ol type="1">
<li>其中，属性<span class="math inline">\(A\)</span>将<span
class="math inline">\(D\)</span>划分为了<span
class="math inline">\(n\)</span>类，分别为<span
class="math inline">\(D_1,...D_n\)</span></li>
</ol></li>
<li><p>取<strong>信息增益最大</strong>的属性，作为决策树中靠近根节点的属性</p></li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230412102039855.png" alt="image-20230412102039855" style="zoom:80%;" /></p>
</blockquote>
<h3 id="信息熵和物理熵的区别">3.3.3 信息熵和物理熵的区别</h3>
<ol type="1">
<li>信息熵：描述信息的价值，某个信息发生概率越小，带来的信息量越大，信息熵就越大
<ol type="1">
<li>香农用信息熵的概念来描述信源的不确定度</li>
</ol></li>
<li>物理熵：表示分子状态混乱程度的物理量
<ol type="1">
<li>熵越大，越失去次序，不确定性越大</li>
</ol></li>
</ol>
<h2 id="线性区别分析-lda">3.4 线性区别分析 LDA</h2>
<blockquote>
<p><strong>Linear discriminant analysis</strong></p>
</blockquote>
<p>线性判别分析是一种基于监督学习的<strong>降维方法</strong>，也称为Fisher线性判别分析</p>
<ol type="1">
<li>对于一组具有标签信息的高维数据样本，LDA利用其类别信息，将其线性投影到一个低维空间</li>
<li>在低维空间中同一类别样本尽可能靠近，不同类别样本尽可能彼此远离，不要有交叉的部分</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230412102938493.png" alt="image-20230412102938493" style="zoom:80%;" /></p>
</blockquote>
<h3 id="符号定义">3.4.1 符号定义</h3>
<ol type="1">
<li>设样本集为：<span
class="math inline">\(D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)</span>
<ol type="1">
<li><span class="math inline">\(x_i\in
R^d\)</span>表示样本，对应的标签为<span
class="math inline">\(y_i\)</span></li>
<li><span class="math inline">\(y_i \in
\{C_1,C_2,...,C_k\}\)</span>，表示一共有<span
class="math inline">\(K\)</span>类样本</li>
</ol></li>
<li><span class="math inline">\(X\)</span>：所有样本构成的集合</li>
<li><span class="math inline">\(N_i\)</span>：第<span
class="math inline">\(i\)</span>个类别包含的样本个数</li>
<li><span class="math inline">\(X_i\)</span>：第<span
class="math inline">\(i\)</span>类样本的集合</li>
<li><span class="math inline">\(m\)</span>：所有样本的均值向量</li>
<li><span class="math inline">\(m_i\)</span>：第<span
class="math inline">\(i\)</span>类样本的均值向量</li>
<li><span class="math inline">\(\Sigma_i\)</span>：第<span
class="math inline">\(i\)</span>类样本的协方差矩阵，<span
class="math inline">\(\Sigma_i=\sum_{x\in
X_i}(x-m_i)(x-m_i)^T\)</span></li>
</ol>
<h3 id="二分类问题-1">3.4.2 二分类问题</h3>
<p><span
class="math inline">\(K=2\)</span>时，表示二分类问题，其样本的标签为<span
class="math inline">\(\{C_1,C_2\}\)</span>，并通过线性函数<span
class="math inline">\(y(x)=w^Tx\ \ \ (w\in
R^n)\)</span>投影到一维空间</p>
<ol type="1">
<li><span
class="math inline">\(w\)</span>：系数<strong>矩阵</strong>，需要通过数据学习</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230412104306809.png" alt="image-20230412104306809" style="zoom:80%;" /></p>
</blockquote>
<p>计算系数矩阵<span class="math inline">\(w\)</span>的方法： <span
class="math display">\[
w=S_w^{-1}(m_2-m_1)=(\Sigma_1+\Sigma_2)^{-1}(m_2-m_1)
\]</span>
<img src="/images/AssetMarkdown/image-20230412104553768.png" alt="image-20230412104553768" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412104858207.png" alt="image-20230412104858207" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412105046102.png" alt="image-20230412105046102" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412105534388.png" alt="image-20230412105534388" style="zoom:80%;" /></p>
<p>投影过程中，保持方差结构不变：即原来两类包含的是某些样本，投影完之后依旧分别包含这些样本</p>
<h3 id="多分类问题">3.4.3 多分类问题</h3>
<p><img src="/images/AssetMarkdown/image-20230412110526430.png" alt="image-20230412110526430" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412110537158.png" alt="image-20230412110537158" style="zoom:80%;" /></p>
<h3 id="线性判别分析的降维步骤">3.4.4 线性判别分析的降维步骤</h3>
<ol type="1">
<li>计算数据样本集中，每个类别样本的均值<span
class="math inline">\(m_i\)</span></li>
<li>计算类内散度矩阵<span
class="math inline">\(S_w\)</span>、类间散度矩阵<span
class="math inline">\(S_b\)</span></li>
<li>根据<span class="math inline">\(S_w^{-1}S_bW=\lambda
W\)</span>，求解<span
class="math inline">\(S_w^{-1}S_b\)</span>对应的前<span
class="math inline">\(r\)</span>个最大特征根对应的特征向量<span
class="math inline">\((w_1,w_2,...,w_r)\)</span>，构成投影矩阵<span
class="math inline">\(W\)</span></li>
<li>通过矩阵<span
class="math inline">\(W\)</span>将每个样本映射到低维空间，实现特征降维</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230412110824855.png" alt="image-20230412110824855" style="zoom:80%;" /></p>
<h2 id="ada-boosting">3.5 Ada Boosting</h2>
<blockquote>
<p>adaptive boosting，自适应提升</p>
</blockquote>
<ol type="1">
<li>对于一个复杂的分类任务，可以将其分解为若干子任务，然后将若干子任务完成方法综合，最终完成该复杂任务</li>
<li>将若干个弱分类器(weak classifiers)组合起来，形成一个强分类器(strong
classifier)</li>
</ol>
<h3 id="计算学习理论霍夫丁不等式">3.5.1 计算学习理论：霍夫丁不等式</h3>
<ol type="1">
<li>学习任务：统计某个电视节目在全国的收视率</li>
<li>方法：不可能去统计整个国家中每个人是否观看电视节目、进而算出收视率。只能抽样一部分人口，然后将<strong>抽样人口</strong>中观看该电视节目的比例作为该电视节目的全国收视率</li>
<li><strong>霍夫丁不等式</strong>：全国人口中看该电视节目的人口比例<span
class="math inline">\(x\)</span>与抽样人口中观看该电视节目的人口比例<span
class="math inline">\(y\)</span>满足关系：<span
class="math inline">\(P(|x-y|\ge \epsilon) \le
2e^{-2N\epsilon^2}\)</span>
<ol type="1">
<li><span class="math inline">\(N\)</span>：采样人口总数</li>
<li><span class="math inline">\(\epsilon
\in(0,1)\)</span>：设定的可容忍误差范围</li>
<li>当<span
class="math inline">\(N\)</span>足够大时，“全国人口中电视节目收视率”与“样本人口中电视节目收视率”差值超过误差范围<span
class="math inline">\(\epsilon\)</span>的概率非常小</li>
</ol></li>
</ol>
<h3 id="计算学习理论概率近似正确-pac">3.5.2 计算学习理论：概率近似正确
PAC</h3>
<blockquote>
<p><strong>probably approximately correct</strong></p>
</blockquote>
<ol type="1">
<li>对于统计电视节目收视率这样的任务，可以通过不同的采样方法（即不同模型）来计算收视率。每个模型会产生不同的误差</li>
<li>任务：得到完成该任务的<strong>若干“弱模型”</strong>，将这些弱模型组合起来<strong>形成一个“强模型”</strong></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230412112007331.png" alt="image-20230412112007331" style="zoom:80%;" /></p>
<h3 id="ada-boosting思路描述">3.5.3 Ada Boosting：思路描述</h3>
<p><img src="/images/AssetMarkdown/image-20230412112101726.png" alt="image-20230412112101726" style="zoom:80%;" /></p>
<p>两个核心问题：</p>
<ol type="1">
<li>在每个<strong>弱分类器学习</strong>过程中：<strong>改变训练数据的权重</strong>
<ol type="1">
<li><strong>提高</strong>在上一轮中分类<strong>错误样本的权重</strong></li>
<li>即关注上一个弱分类器无法解决的结果，进行优化</li>
</ol></li>
<li>如何将一系列弱分类器<strong>组合成强分类器</strong>：<strong>加权多数表决方法</strong>
<ol type="1">
<li>提高分类误差小的弱分类器的权重，让其在最终分类中起到更大作用</li>
<li>减少分类误差大的弱分类器的权重，让其在最终分类中仅起到较小作用</li>
</ol></li>
</ol>
<h3 id="ada-boosting算法描述">3.5.4 Ada Boosting：算法描述</h3>
<h4 id="数据样本权重初始化">3.5.4.1 数据样本权重初始化</h4>
<ol type="1">
<li>给定包含<span
class="math inline">\(N\)</span>个标注数据的训练集合<span
class="math inline">\(\Gamma\)</span>
<ol type="1">
<li><span
class="math inline">\(\Gamma=\{(x_1,y_1),...,(x_N,y_N)\}\)</span>，其中<span
class="math inline">\(x_i\in X\sub R^n, y_i\in Y=\{-1,1\}\)</span></li>
</ol></li>
<li>初始化每个训练样本的权重：
<ol type="1">
<li><span class="math inline">\(D_1=(w_{11},...,w_{1N}),\
w_{1i}=\frac{1}{N}\)</span></li>
</ol></li>
</ol>
<h4 id="第m个弱分类器的训练">3.5.4.2 第m个弱分类器的训练</h4>
<p>对于<span class="math inline">\(m=1,2,...M\)</span></p>
<ol type="1">
<li>使用具有分布权重<span
class="math inline">\(D_m\)</span>的训练数据，学习得到第m个弱分类器<span
class="math inline">\(G_m\)</span>
<ol type="1">
<li><span class="math inline">\(G_m(x): X \rightarrow
\{-1,1\}\)</span></li>
</ol></li>
<li>计算<span
class="math inline">\(G_m(x)\)</span>在训练数据集上的分类误差：
<ol type="1">
<li><span class="math inline">\(err_m=\sum_{i=1}^Nw_{mi}\ I(G_m(x_i)\neq
y_i)\)</span></li>
<li>其中，<span class="math inline">\(I(G_m(x_i)\neq
y_i)\)</span>表示：当<span class="math inline">\(G_m(x_i)\neq
y_i\)</span>时为1，否则为0</li>
</ol></li>
<li>计算<span
class="math inline">\(G_m(x)\)</span>的权重：<strong>弱分类器的权重和不为1</strong>
<ol type="1">
<li><span class="math inline">\(\alpha_m=\frac{1}{2}\ln
\frac{1-err_m}{err_m}\)</span></li>
</ol></li>
<li>更新训练样本数据的分布权重：<strong>每一轮样本的权重和为1</strong>
<ol type="1">
<li><span
class="math inline">\(D_{m+1}=w_{m+1,i}=\frac{w_{m,i}}{Z_m}e^{-\alpha_my_iG_m(x_i)}\)</span></li>
<li>其中，<span
class="math inline">\(Z_m\)</span>是归一化因子，使得<span
class="math inline">\(D_{m+1}\)</span>为概率分布</li>
<li><span
class="math inline">\(Z_m=\sum_{i=1}^Nw_{m,i}e^{-\alpha_my_iG_m(x_i)}\)</span></li>
</ol></li>
</ol>
<h4 id="弱分类器组合成强分类器">3.5.4.3 弱分类器组合成强分类器</h4>
<ol type="1">
<li>以线性加权形式来组合弱分类器<span
class="math inline">\(f(x)\)</span>：<span
class="math inline">\(f(x)=\sum_{i=1}^M\alpha_mG_m(x)\)</span></li>
<li>得到强分类器<span class="math inline">\(G(x)\)</span>：<span
class="math inline">\(G(x)=sign(f(x))=sign(\sum_{i=1}^M\alpha_mG_m(x))\)</span></li>
</ol>
<h3 id="ada-boosting算法解释">3.5.5 Ada Boosting：算法解释</h3>
<p><img src="/images/AssetMarkdown/image-20230412114227541.png" alt="image-20230412114227541" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412114209654.png" alt="image-20230412114209654" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412114237511.png" alt="image-20230412114237511" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412114246140.png" alt="image-20230412114246140" style="zoom:80%;" /></p>
<h3 id="ada-boosting回看霍夫丁不等式">3.5.6 Ada
Boosting：回看霍夫丁不等式</h3>
<p><img src="/images/AssetMarkdown/image-20230412114318932.png" alt="image-20230412114318932" style="zoom:80%;" /></p>
<h3 id="ada-boosting优化目标">3.5.7 Ada Boosting：优化目标</h3>
<p><img src="/images/AssetMarkdown/image-20230412114340961.png" alt="image-20230412114340961" style="zoom:80%;" /></p>
<h3 id="回归和分类的区别">3.5.8 回归和分类的区别</h3>
<p><img src="/images/AssetMarkdown/image-20230412115552314.png" alt="image-20230412115552314" style="zoom:80%;" /></p>
<h2 id="不考3.6-支持向量机">(不考)3.6 支持向量机</h2>
<h3 id="不考3.6.1-vc维与结构风险最小化">(不考)3.6.1
VC维与结构风险最小化</h3>
<p><img src="/images/AssetMarkdown/image-20230412120217404.png" alt="image-20230412120217404" style="zoom:80%;" /></p>
<blockquote>
<p>h：反应机器复杂程度的VC维，要求机器本身不要太复杂</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230412120649871.png" alt="image-20230412120649871" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412120933462.png" alt="image-20230412120933462" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412121034169.png" alt="image-20230412121034169" style="zoom:80%;" /></p>
<h3 id="不考3.6.2-线性可分支持向量机">(不考)3.6.2
线性可分支持向量机</h3>
<p><img src="/images/AssetMarkdown/image-20230412121242624.png" alt="image-20230412121242624" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412121251130.png" alt="image-20230412121251130" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412121440464.png" alt="image-20230412121440464" style="zoom:80%;" /></p>
<h3 id="不考3.6.3-松弛变量软间隔与hinge损失函数">(不考)3.6.3
松弛变量，软间隔与hinge损失函数</h3>
<p><img src="/images/AssetMarkdown/image-20230412121602099.png" alt="image-20230412121602099" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230412121629302.png" alt="image-20230412121629302" style="zoom:80%;" /></p>
<h2 id="不考3.7-生成学习模型">(不考)3.7 生成学习模型</h2>
<p>生成学习方法从数据中学习联合概率分布<span
class="math inline">\(P(X,C)\)</span>，然后求出条件概率分布<span
class="math inline">\(P(C|X)\)</span>作为预测模型，即<span
class="math inline">\(P(c_i|x)=\frac{P(x,c_i)}{P(x)}\)</span></p>
<ol type="1">
<li><span class="math inline">\(P(x,c_i)=P(x|c_i)×P(c_i)\)</span>
<ol type="1">
<li><span class="math inline">\(P(x|c_i)\)</span>：似然概率</li>
<li><span class="math inline">\(P(c_i)\)</span>：先验概率</li>
</ol></li>
<li><span
class="math inline">\(P(c_i|x)=\frac{P(x,c_i)}{P(x)}=\frac{P(x|c_i)×P(c_i)}{P(x)}\)</span>
<ol type="1">
<li><span
class="math inline">\(P(c_i|x)\)</span>：后验概率，表示样本<span
class="math inline">\(x\)</span>属于类别<span
class="math inline">\(c_i\)</span>的概率</li>
<li><span class="math inline">\(P(x,c_i)\)</span>：联合概率</li>
</ol></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230419101607549.png" alt="image-20230419101607549" style="zoom:80%;" /></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230419101552222.png" alt="image-20230419101552222" style="zoom:80%;" /></p>
<p>生成式学习：</p>
<ol type="1">
<li>学习某个输入与某个类别同时出现的概率，根据某个输入出现的概率，计算某个输入对应某个类别的概率</li>
<li>模拟了数据生成的方法</li>
<li>但是联合概率难以学习，因为训练数据中输入与类别没有同时出现，不代表以后永远不会同时出现</li>
<li>以购买商品为例：
<ol type="1">
<li>联合概率<span
class="math inline">\(P(x,c_i)\)</span>：某个人购买了某个商品的概率</li>
<li>似然概率<span
class="math inline">\(P(x|c_i)\)</span>：某个商品让某个人购买的概率，某个人没有买，但是与之类似的人买过，此时可以转换为某个商品让某个人购买的概率有多大</li>
<li>先验概率<span
class="math inline">\(P(c_i)\)</span>：某个商品出现的概率，即在所有卖出的商品中，该商品出现的概率</li>
</ol></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230419101626670.png" alt="image-20230419101626670" style="zoom:80%;" /></p>
<p>判别式学习：直接学习某个输入对应某个类别的概率</p>
<p><img src="/images/AssetMarkdown/image-20230419102048725.png" alt="image-20230419102048725" style="zoom:80%;" /></p>
</blockquote>
<h1 id="四无监督学习">四、无监督学习</h1>
<p>无监督学习从<strong>非标注样本</strong>出发来<strong>学习数据的分布</strong>，这是一个异常困难的工作。由于无法利用标注信息，因此在无监督学习只能利用假设数据具有某些结构来进行学习。正如拉普拉斯所言“概率论只不过是把常识用数学公式表达了出来”，无监督学习就是把预设数据具有某种结构作为一种“知识”来指导模型的学习。</p>
<h2 id="k均值聚类">4.1 K均值聚类</h2>
<p>输入：n个数据，无标注信息</p>
<p>输出：k个聚类结果</p>
<p>目的：将n个数据聚类到k个集合(类簇)</p>
<h3 id="算法描述">4.1.1 算法描述</h3>
<p>定义：</p>
<ol type="1">
<li>n个m维数据<span class="math inline">\(\{x_1,x_2,...x_n\}, x_i\in
R^m\)</span></li>
<li>两个m维数据之间的欧氏距离为：<span
class="math inline">\(d(x_i,x_j)=\sqrt{(x_{i1}-x_{j1})^2+(x_{i2}-x_{j2})^2+...+(x_{im}-x_{jm})^2}\)</span>
<ol type="1">
<li>欧氏距离并不一定能够刻画语义上的相似性</li>
<li>但是总要找一个函数表示数据之间的相似性，这里就以欧氏距离为例</li>
</ol></li>
<li>聚类集合数目：<span class="math inline">\(k\)</span></li>
</ol>
<h4 id="初始化聚类质心">4.1.1.1 初始化聚类质心</h4>
<ol type="1">
<li>初始化<span class="math inline">\(k\)</span>个聚类质心<span
class="math inline">\(c=\{c_1, c_2,...,c_k\}\)</span>，<span
class="math inline">\(c_j\in R^m,1\le j \le k\)</span></li>
<li>每个聚类质心<span
class="math inline">\(c_j\)</span>所在的集合记为<span
class="math inline">\(G_j\)</span></li>
</ol>
<h4 id="将每个待聚类数据放入唯一一个聚类集合中">4.1.1.2
将每个待聚类数据放入唯一一个聚类集合中</h4>
<ol type="1">
<li>计算待聚类数据<span class="math inline">\(x_i\)</span>和质心<span
class="math inline">\(c_j\)</span>之间的欧氏距离<span
class="math inline">\(d(x_i,c_j)\)</span>，<span
class="math inline">\(1\le i\le n, 1\le j \le k\)</span></li>
<li>将每个<span
class="math inline">\(x_i\)</span>放入与之距离最近的聚类质心所在聚类集合中，即</li>
</ol>
<p><span class="math display">\[
argmin_{c_{j\in C}}d(x_i,c_j)
\]</span></p>
<h4 id="根据聚类结果更新聚类质心">4.1.1.3
根据聚类结果，更新聚类质心</h4>
<ol type="1">
<li>聚类<span class="math inline">\(G_j\)</span>的新的聚类质心<span
class="math inline">\(c_j\)</span>为：</li>
</ol>
<p><span class="math display">\[
c_j=\frac{1}{|G_j|}\sum_{x_i\in G_j}x_i
\]</span></p>
<h4 id="算法循环迭代直到满足条件">4.1.1.4
算法循环迭代，直到满足条件</h4>
<ol type="1">
<li>在新的聚类质心基础上，根据欧氏距离的大小，将每个待聚类数据放入唯一一个聚类集合中</li>
<li>根据聚类结果，更新聚类质心</li>
<li>重复上述两个步骤，直到满足一下任意一个条件：
<ol type="1">
<li>前后两次迭代中，聚类质心基本保持不变
<ol type="1">
<li>此时算法收敛，结果相对来说比较好</li>
</ol></li>
<li>迭代次数达到上限
<ol type="1">
<li>此时算法不收敛，可能是由于欧氏距离并不能很好的刻画数据的相似度</li>
</ol></li>
</ol></li>
</ol>
<h3 id="另一个视角最小化每个类簇的方差">4.1.2
另一个视角：最小化每个类簇的方差</h3>
<p>方差：计算变量(观察值)与样本平均值之间的差异 <span
class="math display">\[
argmin_{G}\sum_{i=1}^k\sum_{x \in
G_i}|x-G_i|^2=argmin_{G}\sum_{i=1}^k|G_i|Var\ G_i
\]</span> 第<span class="math inline">\(i\)</span>个类簇的方差： <span
class="math display">\[
var(G_i)=\frac{1}{|G_i|}\sum_{x \in G_i}|x-G_i|^2
\]</span></p>
<ol type="1">
<li>欧氏距离与方差量纲相同</li>
<li>最小化每个类簇方差将使得最终聚类结果中每个聚类集合中所包含数据呈现出来差异性最小</li>
</ol>
<h3 id="k均值聚类算法的不足">4.1.3 K均值聚类算法的不足</h3>
<ol type="1">
<li>需要事先确定聚类数目，很多时候我们并不知道数据应被聚类的数目</li>
<li>需要初始化聚类质心，初始化聚类中心对聚类结果有较大的影响</li>
<li>算法是迭代执行，时间开销非常大</li>
<li>欧氏距离假设数据每个维度之间的重要性是一样的</li>
</ol>
<h2 id="主成分分析-pca">4.2 主成分分析 PCA</h2>
<blockquote>
<p><strong>Principle Component Analysis</strong></p>
</blockquote>
<p>主成分分析是一种<strong>特征降维</strong>方法（与线性区别分析的目的是一样的）</p>
<ol type="1">
<li>要尽可能将投影后的数据打散，即方差最大</li>
<li>设原数据为<span
class="math inline">\(X_{n×d}\)</span>，投影后的数据为<span
class="math inline">\(Y_{n×l}\)</span>，投影矩阵为<span
class="math inline">\(W_{d×l}\)</span>，则有<span
class="math inline">\(Y=XW\)</span></li>
<li>投影矩阵<span
class="math inline">\(W_{d×l}\)</span>为：协方差矩阵<span
class="math inline">\(\Sigma\)</span>前<span
class="math inline">\(l\)</span>个最大的特征根对应的特征向量<span
class="math inline">\(_{d×1}\)</span>组成的矩阵<span
class="math inline">\(_{d×l}\)</span></li>
</ol>
<h3 id="相关概念-1">4.2.1 相关概念</h3>
<p>假设有n个数据，记为<span class="math inline">\(X=\{x_i\},
i=1,...n\)</span></p>
<ol type="1">
<li><strong>方差</strong>：描述样本数据的波动程度 <span
class="math display">\[
Var(X)=\frac{1}{n}\sum_{i=1}^n(x_i-u)^2\\
u=\frac{1}{n}\sum_{i=1}^n x_i
\]</span></li>
</ol>
<p>假设有n个二维变量数据，记为<span
class="math inline">\((X,Y)=\{(x_i,y_i)\}, i=1,...n\)</span></p>
<ol type="1">
<li><p><strong>协方差</strong>：衡量两个维度之间的相关度</p>
<ol type="1">
<li>协方差<span
class="math inline">\(cov(X,Y)&gt;0\)</span>时：正相关</li>
<li>协方差<span
class="math inline">\(cov(X,Y)&lt;0\)</span>时：负相关</li>
<li>协方差<span
class="math inline">\(cov(X,Y)=0\)</span>时：线性意义下不相关</li>
</ol>
<p><span class="math display">\[
cov(X,Y)=\frac{1}{n}\sum_{i=1}^n(x_i-E(X))(y_i-E(Y))\\
E(X)=\frac{1}{n}\sum_{i=1}^nx_i,\ \ E(Y)=\frac{1}{n}\sum_{i=1}^ny_i
\]</span></p></li>
<li><p><strong>皮尔逊相关系数</strong>：将协方差归一化</p>
<ol type="1">
<li><span class="math inline">\(|corr(X,Y)|\le1\)</span></li>
<li><span class="math inline">\(|corr(X,Y)=1|\)</span> &lt;=&gt;
存在常数<span class="math inline">\(a,b\)</span>，使得<span
class="math inline">\(Y=aX+b\)</span></li>
<li><span class="math inline">\(corr(X,Y)=corr(Y,X)\)</span></li>
</ol>
<p><span class="math display">\[
corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}=\frac{Cov(X,Y)}{\sigma_x\sigma_y}
\]</span></p></li>
<li><p><strong>相关性</strong>与<strong>独立性</strong>：</p>
<ol type="1">
<li>如果<span class="math inline">\(X\)</span>与<span
class="math inline">\(Y\)</span>线性不相关，则<span
class="math inline">\(|corr(X,Y)|=0\)</span></li>
<li>如果<span class="math inline">\(X\)</span>与<span
class="math inline">\(Y\)</span>独立，则<span
class="math inline">\(|corr(X,Y)|=0\)</span>，且<span
class="math inline">\(X\)</span>与<span
class="math inline">\(Y\)</span>不存在任何线性、非线性关系</li>
<li>独立一定不相关，但不相关不一定独立</li>
</ol></li>
</ol>
<h3 id="算法动机">4.2.2 算法动机</h3>
<ol type="1">
<li>在数理统计中，方差被经常用来度量数据和其数学期望（即均值）之间偏离程度，这个偏离程度反映了数据分布结构</li>
<li>在许多实际问题中，研究数据和其均值之间的偏离程度有着很重要的意义</li>
<li>在降维之中，需要尽可能将数据向<strong>方差最大方向</strong>进行投影，使得数据所蕴含信息没有丢失，彰显个性。如左下图所示，向𝒚方向投影（使得二维数据映射为一维）就比向𝒙方向投影结果在降维这个意义上而言要好；右下图则是向黑斜线方向投影要好。</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230419112131702.png" alt="image-20230419112131702" style="zoom:80%;" /></p>
<ol type="1">
<li>主成分分析思想是将𝒏维特征数据映射到𝒍维空间(𝒏 ≫
𝒍)，去除原始数据之间的冗余性（通过去除相关性手段达到这一目的）。</li>
<li>将原始数据向这些数据方差最大的方向进行投影。一旦发现了方差最大的投影方向，则继续寻找保持方差第二的方向且进行投影。</li>
<li>将每个数据从𝒏维高维空间映射到𝒍维低维空间，每个数据所得到最好的𝒌维特征就是使得每一维上样本方差都尽可能大。</li>
</ol>
<h3 id="算法描述-1">4.2.3 算法描述</h3>
<ol type="1">
<li>假设有<span class="math inline">\(𝑛\)</span>个<span
class="math inline">\(𝑑\)</span>维样本数据所构成的集合<span
class="math inline">\(𝐷=\{x_1,x_2,…,x_n\}\)</span>，其中<span
class="math inline">\(x_i(1≤i≤n)∈R^d\)</span></li>
<li>集合<span class="math inline">\(𝐷\)</span>可以表示成矩阵<span
class="math inline">\(X_{𝑛×𝑑}\)</span>
<ol type="1">
<li>假定每一维度的特征均值均为零（已经标准化）</li>
</ol></li>
<li>主成分分析的目的是求映射矩阵<span
class="math inline">\(W_{𝑑×𝑙}\)</span>
<ol type="1">
<li>给定一个样本<span class="math inline">\(x_𝑖\)</span>，可将$x_𝑖 <span
class="math inline">\(从\)</span>𝑑<span
class="math inline">\(维空间如下映射到\)</span>𝑙$维空间： <span
class="math inline">\((x_i)_{1×d}(W)_{d×l}\)</span></li>
</ol></li>
<li>将所有降维后数据用<span
class="math inline">\(Y\)</span>表示，有<span
class="math inline">\(Y=XW\)</span>
<ol type="1">
<li><span class="math inline">\(Y\)</span>：降维结果，<span
class="math inline">\(n×l\)</span></li>
<li><span class="math inline">\(X\)</span>：原始数据，<span
class="math inline">\(n×d\)</span></li>
<li><span class="math inline">\(W\)</span>：映射矩阵，<span
class="math inline">\(d×l\)</span></li>
</ol></li>
</ol>
<p>投影矩阵<span
class="math inline">\(W_{d×l}\)</span>为：协方差矩阵<span
class="math inline">\(\Sigma\)</span>前<span
class="math inline">\(l\)</span>个最大的特征根对应的特征向量<span
class="math inline">\(_{d×1}\)</span>组成的矩阵<span
class="math inline">\(_{d×l}\)</span></p>
<p>最优方差为：协方差矩阵<span
class="math inline">\(\Sigma\)</span>前<span
class="math inline">\(l\)</span>个最大的特征根之和</p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230419113127137.png" alt="image-20230419113127137" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230419113624996.png" alt="image-20230419113624996" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230419113632079.png" alt="image-20230419113632079" style="zoom:80%;" /></p>
</blockquote>
<h3 id="其他常用降维方法">4.2.4 其他常用降维方法</h3>
<ol type="1">
<li>非负矩阵分解 NMF：non-negative matrix factorization</li>
<li>多为尺度法MDS：Metric multidimensional</li>
<li>局部先行嵌入LLE：Locally Linear Embedding</li>
</ol>
<h2 id="特征人脸方法">4.3 特征人脸方法</h2>
<h3 id="动机">4.3.1 动机</h3>
<ol type="1">
<li>特征人脸方法是一种应用主成份分析来实现<strong>人脸图像降维</strong>的方法</li>
<li>其本质是用一种称为<strong>“特征人脸(eigenface)”</strong>的特征向量按照线性组合形式来表达每一张原始人脸图像，进而实现人脸识别</li>
<li>由此可见，这一方法的关键之处在于如何得到特征人脸</li>
<li>用特征人脸表示人脸，而非用像素点表示人脸</li>
</ol>
<h3 id="算法描述-2">4.3.2 算法描述</h3>
<ol type="1">
<li><p>将每幅人脸图像转化为列向量</p>
<ol type="1">
<li>例如：将一幅32×32的人脸图像转化为1024×1的列向量</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230419115115486.png" alt="image-20230419115115486" style="zoom:80%;" /></p></li>
<li><p>执行PCA特征降维</p>
<p><img src="/images/AssetMarkdown/image-20230419115230356.png" alt="image-20230419115230356" style="zoom:80%;" /></p></li>
<li><p>每个人脸特征向量<span
class="math inline">\(w_i\)</span>与原始人脸数据<span
class="math inline">\(x_i\)</span>的维数是一样的，均为<span
class="math inline">\(1024\)</span></p></li>
<li><p>可将每个特征向量还原为𝟑𝟐×𝟑𝟐的人脸图像，称之为特征人脸，因此可得到<span
class="math inline">\(l\)</span>个特征人脸</p>
<p><img src="/images/AssetMarkdown/image-20230419115421635.png" alt="image-20230419115421635" style="zoom:80%;" /></p></li>
<li><p>将每幅人脸分别与特征人脸做矩阵乘法，得到一个相关系数</p></li>
<li><p>每幅人脸得到<span class="math inline">\(l\)</span>个相关系数
=&gt; 每幅人脸从1024维约简到<span
class="math inline">\(l\)</span>维</p></li>
<li><p>由于每幅人脸是所有特征人脸的线性组合，因此就实现人脸从“像素点表达”到“特征人脸表达”的转变。每幅人脸从1024维约减到<span
class="math inline">\(l\)</span>维</p>
<p><img src="/images/AssetMarkdown/image-20230419120306729.png" alt="image-20230419120306729" style="zoom:80%;" /></p></li>
</ol>
<h3 id="其他人脸表达的方法聚类pca非负矩阵分解">4.3.3
其他人脸表达的方法：聚类、PCA、非负矩阵分解</h3>
<p><img src="/images/AssetMarkdown/image-20230419120942125.png" alt="image-20230419120942125" style="zoom:80%;" /></p>
<h2 id="潜在语义分析">4.4 潜在语义分析</h2>
<blockquote>
<p><strong>latent semantic analysis</strong>、<strong>latent semantic
indexing</strong></p>
</blockquote>
<p><strong>潜在语义分析(LSA或LSI)</strong>：又叫隐形语义分析</p>
<ol type="1">
<li>是一种从海量文本数据中学习<strong>单词-单词</strong>、<strong>单词-文档</strong>以及<strong>文档-文档</strong>之间隐性关系，进而得到<strong>文档和单词</strong>表达<strong>特征</strong>的方法</li>
<li><strong>基本思想</strong>：综合考虑某些单词在哪些文档中同时出现，以此来决定该词语的含义与其他的词语的相似度</li>
</ol>
<h3 id="潜在语义分析思想">4.4.1 潜在语义分析思想</h3>
<ol type="1">
<li>先构建一个<strong>单词-文档</strong>(term-document)<strong>矩阵A</strong></li>
<li>进而寻找该<strong>矩阵的低秩逼近</strong>(low rank
approximation)，来挖掘<strong>单词-单词、单词-文档以及文档-文档</strong>之间的关联关系</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230426101927415.png" alt="image-20230426101927415" style="zoom:80%;" /></p>
</blockquote>
<h3 id="分析过程">4.4.2 分析过程</h3>
<ol type="1">
<li><p>计算<strong>单词-文档矩阵：<span
class="math inline">\(A_{m×n}\)</span></strong></p>
<ol type="1">
<li>某个term在某个document中出现了几次，对应的值即为几</li>
<li>也可以统计：TF×iDF，TF在某个文章中出现的频率，DF在所有文章中出现的频率</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230426101941266.png" alt="image-20230426101941266" style="zoom:80%;" /></p></li>
<li><p><strong>奇异值分解</strong>：<span
class="math inline">\(A_{m×n}=U_{m×r}D_{r×r}V^T_{r×n}\)</span></p>
<ol type="1">
<li>SVD分解：将矩阵<span
class="math inline">\(A_{m×n}\)</span>分解为三个矩阵的的乘积<span
class="math inline">\(U_{m×r}D_{r×r}V^T_{r×n}\)</span></li>
<li><span
class="math inline">\(D_{r×r}=diag(\sigma_1,\sigma_2,...,\sigma_r)\)</span>，是<span
class="math inline">\(A_{m×n}\)</span>的所有奇异值，也是<span
class="math inline">\(AA^T\)</span>特征值的非负平方根，满足<span
class="math inline">\(\sigma_1\ge\sigma_2\ge,...,\ge\sigma_r&gt;0\)</span></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230426102742675.png" alt="image-20230426102742675" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230426103032436.png" alt="image-20230426103032436" style="zoom:80%;" /></p></li>
<li><p><strong>重建矩阵</strong>：<span
class="math inline">\(A_{k}=U_{m×k}D_{k×k}V^T_{k×n}\)</span></p>
<ol type="1">
<li>选取前k个特征根及其对应的特征向量，对矩阵A进行重建</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230426103044250.png" alt="image-20230426103044250" style="zoom:80%;" /></p></li>
<li><p><strong>挖掘语义关系</strong></p>
<ol type="1">
<li>根据由主题重建出来的矩阵<span
class="math inline">\(A_k\)</span>，计算皮尔逊相关系数</li>
<li>通过分解和重建，可以将主题相关的两个文档之间的相关系数变得更大</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230426103537877.png" alt="image-20230426103537877" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230426103752281.png" alt="image-20230426103752281" style="zoom:80%;" /></p></li>
</ol>
<h2 id="期望最大化算法-em">4.5 期望最大化算法 EM</h2>
<blockquote>
<p><strong>expectation maximization</strong></p>
</blockquote>
<h3 id="模型参数估计">4.5.1 模型参数估计</h3>
<ol type="1">
<li><p>最大似然估计：</p>
<p><img src="/images/AssetMarkdown/image-20230426104845910.png" alt="image-20230426104845910" style="zoom:80%;" /></p></li>
<li><p>最大后验估计：</p>
<p><img src="/images/AssetMarkdown/image-20230426104854324.png" alt="image-20230426104854324" style="zoom:80%;" /></p></li>
<li><p>无论是最大似然估计算法或者是最大后验估计算法，都是充分利用已有数据，在<strong>参数模型确定</strong>（只是参数值未知）情况下，对所优化目标中的<strong>参数求导</strong>，<strong>令导数为0</strong>，求取模型的参数值</p></li>
<li><p>在解决一些具体问题时，难以事先就将模型确定下来，然后利用数据来求取模型中的参数值。在这样情况下，无法直接利用最大似然估计算法或者最大后验估计算法来求取模型参数</p></li>
</ol>
<h3 id="期望最大化算法">4.5.2 期望最大化算法</h3>
<blockquote>
<p>Expectation Maximization</p>
</blockquote>
<ol type="1">
<li>EM算法是一种重要的用于解决<strong>含有隐变量</strong>(latent
variable)问题的<strong>参数估计方法</strong></li>
<li>EM算法分为<strong>求取期望</strong>(E步骤，expectation)和<strong>期望最大化</strong>(M步骤，maximization)两个步骤
<ol type="1">
<li>在EM算法的<strong>E步骤</strong>时，先假设模型参数的初始值，估计隐变量取值</li>
<li>在EM算法的<strong>M步骤</strong>时，基于观测数据、模型参数和隐变量取值一起来最大化“拟合”数据，更新模型参数</li>
<li>基于所更新的模型参数，得到新的隐变量取值(EM算法的 E
步)，然后继续极大化“拟合”数据，更新模型参数(EM算法的M步)</li>
<li>以此类推迭代，直到算法收敛，得到合适的模型参数</li>
</ol></li>
</ol>
<h3 id="em示例二硬币投掷例子">4.5.3 EM示例：二硬币投掷例子</h3>
<p><img src="/images/AssetMarkdown/image-20230426110545341.png" alt="image-20230426110545341" style="zoom:80%;" /></p>
<p><strong>目标/模型参数</strong>：求A/B为正面的概率<span
class="math inline">\(\{\theta_A,\theta_B\}\)</span></p>
<p><strong>隐变量</strong>：某一轮是A投出的，还是B投出的</p>
<blockquote>
<p>初始化模型参数<span
class="math inline">\(\{\theta_A,\theta_B\}\)</span>，即可得到得出某一轮结果是由A/B投出的概率</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230426105838487.png" alt="image-20230426105838487" style="zoom:80%;" /></p>
<blockquote>
<p>然后即可得某一轮结果中，A/B投为正/反面的期望次数</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230426110335936.png" alt="image-20230426110335936" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230426110349882.png" alt="image-20230426110349882" style="zoom:80%;" /></p>
<blockquote>
<p>根据之前的计算结果，得出A/B投为正/反面的期望次数，从而更新A/B分别投正面的概率</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230426110256967.png" alt="image-20230426110256967" style="zoom:80%;" /></p>
<blockquote>
<p>不断迭代上述过程，直到算法收敛，就可以得到A/B投为正面的概率</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230426110404057.png" alt="image-20230426110404057" style="zoom:80%;" /></p>
<h3 id="em示例三硬币投掷例子">4.5.4 EM示例：三硬币投掷例子</h3>
<p><img src="/images/AssetMarkdown/image-20230426112028228.png" alt="image-20230426112028228" style="zoom:80%;" /></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230426112208421.png" alt="image-20230426112208421" style="zoom:80%;" /></p>
</blockquote>
<p><strong>目标/模型参数</strong>：求0/1/2为正面的概率<span
class="math inline">\(\{\lambda,p_1,p_2\}\)</span></p>
<p><strong>隐变量</strong>：硬币0的投掷结果</p>
<blockquote>
<p>初始化模型参数<span
class="math inline">\(\{\lambda,p_1,p_2\}\)</span>：通过假设每一次0号硬币的结果得出</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230426112602109.png" alt="image-20230426112602109" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230426112910620.png" alt="image-20230426112910620" style="zoom:80%;" /></p>
<blockquote>
<p>迭代更新模型参数值，直到收敛</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230426113059647.png" alt="image-20230426113059647" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230426113104146.png" alt="image-20230426113104146" style="zoom:80%;" /></p>
<h3 id="em算法的一般形式">4.5.5 EM算法的一般形式</h3>
<p>​ 对n个相互独立的样本<span
class="math inline">\(X=\{x_1,x_2,...,x_n\}\)</span>及其对应的隐变量<span
class="math inline">\(Z=\{z_1,z_2,...,z_n\}\)</span>，在假设样本的模型参数为<span
class="math inline">\(\Theta\)</span>前提下，观测数据<span
class="math inline">\(x_i\)</span>的概率为<span
class="math inline">\(P(x_i|\Theta)\)</span>，完全数据<span
class="math inline">\((x_i,z_i)\)</span>的<strong>似然函数</strong>为<span
class="math inline">\(P(x_i,z_i|\Theta)\)</span></p>
<p>​ 在这种表示基础上，优化目标为：求解合适的<span
class="math inline">\(\Theta\)</span>和<span
class="math inline">\(Z\)</span>，使得<strong>对数似然函数</strong>最大：
<span class="math display">\[
(\Theta,Z)=argmax_{\Theta,Z}L(\Theta,Z)=argmax_{\Theta,Z}\sum_{i=1}^nlog\sum_{z_i}P(x_i,z_i|\Theta)
\]</span> ​ 但是，优化求解含有未观测数据Z的对数似然函数<span
class="math inline">\(L(\Theta,Z)\)</span>十分困难，EM算法不断构造对数似然函数<span
class="math inline">\(L(\Theta,Z)\)</span>的一个<strong>下界(E步骤)</strong>，然后<strong>最大化这个下界(M步骤)</strong>，以迭代方式逼近模型参数所能取得极大似然值</p>
<h1 id="五深度学习">五、深度学习</h1>
<h2 id="深度学习的历史发展">5.1 深度学习的历史发展</h2>
<p><img src="/images/AssetMarkdown/image-20230426115207956.png" alt="image-20230426115207956" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230426115150060.png" alt="image-20230426115150060" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230426115238544.png" alt="image-20230426115238544" style="zoom:80%;" /></p>
<h2 id="前馈神经网络-fnn">5.2 前馈神经网络 FNN</h2>
<h3 id="浅层学习-vs-深层学习分段学习逐层端到端学习">5.2.1 浅层学习 vs
深层学习：分段学习=&gt;逐层端到端学习</h3>
<p><img src="/images/AssetMarkdown/image-20230426115528872.png" alt="image-20230426115528872" style="zoom:80%;" /></p>
<h3 id="深度学习以端到端的方式逐层抽象逐层学习">5.2.2
深度学习：以端到端的方式逐层抽象、逐层学习</h3>
<ol type="1">
<li>深度学习所得的模型可以视为一个复杂函数</li>
<li>非线性变换与映射的过程：像素点→语义</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230514150442883.png" alt="image-20230514150442883" style="zoom:80%;" /></p>
<h3 id="mcp神经元">5.2.3 MCP神经元</h3>
<ol type="1">
<li>对n个输入数据<span
class="math inline">\(x_i\)</span>进行线性加权求和，然后利用函数<span
class="math inline">\(\Phi(·)\)</span>将结果映射为0/1</li>
<li>特点：
<ol type="1">
<li>可计算、清晰描述</li>
<li>既包含了物理反应(线性加权求和)，也包含了化学反应(阈值函数)</li>
</ol></li>
<li>缺点：
<ol type="1">
<li>权重<span
class="math inline">\(w_i\)</span>是人工定制的，没有通过数据驱动，没有学习的过程</li>
<li>阈值函数的阈值也是人工定制的</li>
</ol></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230426121027846.png" alt="image-20230426121027846" style="zoom:80%;" /></p>
<h3 id="激活函数对输入信息进行非线性变换">5.2.3
激活函数：对输入信息进行非线性变换</h3>
<h4 id="常用激活函数sigmoidtanhrelu">5.2.3.1
常用激活函数：sigmoid、tanh、ReLU</h4>
<p><img src="/images/AssetMarkdown/image-20230514151508626.png" alt="image-20230514151508626" style="zoom:80%;" /></p>
<h4 id="softmax函数将输出映射到01概率空间">5.2.3.2
softmax函数：将输出映射到[0,1]概率空间</h4>
<p><span class="math display">\[
y_i=softmax(x_i)=\frac{e^{x_i}}{\sum_{j=1}^k e^{x_j}}
\]</span></p>
<ol type="1">
<li>将输入数据<span class="math inline">\(x_i\)</span>映射到[0,
1]的概率空间，用于刻画每个输入的<strong>相对重要程度</strong></li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230514151849630.png" alt="image-20230514151849630" style="zoom:80%;" /></p>
</blockquote>
<h3 id="单个神经元的功能加权累加-非线性变换">5.2.4
单个神经元的功能：加权累加 + 非线性变换</h3>
<p><strong>参数是学习出来的</strong></p>
<p><img src="/images/AssetMarkdown/image-20230514152116968.png" alt="image-20230514152116968" style="zoom:80%;" /></p>
<h3 id="损失函数-loss-function">5.2.5 损失函数 Loss Function</h3>
<p><strong>损失函数</strong>：又称为<strong>代价函数</strong>(Cost
Function)</p>
<ol type="1">
<li>用来计算<strong>模型预测值</strong>与<strong>真实值</strong>之间的<strong>误差</strong></li>
<li>损失函数是神经网络设计中的一个重要组成部分，通过定义与任务相关的良好损失函数，在训练过程中可根据损失函数来计算神经网络的误差大小，进而优化神经网络参数</li>
</ol>
<p>两种最常用损失函数：</p>
<ol type="1">
<li>均方误差损失函数</li>
<li>交叉熵损失函数</li>
</ol>
<h4 id="均方误差损失-mse预测值和实际值之间的差">5.2.5.1 均方误差损失
MSE：预测值和实际值之间的差</h4>
<p>均方误差损失函数：</p>
<ol type="1">
<li>计算预测值和实际值之间的<strong>距离</strong>(即误差)<strong>的平方</strong>，衡量模型的优劣</li>
<li>设有n个训练数据<span
class="math inline">\(x_i\)</span>，每个训练数据的真实输出为<span
class="math inline">\(y_i\)</span>，模型对<span
class="math inline">\(x_i\)</span>的预测为<span
class="math inline">\(\hat{y_i}\)</span>，则该模型在n个训练数据下产生的均方误差损失为：</li>
</ol>
<p><span class="math display">\[
MSE=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2
\]</span></p>
<h4 id="交叉熵损失函数两个概率分布之间的距离">5.2.5.2
交叉熵损失函数：两个概率分布之间的距离</h4>
<p>交叉熵(cross entropy)：</p>
<ol type="1">
<li>交叉熵刻画了<strong>两个概率分布之间的距离</strong>，旨在描绘通过概率分布q来表达概率分布p的困难程度</li>
<li><strong>交叉熵越小，两个概率分布p和q越接近</strong></li>
<li>设p和q是数据x的两个概率分布，则<strong>通过q来表示p的交叉熵</strong>为：</li>
</ol>
<p><span class="math display">\[
H(p,q)=-\sum_x p(x)*\log q(x)
\]</span></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230514153303851.png" alt="image-20230514153303851" style="zoom:80%;" /></p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230514194138200.png" alt="image-20230514194138200" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230514194151894.png" alt="image-20230514194151894" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230514194235428.png" alt="image-20230514194235428" style="zoom:80%;" /></p>
<h3 id="感知机模型">5.2.6 感知机模型</h3>
<h4 id="单层感知机">5.2.6.1 单层感知机</h4>
<ol type="1">
<li>早期的感知机结构和MCP模型相似，由一个输入层和一个输出层构成，因此也被称为“单层感知机”</li>
<li>感知机的输入层负责接收实数值的输入向量，输出层则能输出1或-1两个值</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230514153853900.png" alt="image-20230514153853900" style="zoom:80%;" /></p>
<ol type="1">
<li>单层感知机可被用来<strong>区分线性可分数据</strong></li>
<li>在下图中，AND、NAND和OR为线性可分函数，所以可利用单层感知机来模拟这些逻辑函数</li>
<li>但是，由于XOR是非线性可分的逻辑函数，因此单层感知机无法模拟逻辑异或函数的功能</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230514154041975.png" alt="image-20230514154041975" style="zoom:80%;" /></p>
<h4 id="多层感知机前馈神经网络fnn">5.2.6.2
多层感知机/前馈神经网络(FNN)</h4>
<p><strong>多层感知机</strong>：由<strong>输入层</strong>、<strong>输出层</strong>和至少一层的<strong>隐藏层</strong>构成</p>
<ol type="1">
<li>网络中各个隐藏层中神经元可接收相邻<strong>前序隐藏层</strong>中<strong>所有神经元</strong>传递而来的信息，经过加工处理后将信息输出给相邻<strong>后续隐藏层</strong>中<strong>所有神经元</strong></li>
<li>各个神经元接受前一级的输入，并输出到下一级，模型中没有反馈</li>
<li>层与层之间通过“<strong>全连接</strong>”进行链接，即两个相邻层之间的神经元完全成对连接，但层内的神经元不相互连接</li>
<li><strong>前馈神经网络 feedforward neural network</strong></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230514154308251.png" alt="image-20230514154308251" style="zoom:80%;" /></p>
<h3 id="参数优化">5.2.7 参数优化</h3>
<blockquote>
<p><strong>目标：最小化损失函数</strong></p>
</blockquote>
<h4 id="梯度下降-gradient-descent">5.2.7.1 梯度下降 Gradient
Descent</h4>
<p><strong>梯度</strong>：</p>
<ol type="1">
<li><p><strong>一元变量</strong>所构成的函数<span
class="math inline">\(f\)</span>在<span
class="math inline">\(x\)</span>处的梯度为： <span
class="math display">\[
\frac{df(x)}{dx}=\lim_{h→0} \frac{f(x+h)-f(x)}{h}
\]</span></p></li>
<li><p>在<strong>多元函数</strong>中，梯度是对<strong>每一变量所求导数</strong>组成的<strong>向量</strong></p></li>
<li><p><strong>梯度的反方向</strong>是函数值下降最快的方向，因此是损失函数求解的方向</p></li>
<li><p>在实际中，引入<strong>步长η(为定值)</strong>，用<span
class="math inline">\(x-η𝜵f(x)\)</span>来更新<span
class="math inline">\(x\)</span></p></li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230514191448977.png" alt="image-20230514191448977" style="zoom:80%;" /></p>
<blockquote>
<p><strong>证明过程</strong>：</p>
<ol type="1">
<li><p>设损失函数<span
class="math inline">\(f(x)\)</span>是连续可微的多元变量函数，其泰勒展开为：
<span class="math display">\[
f(x+Δx) = f(x) + f&#39;(x)Δx +
\frac{1}{2}f&#39;&#39;(x)(Δx)^2+...+\frac{1}{n!}f^{(n)}(Δx)^n\\
f(x+Δx) - f(x) ≈ (𝜵f(x))^TΔx
\]</span></p></li>
<li><p>因为我们的目的是最小化损失函数<span
class="math inline">\(f(x)\)</span>，则<span
class="math inline">\(f(x+Δx) &lt; f(x)\)</span>，即<span
class="math inline">\((𝜵f(x))^TΔx&lt;0\)</span></p></li>
<li><p>在<span class="math inline">\((𝜵f(x))^TΔx =
|𝜵f(x)||Δx|cosθ\)</span>中，<span
class="math inline">\(|𝜵f(x)|\)</span>为损失函数梯度的模，<span
class="math inline">\(|Δx|\)</span>为下一轮迭代中x取值增量的模，两者均为正数。为了保证损失误差减少，只要保证<span
class="math inline">\(cos θ&lt;0\)</span></p></li>
<li><p>当<span class="math inline">\(θ = 180°\)</span>时，<span
class="math inline">\(cos θ=-1\)</span>，此时损失函数减少的幅度值<span
class="math inline">\((𝜵f(x))^TΔx\)</span>取到最小值</p></li>
<li><p>因此<span class="math inline">\(Δx\)</span>的选取应该为<span
class="math inline">\(𝜵f(x)\)</span>的<strong>反方向</strong></p></li>
</ol>
</blockquote>
<h4 id="误差反向传播-bp">5.2.7.2 误差反向传播 BP</h4>
<blockquote>
<p><strong>Error Back Propagation</strong></p>
</blockquote>
<ol type="1">
<li>BP算法是一种将<strong>输出层误差</strong>反向传播给<strong>隐藏层</strong>进行<strong>参数更新</strong>的方法</li>
<li>将误差从后向前传递，将误差分摊给各层所有单元，从而获得各层单元所产生的误差，进而依据这个误差来让各层单元负起各自责任、修正各单元参数</li>
<li>每个参数经过误差反向传播后，新的参数值为：</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
w^{new} &amp;=  w_1-η×\frac{\partial{\mathcal{L}}}{\partial{w_1}}\\
        &amp;=  w_1-η×\frac{d\mathcal{L}}{d\mathcal{O}}×\frac{d\mathcal{O}}{d\mathcal{X}}×\frac{d\mathcal{X}}{d\mathcal{w_1}}\\
        &amp;=  w_1-η×\frac{d\mathcal{L}}{d\mathcal{O}}×\frac{1}{1+e^{-x}}×(1-\frac{1}{1+e^{-x}})×out_1
\end{aligned}
\]</span></p>
<p><img src="/images/AssetMarkdown/image-20230514191608315.png" alt="image-20230514191608315" style="zoom:80%;" /></p>
<h4 id="链式求导法则">5.2.7.3 链式求导法则</h4>
<ol type="1">
<li>由于<span class="math inline">\(w_1\)</span>与加权累加函数<span
class="math inline">\(\mathcal{X}\)</span>和<strong>sigmoid</strong>函数均有关，因此<span
class="math inline">\(\frac{d\mathcal{L}}{dw_1}=\frac{d\mathcal{L}}{d\mathcal{O}}·\frac{d\mathcal{O}}{d\mathcal{X}}·\frac{d\mathcal{X}}{d\mathcal{w_1}}\)</span></li>
<li>在这个链式求导中：
<ol type="1">
<li><span
class="math inline">\(\frac{d\mathcal{L}}{d\mathcal{O}}\)</span>：与损失函数的定义有关</li>
<li><span
class="math inline">\(\frac{d\mathcal{O}}{d\mathcal{X}}\)</span>：是对<strong>sigmoid</strong>函数求导，结果为<span
class="math inline">\(\frac{1}{1+e^{-x}}·(1-\frac{1}{1+e^{-x}})\)</span></li>
<li><span
class="math inline">\(\frac{d\mathcal{X}}{dw_1}\)</span>：是加权累加函数<span
class="math inline">\(\sum_{i=1}^n w_i·out_i\)</span>，结果为<span
class="math inline">\(out_i\)</span></li>
</ol></li>
<li>链式求导实现了<strong>损失函数对某个自变量求偏导</strong>，好比将损失误差从输出端向输入端逐层传播，通过这个传播过程来更新该自变量取值。梯度下降法告诉我们，只要沿着损失函数梯度的反方向来更新参数，就可使得损失函数下降最快</li>
</ol>
<h3 id="机器学习的能力在于拟合和优化">5.2.8
机器学习的能力在于拟合和优化</h3>
<p><img src="/images/AssetMarkdown/image-20230514194017518.png" alt="image-20230514194017518" style="zoom:80%;" /></p>
<h2 id="卷积神经网络-cnn">5.3 卷积神经网络 CNN</h2>
<blockquote>
<p><strong>convolution neural network</strong></p>
</blockquote>
<p>CNN引入了一个卷积层，将原始图像先经过一次卷积(即下采样)，然后再送入FNN进行学习</p>
<ol type="1">
<li><strong>CNN = 卷积层 + 池化层 + 全连接层 +
Softmax激活函数</strong></li>
</ol>
<h3 id="卷积操作线性操作">5.3.1 卷积操作：线性操作</h3>
<h4 id="卷积核的权重是通过学习得出的">5.3.1.1
卷积核的权重是通过学习得出的</h4>
<ol type="1">
<li><strong>图像</strong>中像素点具有很强的<strong>空间依赖性</strong>，卷积就是针对像素点的空间依赖性来对图像进行处理的一种技术</li>
<li>在图像卷积计算中，需要定义一个<strong>卷积核(kernel)</strong>
<ol type="1">
<li>卷积核是一个<strong>二维矩阵</strong></li>
<li><strong>矩阵中数值</strong>为对图像中与卷积核同样大小的子块像素点进行卷积计算时所采用的<strong>权重</strong></li>
<li>卷积核中的<strong>权重系数w<sub>i</sub></strong>是通过数据驱动机制<strong>学习</strong>得到，其用来捕获图像中某像素点及其邻域像素点所构成的特有空间模式</li>
<li>一旦从数据中学习得到权重系数，这些<strong>权重系数</strong>就刻画了<strong>图像中像素点构成的空间分布不同模式</strong></li>
</ol></li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230514194854168.png" alt="image-20230514194854168" style="zoom:80%;" /></p>
</blockquote>
<h4 id="对图像进行卷积操作">5.3.1.2 对图像进行卷积操作</h4>
<ol type="1">
<li>如果卷积核中心位置的权重系数越小且与其它卷积权重系数差别越小，则卷积所得到图像滤波结果越模糊，这被称为<strong>图像平滑操作</strong></li>
<li><strong>7×7</strong>大小的图像，通过<strong>3×3大小卷积核</strong>以<strong>1的步长</strong>进行卷积操作，可得到<strong>5×5</strong>大小的卷积结果</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230514195631109.png" alt="image-20230514195631109" style="zoom:80%;" /></p>
<h4 id="卷积核特征图感受野">5.3.1.3 卷积核、特征图、感受野</h4>
<p><img src="/images/AssetMarkdown/image-20230514195930797.png" alt="image-20230514195930797" style="zoom:80%;" /></p>
<h3 id="池化操作非线性操作">5.3.2 池化操作：非线性操作</h3>
<ol type="1">
<li>进一步进行<strong>下采样</strong>操作</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230514200028614.png" alt="image-20230514200028614" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230514200107035.png" alt="image-20230514200107035" style="zoom:80%;" /></p>
<h3 id="神经网络正则化解决过拟合问题">5.3.3
神经网络正则化：解决过拟合问题</h3>
<p>为了缓解神经网络在训练过程中出现的<strong>过拟合</strong>问题，需要采取一些<strong>正则化技术</strong>来提升神经网络的<strong>泛化能力</strong>(generalization)</p>
<ol type="1">
<li><p><strong>Dropout</strong>：随机删除一定比例的连接</p>
<p><img src="/images/AssetMarkdown/image-20230514200437118.png" alt="image-20230514200437118" style="zoom:67%;" /></p></li>
<li><p><strong>Batch-Normalization</strong>：一次送入一批图像，而不是一张图像，一批图像之间有一定的关联度</p></li>
<li><p><strong>L1-Norm &amp; L2-Norm</strong>：</p>
<p><img src="/images/AssetMarkdown/image-20230514200544741.png" alt="image-20230514200544741" style="zoom:80%;" /></p></li>
</ol>
<h2 id="循环神经网络-rnn"><strong>5.4 循环神经网络 RNN</strong></h2>
<p>循环神经网络：处理<strong>序列数据</strong>时所采用的网络结构</p>
<ol type="1">
<li>先前所介绍的前馈神经网络或卷积神经网络所需要处理的输入数据一次性给定，难以处理存在前后依赖关系的数据</li>
<li>循环神经网络的本质是希望模拟人所具有的记忆能力，在学习过程中<strong>记住部分已经出现的信息</strong>，并利用所记住的信息影响后续结点输出</li>
<li>循环神经网络在自然语言处理，例如语音识别、情感分析、机器翻译等领域有重要应用</li>
</ol>
<h3 id="rnn的结构">5.4.1 RNN的结构</h3>
<p><img src="/images/AssetMarkdown/image-20230514214259608.png" alt="image-20230514214259608" style="zoom:80%;" /></p>
<ol type="1">
<li><p>循环神经网络在处理数据过程中构成了一个循环体</p>
<ol type="1">
<li>对于一个序列数据，在每一时刻<span
class="math inline">\(t\)</span>，循环神经网络单元会读取当前输入数据<span
class="math inline">\(x_t\)</span>和前一时刻输入数据<span
class="math inline">\(x_{t−1}\)</span>所对应的隐式编码结果<span
class="math inline">\(h_{t−1}\)</span>，一同生成<span
class="math inline">\(t\)</span>时刻的隐式编码结果<span
class="math inline">\(h_t\)</span></li>
<li>接着将<span class="math inline">\(h_t\)</span>后传，去参与生成<span
class="math inline">\(t+1\)</span>时刻输入数据<span
class="math inline">\(x_{t+1}\)</span>的隐式编码<span
class="math inline">\(h_{t+1}\)</span></li>
</ol></li>
<li><p><strong>时刻<span
class="math inline">\(t\)</span>所得到的隐式编码<span
class="math inline">\(h_t\)</span>是由上一时刻隐式编码<span
class="math inline">\(h_{t-1}\)</span>和当前输入<span
class="math inline">\(x_t\)</span>共同参与生成的</strong></p>
<ol type="1">
<li>这可认为隐式编码<span
class="math inline">\(h_{t-1}\)</span>已经“记忆”了<span
class="math inline">\(t\)</span>时刻之前的时序信息，或者说前序时刻信息影响了后续时刻信息的处理</li>
<li>与前馈神经网络和卷积神经网络在处理时需要将所有数据一次性输入不同，这体现了循环神经网络可刻画<strong>序列数据存在时序依赖</strong>这一重要特点</li>
</ol></li>
<li><p>在时刻<span
class="math inline">\(t\)</span>，一旦得到当前输入数据<span
class="math inline">\(x_t\)</span>，RNN会结合前一时刻的隐式编码<span
class="math inline">\(h_{t-1}\)</span>，生成当前时刻的隐式编码<span
class="math inline">\(h_t\)</span>： <span class="math display">\[
\begin{aligned}
h_t  &amp;=  \Phi(U×x_t+W×h_{t-1}) \\
&amp;=  \Phi(U×x_t+W×\Phi(U×x_{t-1}+W×h_{t-2})) \\
&amp;=  \Phi(U×x_t+W×\Phi(U×x_{t-1}+W×\Phi(U×x_{t-2}+...))))
\end{aligned}
\]</span></p>
<ol type="1">
<li><span
class="math inline">\(\Phi(·)\)</span>：激活函数，一般为Sigmoid或Tanh，使模型能够忘掉一些信息，同时更新记忆内容</li>
<li><span
class="math inline">\(U、W\)</span>：模型参数，对于每一时刻都是共用的</li>
</ol></li>
</ol>
<h3 id="沿时间反向传播算法-bptt">5.4.2 沿时间反向传播算法 BPTT</h3>
<blockquote>
<p><strong>Back Propagation Through Time</strong></p>
</blockquote>
<h4 id="思想">5.4.2.1 思想</h4>
<ol type="1">
<li>按照时间将RNN展开后，可以得到一个和FNN相似的网络结构</li>
<li>这个网络结构可以利用BP和Gradient
Descent算法来训练模型参数，这种训练方法被称为<strong>BPTT</strong></li>
<li>由于RNN每一时刻都有一个输出，因此在计算RNN的损失时，需要将所有时刻上的损失进行累加</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230514220258231.png" alt="image-20230514220258231" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230514220315167.png" alt="image-20230514220315167" style="zoom:80%;" /></p>
</blockquote>
<h4 id="算法">5.4.2.2 算法</h4>
<ol type="1">
<li><p>设时刻<span class="math inline">\(t\)</span>隐式编码如下得到：
<span class="math display">\[
h_t=tanh(W_xt_x+W_hh_{t-1}+b)
\]</span></p></li>
<li><p>使用交叉熵损失函数计算时刻<span
class="math inline">\(t\)</span>预测输出与实际输出的误差为<span
class="math inline">\(E_t\)</span>，则整个序列产生的误差为： <span
class="math display">\[
E=\frac{1}{2}\sum_{t=1}^T E_t
\]</span></p></li>
<li><p>根据时刻<span
class="math inline">\(t\)</span>所得到的误差来更新参数<span
class="math inline">\(W_x\)</span>的方法为：</p>
<ol type="1">
<li>在时刻<span class="math inline">\(t\)</span>计算所得的<span
class="math inline">\(O_t\)</span>不仅涉及到了<span
class="math inline">\(t\)</span>时刻的<span
class="math inline">\(W_x\)</span>，而且也涉及了前面所有时刻的<span
class="math inline">\(W_x\)</span></li>
<li>按照链式求导法则，<span class="math inline">\(E_t\)</span>对<span
class="math inline">\(W_x\)</span>求导时，也需要对前面时刻的<span
class="math inline">\(W_x\)</span>依次求导，然后再将求导结构进行累加，即：</li>
</ol>
<p><span class="math display">\[
\frac{\partial E_t}{\partial W_x}=\sum_{i=1}^y
\frac{dE_t}{dO_t}·\frac{dO_t}{dh_t}·(\prod_{j=i+1}^t
\frac{dh_j}{dh_{j-1}})·\frac{dh_i}{dW_x}\\
其中, \prod_{j=i+1}^t \frac{dh_j}{dh_{j-1}}=\prod_{j=i+1}^t
tanh&#39;×W_h
\]</span></p></li>
<li><p>令<span class="math inline">\(t=3\)</span>，则有： <span
class="math display">\[
\begin{aligned}
\frac{\partial E_3}{\partial
W_x}&amp;=\frac{dE_3}{dO_3}·\frac{dO_3}{dh_3}·\frac{dh_3}{dW_x}\\
&amp;+\frac{dE_3}{dO_3}·\frac{dO_3}{dh_3}·\frac{dh_3}{dh_2}·\frac{dh_2}{dW_x}\\
&amp;+\frac{dE_3}{dO_3}·\frac{dO_3}{dh_3}·\frac{dh_3}{dh_2}·\frac{dh_2}{dh_1}·\frac{dh_1}{dW_x}
\end{aligned}
\]</span></p></li>
<li><p>由于tanh函数的导数取值位于[0,1]区间，对于长序列而言，若干个[0,1]区间的小数相乘，会使得参数求导结果很小，引发<strong>梯度消失</strong>问题。<span
class="math inline">\(E_t\)</span>对<span
class="math inline">\(W_h\)</span>的求导类似。</p></li>
<li><p>为了解决梯度消失问题，提出了<strong>长短时记忆模型(LSTM)</strong></p></li>
</ol>
<h4 id="应用">5.4.2.3 应用</h4>
<p><img src="/images/AssetMarkdown/image-20230514221952266.png" alt="image-20230514221952266" style="zoom:80%;" /></p>
<h3 id="长短时记忆模型-lstm解决rnn梯度消失的问题">5.4.3 长短时记忆模型
LSTM：解决RNN梯度消失的问题</h3>
<blockquote>
<p><strong>Long-Short-Term-Memory</strong></p>
</blockquote>
<h4 id="思想-1">5.4.3.1 思想</h4>
<p>LSTM网络中引入了<strong>内部记忆单元(internal memory
cell)</strong>和<strong>门(gates)</strong>两种结构来对当前时刻输入信息以及前序时刻所生成信息进行整合和传递</p>
<ol type="1">
<li>在这里，内部记忆单元中信息可视为对“历史信息”的累积</li>
<li>常见的LSTM模型中有<strong>输入门(input
gate)</strong>、<strong>遗忘门(forget
gate)</strong>和<strong>输出门(output gate)</strong>三种门结构</li>
<li>对于给定的当前时刻输入数据<span
class="math inline">\(x_t\)</span>和前一时刻隐式编码<span
class="math inline">\(h_{t-1}\)</span>，输入门、遗忘门和输出门通过各自参数对其编码，分别得到三种门结构的输出<span
class="math inline">\(i_t, f_t, o_t\)</span></li>
<li>在此基础上，再进一步结合前一时刻内部记忆单元信息<span
class="math inline">\(c_{t-1}\)</span>来更新当前时刻内部记忆单元信息<span
class="math inline">\(c_t\)</span>，最终得到当前时刻的隐式编码<span
class="math inline">\(h_t\)</span></li>
<li><span class="math inline">\(h_t\)</span>对应短期记忆，<span
class="math inline">\(c_t\)</span>对应长期记忆
<ol type="1">
<li>RNN直接传递短期记忆，因此容易遗忘</li>
<li>而LSTM通过门的作用，将短期记忆转化为长期记忆，因此记忆能力更强</li>
</ol></li>
</ol>
<h4 id="符号定义-1">5.4.3.2 符号定义</h4>
<p><img src="/images/AssetMarkdown/image-20230514223119712.png" alt="image-20230514223119712" style="zoom:80%;" /></p>
<ol type="1">
<li>输入门、遗忘门和输出门通过各自参数对当前时刻输入数据<span
class="math inline">\(x_𝑡\)</span>和前一时刻隐式编码<span
class="math inline">\(h_{t-1}\)</span>处理后，利用<span
class="math inline">\(𝑠𝑖𝑔𝑚𝑜𝑖𝑑\)</span>对处理结果进行非线性映射，因此三种门结构的输出<span
class="math inline">\(i_t,f_t,o_t \in(0,1)\)</span></li>
<li>正是由于三个门结构的输出值为位于0到1之间的向量，因此其在信息处理中起到了“调控开关”的“门”作用</li>
<li>三个门结构所输出向量的维数、内部记忆单元的维数和隐式编码的维数均相等</li>
</ol>
<h4 id="lstm结构">5.4.3.3 LSTM结构</h4>
<p><img src="/images/AssetMarkdown/image-20230515001733288.png" alt="image-20230515001733288" style="zoom:80%;" /></p>
<ol type="1">
<li>在每个时刻<span class="math inline">\(t\)</span>
<ol type="1">
<li>包含序列信息的变量为：<span
class="math inline">\(c_t，h_t\)</span></li>
<li>输出为<span class="math inline">\(t\)</span>时刻的隐式编码：<span
class="math inline">\(h_t=o_t⊙tanh(c_t)\)</span></li>
</ol></li>
<li>LSTM中的参数有：
<ol type="1">
<li>输入门<span class="math inline">\(i_t\)</span>的参数： <span
class="math inline">\(W_{xi},W_{hi},b_i →
i_t=sigmoid(W_{xi}x_t+W_{hi}h_{t-1}+b_i)\)</span></li>
<li>遗忘门<span class="math inline">\(f_t\)</span>的参数： <span
class="math inline">\(W_{xf},W_{hf},b_f→
f_t=sigmoid(W_{xf}x_t+W_{hf}h_{t-1}+b_f)\)</span></li>
<li>输出门<span class="math inline">\(o_t\)</span>的参数： <span
class="math inline">\(W_{xo},W_{ho},b_o→
o_t=sigmoid(W_{xo}x_t+W_{ho}h_{t-1}+b_o)\)</span></li>
<li>内部记忆单元<span class="math inline">\(c_t\)</span>的参数：<span
class="math inline">\(W_{xc},W_{hc},b_c\)</span></li>
</ol></li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230515191651335.png" alt="image-20230515191651335" style="zoom:80%;" /></p>
</blockquote>
<ol start="3" type="1">
<li>从<span
class="math inline">\(c_t=f_t⊙c_{t-1}+i_t⊙tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c)\)</span>可以得出，对于当前时刻<span
class="math inline">\(t\)</span>所对应内部记忆单元种信息的调整，涉及到：
<ol type="1">
<li>遗忘门信息<span class="math inline">\(f_t\)</span></li>
<li><span
class="math inline">\(t-1\)</span>时刻所对应内部记忆单元中的信息<span
class="math inline">\(c_{t-1}\)</span></li>
<li>输入门信息<span class="math inline">\(i_t\)</span></li>
<li><span class="math inline">\(t-1\)</span>时刻的隐式编码<span
class="math inline">\(h_{t-1}\)</span></li>
</ol></li>
<li><span
class="math inline">\(sigmoid(x)\in(0,1)\)</span>：因此输入门、输出门、遗忘门三种门结构起到了信息控制门的作用</li>
<li><span
class="math inline">\(tanh(x)\in(-1,1)\)</span>：因此内部记忆单元<span
class="math inline">\(c_t\)</span>、隐式编码<span
class="math inline">\(h_t\)</span>在进行信息整合时，可以起到信息增(为正)或信息减(为负)的效果</li>
</ol>
<h4 id="lstm如何克服梯度消失">5.4.3.4 LSTM如何克服梯度消失</h4>
<p><img src="/images/AssetMarkdown/image-20230515193712715.png" alt="image-20230515193712715" style="zoom:80%;" /></p>
<h3 id="gru门控循环单元神经网络">5.4.4 GRU门控循环单元神经网络</h3>
<p>与LSTM类似，引入<strong>门结构</strong>解决梯度消失问题</p>
<p><img src="/images/AssetMarkdown/image-20230515193901099.png" alt="image-20230515193901099" style="zoom:80%;" /></p>
<h2 id="不考5.5-深度生成学习">(不考)5.5 深度生成学习</h2>
<p><img src="/images/AssetMarkdown/image-20230515194317721.png" alt="image-20230515194317721" style="zoom:80%;" /></p>
<ol type="1">
<li>在本章之前的介绍中，神经网络模型从数据中提取出高层语义在数据中所蕴含的<strong>“模式”</strong>，并利用这些模式实现<strong>对数据的分类和检测</strong>等，这种模型通常称为<strong>判别模型</strong>：判别模型不关心数据如何生成，它只关心数据蕴含哪些模式以及如何将数据进行分类</li>
<li>与之相对的模型类型被称为<strong>生成模型(generative
model)</strong>：生成模型需要学习目标数据的分布规律，以<strong>合成</strong>属于目标数据空间的<strong>新数据</strong></li>
<li>生成模型代表：
<ol type="1">
<li>变分自编码器(variational auto-encoder, VAE)</li>
<li>自回归模型(Autoregressive models)</li>
<li>生成对抗网络（generative adversarial network，GAN）</li>
</ol></li>
</ol>
<h2 id="深度学习应用">5.6 深度学习应用</h2>
<h3 id="自然语言中词向量生成">5.6.1 自然语言中词向量生成</h3>
<p><img src="/images/AssetMarkdown/image-20230515195126133.png" alt="image-20230515195126133" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230515195158097.png" alt="image-20230515195158097" style="zoom:80%;" /></p>
<h3 id="word2vec模型">5.6.2 Word2Vec模型</h3>
<p><img src="/images/AssetMarkdown/image-20230515195221719.png" alt="image-20230515195221719" style="zoom:80%;" /></p>
<ol type="1">
<li><p>假设词典中有<span
class="math inline">\(V\)</span>个不同的单词，现在考虑如何生成第<span
class="math inline">\(k\)</span>个单词的<span
class="math inline">\(N\)</span>维词向量</p>
<ol type="1">
<li>首先，将该单词表示成<span
class="math inline">\(V\)</span>维<strong>one-hot向量</strong><span
class="math inline">\(X\)</span>，向量<span
class="math inline">\(X\)</span>中第<span
class="math inline">\(k\)</span>个位置取值为1、其余位置取值均为0，one-hot向量<span
class="math inline">\(X\)</span>表示词典中的一个单词</li>
<li>隐藏层神经元大小为<span
class="math inline">\(N\)</span>，每个神经元记为<span
class="math inline">\(h_i (1≤i≤N)\)</span></li>
<li>向量<span class="math inline">\(X\)</span>中每个<span
class="math inline">\(x_i
(1≤i≤V)\)</span>与隐藏层神经元是全连接，连接权重矩阵为<span
class="math inline">\(W_{V×N}\)</span></li>
<li>输出层是<span
class="math inline">\(V\)</span>维归一化的概率值，其中<span
class="math inline">\(y_k\)</span>对应第<span
class="math inline">\(k\)</span>个单词归一化的概率值，显然其取值应该远大于其它输出所对应的归一化概率值</li>
</ol></li>
<li><p>因此，某个输入单词对应的<span
class="math inline">\(N\)</span>维词向量为：<span
class="math inline">\([w_{k1},w_{k2},...,w_{kN}]\)</span></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230515200909879.png" alt="image-20230515200909879" style="zoom:80%;" /></p>
</blockquote></li>
<li><p>训练词向量模型的目标为：</p>
<p><img src="/images/AssetMarkdown/image-20230515200951193.png" alt="image-20230515200951193" style="zoom:80%;" /></p></li>
</ol>
<h3 id="cbow模型通过上下文预测单词">5.6.3
CBOW模型：通过上下文预测单词</h3>
<p><img src="/images/AssetMarkdown/image-20230515201117406.png" alt="image-20230515201117406" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230515201230336.png" alt="image-20230515201230336" style="zoom:80%;" /></p>
<h3 id="图像分类和目标定位">5.6.4 图像分类和目标定位</h3>
<p><img src="/images/AssetMarkdown/image-20230515201257374.png" alt="image-20230515201257374" style="zoom:80%;" /></p>
<h1 id="六强化学习">六、强化学习</h1>
<h2 id="强化学习问题定义">6.1 强化学习问题定义</h2>
<h3 id="强化学习中的概念">6.1.1 强化学习中的概念</h3>
<p><img src="/images/AssetMarkdown/image-20230517103207132.png" alt="image-20230517103207132" style="zoom:80%;" /></p>
<ol type="1">
<li><strong>智能体（agent）</strong>：智能体是强化学习算法的主体，它能够根据经验做出主观判断并执行动作，是整个智能系统的核心</li>
<li><strong>环境（environment）</strong>：智能体以外的一切统称为环境，环境在与智能体的交互中，能被智能体所采取的动作影响，同时环境也能向智能体反馈状态和奖励</li>
<li><strong>状态（state）</strong>：状态可以理解为智能体对环境的一种理解和编码，通常包含了对智能体所采取决策产生影响的信息</li>
<li><strong>动作（action）</strong>：动作是智能体对环境产生影响的方式</li>
<li><strong>策略（policy）</strong>：策略是智能体在所处状态下去执行某个动作的依据，即给定一个状态，智能体可根据一个策略来选择应该采取的动作</li>
<li><strong>奖励（reward）</strong>：奖励是智能体序贯式采取一系列动作后从环境获得的收益</li>
</ol>
<h3 id="强化学习的特点">6.1.2 强化学习的特点</h3>
<ol type="1">
<li><strong>基于评估</strong>：强化学习利用环境评估当前策略，以此为依据进行优化</li>
<li><strong>交互性</strong>：强化学习的数据在与环境的交互中产生</li>
<li><strong>序列决策过程</strong>：智能主体在与环境的交互中需要作出一系列的决策，这些决策往往是前后关联的</li>
<li>注：现实中强化学习问题往往还具有<strong>奖励滞后</strong>，<strong>基于采样的评估</strong>等特点</li>
</ol>
<h3 id="马尔可夫决策过程">6.1.3 马尔可夫决策过程</h3>
<p><img src="/images/AssetMarkdown/image-20230517103826163.png" alt="image-20230517103826163" style="zoom: 50%;" /></p>
<p>序列优化问题描述：</p>
<ol type="1">
<li>在下图网格中，假设有一个机器人位于<span
class="math inline">\(s_1\)</span>，其每一步只能向上或向右移动一格，跃出方格会被惩罚（且游戏停止）</li>
<li>如何使用强化学习找到一种策略，使机器人从<span
class="math inline">\(s_1\)</span>到达<span
class="math inline">\(s_9\)</span>？</li>
</ol>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">智能主体 Agent</th>
<th style="text-align: center;">迷宫机器人</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">环境</td>
<td style="text-align: center;">3×3方格</td>
</tr>
<tr class="even">
<td style="text-align: center;">状态</td>
<td style="text-align: center;">机器人当前时刻所处方格</td>
</tr>
<tr class="odd">
<td style="text-align: center;">动作</td>
<td style="text-align: center;">每次移动一个方格</td>
</tr>
<tr class="even">
<td style="text-align: center;">奖励</td>
<td style="text-align: center;">到达<span
class="math inline">\(s_9\)</span>时给予奖励，越界时给予惩罚</td>
</tr>
</tbody>
</table>
<h4 id="离散马尔可夫过程-discrete-markov-process">6.1.3.1
离散马尔可夫过程 Discrete Markov Process</h4>
<ol type="1">
<li><p><strong>离散随机过程</strong>：一个随机过程实际上是一系列随时间变化的随机变量，其中当时间是离散量时，一个随机过程可以表示为<span
class="math inline">\(\{X_t\}_{t=0,1,2...}\)</span>，其中每个<span
class="math inline">\(X_t\)</span>都是一个随机变量，这被称为离散随机过程</p></li>
<li><p><strong>马尔科夫链</strong>：满足马尔可夫性质的离散随机过程。也被称为离散马尔可夫过程</p>
<ol type="1">
<li>马尔可夫性质：<strong>t+1时刻状态仅与t时刻状态相关</strong></li>
</ol>
<p><span class="math display">\[
P(X_{t+1}=x_{t+1}|X_0=x_0,X_1=x_1,...,X_t=x_t)=P(X_{t+1}=x_{t+1}|X_t=x_t)
\]</span></p></li>
</ol>
<h4 id="马尔可夫奖励过程-markov-reward-process引入奖励">6.1.3.2
马尔可夫奖励过程 Markov Reward Process：引入奖励</h4>
<h5 id="奖励机制">6.1.3.2.1 奖励机制</h5>
<p>为了在序列决策中对目标进行优化，在马尔可夫随机过程框架中加入了奖励机制：</p>
<ol type="1">
<li>奖励函数<span class="math inline">\(𝑹:𝑺×𝑺↦ℝ\)</span>，其中<span
class="math inline">\(𝑹(𝑺_𝒕,𝑺_{𝒕+𝟏})\)</span>描述了从第<span
class="math inline">\(𝒕\)</span>步状态转移到第<span
class="math inline">\(𝒕+𝟏\)</span>步状态所获得奖励</li>
<li>在一个序列决策过程中，不同状态之间的转移产生了一系列的奖励<span
class="math inline">\((𝑹_𝟏,𝑹_𝟐,⋯)\)</span>，其中<span
class="math inline">\(𝑹_{𝒕+𝟏}\)</span>为<span
class="math inline">\(𝑹(𝑺_𝒕,𝑺_{𝒕+𝟏})\)</span>的简便记法。</li>
<li>引入奖励机制，这样可以衡量任意序列的优劣，即对序列决策进行评价。</li>
</ol>
<h5 id="反馈折扣系数">6.1.3.2.2 反馈、折扣系数</h5>
<p>问题：给定两个因为状态转移而产生的奖励序列<span
class="math inline">\((𝟏,𝟏,𝟎,𝟎)\)</span>和<span
class="math inline">\((𝟎,𝟎,𝟏,𝟏)\)</span>，哪个奖励序列更好？</p>
<ol type="1">
<li><strong>反馈 Return</strong>：反应累加奖励，<span
class="math inline">\(G_t=R_{t+1}+\gamma R_{t+w}+\gamma^2
R_{t+3}+...\)</span></li>
<li><strong>折扣系数 discount factor</strong>：<span
class="math inline">\(\gamma \in [0,1]\)</span></li>
<li>反馈值反映了某个时刻后所得到累加奖励，当衰退系数小于1时，越是遥远的未来对累加反馈的贡献越少</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230517104754931.png" alt="image-20230517104754931" style="zoom:80%;" /></p>
</blockquote>
<h4 id="马尔可夫决策过程-markov-decision-process引入动作">6.1.3.3
马尔可夫决策过程 Markov Decision Process：引入动作</h4>
<p>在强化学习问题中，智能主体与环境交互过程中可自主决定所采取的动作，不同<strong>动作</strong>会对环境产生不同影响，为此：</p>
<ol type="1">
<li>定义智能主体能够采取的<strong>动作集合</strong>为<span
class="math inline">\(𝑨\)</span></li>
<li>由于不同的动作对环境造成的影响不同，因此状态转移概率定义为<span
class="math inline">\(𝑷(𝑺_{𝒕+𝟏}│𝑺_𝒕,𝒂_𝒕 )\)</span>，其中<span
class="math inline">\(𝒂_𝒕∈𝑨\)</span>为第<span
class="math inline">\(𝒕\)</span>步采取的动作</li>
<li>奖励可能受动作的影响，因此修改奖励函数为<span
class="math inline">\(𝑹(𝑺_𝒕,𝒂_𝒕,𝑺_{𝒕+𝟏})\)</span></li>
<li>动作集合<span
class="math inline">\(𝑨\)</span>可以是有限的，也可以是无限的</li>
<li>状态转移可是确定（deterministic）的，也可以是随机概率性（stochastic）的。</li>
<li>确定状态转移相当于发生从<span
class="math inline">\(𝑺_𝒕\)</span>到<span
class="math inline">\(𝑺_{𝒕+𝟏}\)</span>的转移概率为1</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230517105910270.png" alt="image-20230517105910270" style="zoom:80%;" /></p>
<h4 id="使用离散马尔可夫过程描述机器人移动问题">6.1.3.4
使用离散马尔可夫过程描述机器人移动问题</h4>
<p><img src="/images/AssetMarkdown/image-20230517110515225.png" alt="image-20230517110515225" style="zoom:50%;" /></p>
<ol type="1">
<li>随机变量序列<span
class="math inline">\(\{S_t\}_{t=0,1,2,...}\)</span>：<span
class="math inline">\(S_t\)</span>表示机器人第<span
class="math inline">\(t\)</span>步的位置(即状态)，每个随机变量<span
class="math inline">\(S_t\)</span>的取值范围为<span
class="math inline">\(S={s_1,s_2,⋯, s_9,s_d}\)</span></li>
<li>动作集合：<span class="math inline">\(A=\{上,右\}\)</span></li>
<li>状态转移概率<span class="math inline">\(P(S_{t+1}
|S_t,a_t)\)</span>：满足马尔可夫性，其中<span
class="math inline">\(a_t\in A\)</span>，状态转移如图所示</li>
<li>奖励函数：<span class="math inline">\(R(S_t,a_t,S_{t+𝟏}
)\)</span>，从<span class="math inline">\(S_t\)</span>采取行动<span
class="math inline">\(a_t\)</span>到<span
class="math inline">\(S_{t+1}\)</span>所获得奖励</li>
<li>衰退系数：<span class="math inline">\(\gamma∈[0, 1]\)</span></li>
<li>综上，可以使用<span class="math inline">\(MDP=\{S, A, P, R,
\gamma\}\)</span>来刻画马尔可夫决策过程</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230517110654230.png" alt="image-20230517110654230" style="zoom:67%;" /></p>
<p>马尔可夫过程中产生的状态序列称为<strong>轨迹(trajectory)</strong>，可以如下表示：
<span class="math display">\[
(S_0,a_0,R_1,S_1,a_1,R_2,...,S_T)
\]</span></p>
<ol type="1">
<li>轨迹长度可以是无限的，也可以有终止状态<span
class="math inline">\(S_T\)</span></li>
<li>有终止状态的问题叫做<strong>分段的(episodic,
即存在回合的)</strong>，否则叫做<strong>持续的(continuing)</strong></li>
<li>分段问题中，一个从初始状态到终止状态的完整轨迹称为一个<strong>片段或回合(episode)</strong>。如围棋对弈中一个胜败对局为一个回合</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230517110917886.png" alt="image-20230517110917886" style="zoom:80%;" /></p>
</blockquote>
<h3 id="强化学习中的策略学习">6.1.4 强化学习中的策略学习</h3>
<p><strong>策略函数</strong>：</p>
<ol type="1">
<li>策略函数<span class="math inline">\(𝜋:𝑆×𝐴↦[0, 1]\)</span>，其中<span
class="math inline">\(𝜋(𝑠,𝑎)\)</span>的值表示在状态<span
class="math inline">\(s\)</span>下采取动作<span
class="math inline">\(a\)</span>的概率。</li>
<li>策略函数的输出可以是确定的，即给定<span
class="math inline">\(s\)</span>情况下，只有一个动作<span
class="math inline">\(a\)</span>使得概率<span
class="math inline">\(𝜋(𝑠,𝑎)\)</span>取值为1</li>
<li>对于确定的策略，记为<span class="math inline">\(𝑎=𝜋(𝑠)\)</span></li>
</ol>
<h3 id="强化学习问题定义-1">6.1.5 强化学习问题定义</h3>
<ol type="1">
<li>给定一个马尔可夫决策过程<span
class="math inline">\(MDP=(S,A,P,R,\gamma)\)</span>，学习一个最优策略<span
class="math inline">\(\pi^*\)</span>，对于任意<span
class="math inline">\(s\in S\)</span>，使得<span
class="math inline">\(V_{\pi^*}(s)\)</span>的值最大
<ol type="1">
<li><strong>策略函数</strong>：<span class="math inline">\(𝜋:𝑆×𝐴↦[0,
1]\)</span>，<span
class="math inline">\(𝜋(𝑠,𝑎)\)</span>表示智能体在状态<span
class="math inline">\(𝑠\)</span>下采取动作<span
class="math inline">\(𝑎\)</span>的概率</li>
<li>最大化每一时刻的<strong>回报值</strong>：<span
class="math inline">\(G_t=R_{t+1}+\gamma R_{t+w}+\gamma^2
R_{t+3}+...\)</span></li>
<li><strong>价值函数</strong>：<span
class="math inline">\(𝑉:𝑆↦R\)</span>，其中<span
class="math inline">\(𝑉_𝜋 (𝑠)=𝔼_𝜋 [𝐺_𝑡 |𝑆_𝑡=𝑠]\)</span>
<ol type="1">
<li>表示在第<span class="math inline">\(t\)</span>步状态为<span
class="math inline">\(s\)</span>时，按照策略<span
class="math inline">\(\pi\)</span>行动后，在未来所获得反馈值的期望</li>
</ol></li>
<li><strong>动作-价值函数</strong>：<span
class="math inline">\(𝑞:𝑆×𝐴↦R\)</span>，其中<span
class="math inline">\(𝑞_𝜋 (𝑠,𝑎)= E_𝜋 [𝐺_𝑡 |𝑆_𝑡=𝑠,𝐴_𝑡=𝑎]\)</span>
<ol type="1">
<li>表示在第𝑡步状态为𝑠时，按照策略𝜋采取动作𝑎后，在未来所获得反馈值的期望</li>
</ol></li>
<li><strong>价值函数</strong>和<strong>动作-价值函数</strong>反映了智能体在某一策略下所对应状态序列<strong>获得回报的期望</strong>，它比回报本身更加准确地刻画了智能体的目标</li>
<li><strong>价值函数</strong>和<strong>动作-价值函数</strong>的定义之所以能够成立，离不开决策过程所具有的马尔可夫性，即当位于当前状态<span
class="math inline">\(s\)</span>时，无论当前时刻<span
class="math inline">\(t\)</span>的取值是多少，一个策略回报值的期望是一定的。<strong>当前状态只与前一状态有关，与时间无关</strong></li>
</ol></li>
</ol>
<h3 id="贝尔曼方程-动态规划方程">6.1.6 贝尔曼方程 &lt;=&gt;
动态规划方程</h3>
<blockquote>
<p><strong>Bellman Equation</strong> &lt;=&gt; <strong>Dynamic
Programming Equation</strong></p>
</blockquote>
<ol type="1">
<li><p><strong>价值函数</strong>：<span
class="math inline">\(V_\pi(s)=E_\pi [R_{t+1}+\gamma R_{t+2}+\gamma^2
R_{t+3}+... |S_t=s]\)</span></p></li>
<li><p><strong>动作-价值函数</strong>：<span
class="math inline">\(q_\pi(s,a)=E_\pi [R_{t+1}+\gamma R_{t+2}+\gamma^2
R_{t+3}+... |S_t=s, A_t=a]\)</span></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230517114027577.png" alt="image-20230517114027577" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230517114407250.png" alt="image-20230517114407250" style="zoom:80%;" /></p>
</blockquote></li>
<li><p><strong>贝尔曼方程</strong>：</p>
<ol type="1">
<li><p><strong>价值函数的贝尔曼方程</strong>： <span
class="math display">\[
V_\pi(s)=\sum_{a\in A}\pi(s,a)q_\pi(s,a)
\]</span>
<img src="/images/AssetMarkdown/image-20230517115250091.png" alt="image-20230517115250091" style="zoom:80%;" /></p>
<ol type="1">
<li>描述了当前状态价值函数和其后续状态价值函数之间的关系</li>
<li>即当前状态价值函数等于瞬时奖励的期望加上后续状态的（折扣）价值函数的期望</li>
</ol></li>
<li><p><strong>动作-价值函数的贝尔曼方程</strong>： <span
class="math display">\[
q_\pi(s,a)=\sum_{s&#39;\in S}P(s&#39;|s,a)[R(s,a,s&#39;)+\gamma
V_\pi(s&#39;)]
\]</span>
<img src="/images/AssetMarkdown/image-20230517115306429.png" alt="image-20230517115306429" style="zoom:80%;" /></p>
<ol type="1">
<li>描述了当前动作-价值函数和其后续动作-价值函数之间的关系</li>
<li>即当前状态下的动作-价值函数等于瞬时奖励的期望加上后续状态的（折扣）动作-价值函数的期望</li>
</ol></li>
</ol></li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230517115337147.png" alt="image-20230517115337147" style="zoom:80%;" /></p>
</blockquote>
<h2 id="基于价值的强化学习">6.2 基于价值的强化学习</h2>
<p>强化学习问题的定义：</p>
<ol type="1">
<li>给定一个马尔可夫决策过程<span
class="math inline">\(MDP=(S,A,P,R,\gamma)\)</span></li>
<li>学习一个最优策略<span
class="math inline">\(\pi^*\)</span>，对于任意<span
class="math inline">\(s\in S\)</span>，使得<span
class="math inline">\(V_{\pi^*}(s)\)</span>的值最大</li>
</ol>
<h3 id="策略迭代">6.2.1 策略迭代</h3>
<p><img src="/images/AssetMarkdown/image-20230517115738367.png" alt="image-20230517115738367" style="zoom:80%;" /></p>
<ol type="1">
<li>从一个任意的策略开始，首先计算该策略下价值函数（或动作-价值函数），然后根据价值函数调整改进策略使其更优，不断迭代这个过程直到策略收敛</li>
<li>通过策略计算价值函数的过程叫做<strong>策略评估（policy
evaluation）</strong></li>
<li>通过价值函数优化策略的过程叫做<strong>策略优化（policy
improvement）</strong></li>
<li>策略评估和策略优化交替进行的强化学习求解方法叫做<strong>通用策略迭代（Generalized
Policy Iteration，GPI）</strong></li>
</ol>
<h3 id="强化学习中的策略优化">6.2.2 强化学习中的策略优化</h3>
<p><strong>策略优化定理</strong>：</p>
<ol type="1">
<li>对于确定的策略<span class="math inline">\(\pi\)</span>和<span
class="math inline">\(\pi&#39;\)</span>，如果对于任意状态<span
class="math inline">\(s\in S\)</span>，均有<span
class="math inline">\(q_\pi(s,\pi&#39;(s)) \ge
q_\pi(s,\pi(s))\)</span></li>
<li>那么对于任意状态<span class="math inline">\(s\in S\)</span>，有<span
class="math inline">\(V_{\pi&#39;}(s) \ge V_{\pi}(s)\)</span></li>
</ol>
<p>给定当前策略<span class="math inline">\(\pi\)</span>、价值函数<span
class="math inline">\(V_\pi\)</span>、行动-价值函数<span
class="math inline">\(q_\pi\)</span>，构造新策略<span
class="math inline">\(\pi&#39;\)</span>满足如下条件： <span
class="math display">\[
\forall s \in S,\pi&#39;(s)=argmax_a\ q_{\pi}(s,a)
\]</span></p>
<ol type="1">
<li>则<span class="math inline">\(\pi&#39;\)</span>就是对<span
class="math inline">\(\pi\)</span>的一个改进</li>
<li>于是对于<span class="math inline">\(\forall s \in
S\)</span>，有：</li>
</ol>
<p><span class="math display">\[
q_\pi(s,\pi&#39;(s))=q_\pi(s,argmax_a\ q_\pi(s,a))=max_{a}\ q_\pi(s,a)
\ge q_\pi(s,\pi(s))
\]</span></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230517120713430.png" alt="image-20230517120713430" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230517120730339.png" alt="image-20230517120730339" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230517120756408.png" alt="image-20230517120756408" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230517120829356.png" alt="image-20230517120829356" style="zoom:80%;" /></p>
</blockquote>
<h3 id="强化学习中的策略评估方法">6.2.3 强化学习中的策略评估方法</h3>
<p>假定当前策略为<span
class="math inline">\(𝜋\)</span>，<strong>策略评估</strong>指的是：根据策略<span
class="math inline">\(𝜋\)</span>来计算相应的价值函数<span
class="math inline">\(𝑉_𝜋\)</span>或动作-价值函数<span
class="math inline">\(𝑞_𝜋\)</span>。</p>
<p>这里将介绍在状态集合有限前提下三种常见的策略评估方法，它们分别是</p>
<ol type="1">
<li>动态规划</li>
<li>蒙特卡洛采样</li>
<li>时序差分（temporal difference）</li>
</ol>
<h4 id="动态规划">6.2.3.1 动态规划</h4>
<p>基于动态规划的价值函数更新：使用<strong>迭代</strong>的方法<strong>求解贝尔曼方程组</strong></p>
<p><img src="/images/AssetMarkdown/image-20230524103752806.png" alt="image-20230524103752806" style="zoom:80%;" /></p>
<p>缺点：</p>
<ol type="1">
<li>agent需要事先知道状态转移概率<span
class="math inline">\(Pr(s&#39;|s,a)\)</span></li>
<li>无法处理状态集合大小无限的情况</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230524104106939.png" alt="image-20230524104106939" style="zoom:80%;" /></p>
</blockquote>
<h4 id="蒙特卡洛采样">6.2.3.2 蒙特卡洛采样</h4>
<p>基于蒙特卡洛采样的价值函数更新：</p>
<ol type="1">
<li>轨迹：从当前状态出发，不停采样，直到到达终止状态</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230524104237120.png" alt="image-20230524104237120" style="zoom:80%;" /></p>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230524104717419.png" alt="image-20230524104717419" style="zoom:80%;" /></p>
</blockquote>
<p>优点：</p>
<ol type="1">
<li>agent不必知道状态转移的概率</li>
<li>容易扩展到无限状态集合的问题中</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>状态集合比较大时，一个状态在轨迹可能非常稀疏，不利于估计期望</li>
<li>在实际问题中，最终反馈需要在终止状态才能知晓，导致反馈周期较长</li>
</ol>
<h4 id="时序差分-temporal-difference">6.2.3.3 时序差分 Temporal
Difference</h4>
<p><span class="math display">\[
\begin{aligned}
V_\pi(s) &amp;← (1-\alpha)V_\pi(s) + \alpha[R+\gamma V_\pi(s&#39;)]\\
         &amp;= V_\pi(s) + \alpha[R+\gamma V_\pi(s&#39;)-V_\pi(s)]
\end{aligned}
\]</span></p>
<p><img src="/images/AssetMarkdown/image-20230524111034692.png" alt="image-20230524111034692" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524111010163.png" alt="image-20230524111010163" style="zoom:80%;" /></p>
<ol type="1">
<li><strong>时序差分法</strong>可以看作<strong>蒙特卡罗方法</strong>和<strong>动态规划方法</strong>的有机结合
<ol type="1">
<li>时序差分算法与蒙特卡洛方法相似之处在于：时序差分方法从实际经验中获取信息，无需提前获知环境模型的全部信息</li>
<li>时序差分算法与动态规划方法的相似之处在于：时序差分方法能够利用前序已知信息来进行在线实时学习，无需等到整个片段结束（终止状态抵达）再进行价值函数的更新</li>
</ol></li>
<li>动态规划法根据贝尔曼方程迭代更新价值函数，要求算法事先知道状态之间的转移概率，这往往是不现实的。为了解决这个问题，时序差分法借鉴蒙特卡洛法思想，通过<strong>采样动作<span
class="math inline">\(a\)</span>和下一状态<span
class="math inline">\(s&#39;\)</span>来估计计算<span
class="math inline">\(V_\pi(s)\)</span></strong>
<ol type="1">
<li>由于通过采样进行计算，所得结果可能不准确，因此时序差分法并没有将这个估计值照单全收，而是<strong>以<span
class="math inline">\(\alpha\)</span>作为权重来接受新的估计值</strong>，即把价值函数更新为<span
class="math inline">\((1−𝛼)𝑉_𝜋(𝑠)+𝛼[𝑅+𝛾𝑉_𝜋 (s&#39;)]\)</span></li>
<li>对这个式子稍加整理就能得到算法中的形式：<span
class="math inline">\(𝑉_𝜋(𝑠)←𝑉_𝜋(𝑠)+𝛼[𝑅+𝛾𝑉_𝜋(s&#39;)−𝑉_𝜋(𝑠)]\)</span>
<ol type="1">
<li>时序差分目标：<span
class="math inline">\(𝑅+𝛾𝑉_𝜋(𝑠&#39;)\)</span></li>
<li>时序差分偏差：<span
class="math inline">\(𝑅+𝛾𝑉_𝜋(𝑠&#39;)−𝑉_𝜋(𝑠)\)</span></li>
</ol></li>
</ol></li>
<li>时序差分法和蒙特卡洛法都是通过采样若干个片段来进行价值函数更新的，但是时序差分法并非使用一个片段中的终止状态所提供的实际回报值来估计价值函数，而是<strong>根据下一个状态的价值函数来估计价值函数</strong>，这样就克服了采样轨迹的稀疏性可能带来样本方差较大的不足问题，同时也缩短了反馈周期</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230524110051895.png" alt="image-20230524110051895" style="zoom:80%;" /></p>
</blockquote>
<h4 id="q-learning直接计算q_pi">6.2.3.4 Q-learning：直接计算<span
class="math inline">\(q_\pi\)</span></h4>
<p><span class="math display">\[
\begin{aligned}
q_\pi(s,a) &amp;← (1-\alpha)q_\pi(s,a) + \alpha[R+\gamma \max_{a&#39;}
q_\pi(s&#39;,a&#39;)]\\
           &amp;= q_\pi(s,a) + \alpha[R+\gamma \max_{a&#39;}
q_\pi(s&#39;,a&#39;)-q_\pi(s,a)]
\end{aligned}
\]</span></p>
<p><img src="/images/AssetMarkdown/image-20230524110407594.png" alt="image-20230524110407594" style="zoom:80%;" /></p>
<ol type="1">
<li>Q-learning中<strong>直接记录和更新动作-价值函数<span
class="math inline">\(q_\pi\)</span></strong>
<ol type="1">
<li>这是因为策略优化要求已知动作-价值函数<span
class="math inline">\(q_\pi\)</span>，如果算法仍然记录值函数<span
class="math inline">\(V_\pi\)</span>，在不知道状态转移概率的情况下将无法求出<span
class="math inline">\(q_\pi\)</span></li>
<li>于是，Q-learning中，只有动作-价值函数（即q函数）参与计算</li>
</ol></li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230524111115648.png" alt="image-20230524111115648" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524111206856.png" alt="image-20230524111206856" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524111559060.png" alt="image-20230524111559060" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524111640888.png" alt="image-20230524111640888" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524111811375.png" alt="image-20230524111811375" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524112203863.png" alt="image-20230524112203863" style="zoom:80%;" /></p>
</blockquote>
<h4 id="ε-greedy策略策略学习中探索与利用的平衡">6.2.3.5
ε-greedy策略：策略学习中探索与利用的平衡</h4>
<p><img src="/images/AssetMarkdown/image-20230524112703162.png" alt="image-20230524112703162" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230619221300001.png" alt="image-20230619221300001" style="zoom:80%;" /></p>
<ol type="1">
<li>ε-greedy策略：大体上遵循最优策略的决定，偶尔(以ε的概率)进行探索
<ol type="1">
<li>如上一个例子中，如果偶尔在某些状态随机选择"向右移动一个方格"的动作，则可克服机器人无法走到终点<span
class="math inline">\(s_9\)</span>这一不足</li>
</ol></li>
<li>将动作采样从"确定地选取最优动作" 改为
”按照ε-greedy策略选取动作“</li>
<li>更新时仍保持用max操作选取最佳策略</li>
<li>像这样，更新时的目标策略与采样策略不同的方法，称为<strong>离策略
off-policy</strong>方法</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230524113241486.png" alt="image-20230524113241486" style="zoom:80%;" /></p>
</blockquote>
<h4 id="deep-q-learning用神经网络拟合q_pi">6.2.3.6 Deep
Q-learning：用神经网络拟合<span
class="math inline">\(q_\pi\)</span></h4>
<p>思路：将<strong>q函数参数化</strong>，用一个非线性回归模型来拟合q函数</p>
<ol type="1">
<li>能够用有限的参数刻画无限的状态</li>
<li>由于回归函数的连续性，没有探索过的状态也可以通过周围状态来估计</li>
</ol>
<p><img src="/images/AssetMarkdown/image-20230524113539570.png" alt="image-20230524113539570" style="zoom:80%;" /></p>
<ol type="1">
<li>损失函数刻画了<strong>q的估计值<span class="math inline">\(R+\gamma\
max_{a&#39;}q_\pi(s&#39;,a&#39;;\theta)\)</span>与当前值的平方误差</strong></li>
<li>利用梯度下降算法优化参数<span
class="math inline">\(\theta\)</span></li>
<li>如果使用深度神经网络来拟合q函数，则该算法称为<strong>Deep
Q-learning/深度强化学习</strong></li>
</ol>
<h2 id="不考6.3-基于策略的强化学习">(不考)6.3 基于策略的强化学习</h2>
<p>基于价值的强化学习：</p>
<ol type="1">
<li>以<strong>对价值函数/动作-价值函数的建模</strong>为核心</li>
</ol>
<p>基于策略的强化学习：</p>
<ol type="1">
<li>直接<strong>参数化策略函数</strong>，求解参数化的策略函数的梯度</li>
<li>策略函数的参数化可以表示为<span class="math inline">\(𝝅_𝜽
(𝒔,𝒂)\)</span>，其中<span
class="math inline">\(𝜽\)</span>为一组参数，函数取值表示在状态<span
class="math inline">\(𝒔\)</span>下选择动作<span
class="math inline">\(𝒂\)</span>的概率</li>
<li>和Q学习的<span
class="math inline">\(𝝐\)</span>贪心策略相比，这种参数化的一个显著好处是：选择一个动作的概率是随着参数的改变而光滑变化的，实际上这种光滑性对算法收敛有更好的保证。</li>
</ol>
<h3 id="不考6.3.1-策略梯度定理">(不考)6.3.1 策略梯度定理</h3>
<p>最大化目标： <span class="math display">\[
MAX\ J(\theta)=V_{\pi_\theta}(s_0), 从初始状态出发的价值函数
\]</span> 策略梯度定理： <span class="math display">\[
∇_\theta J(\theta) \propto \sum_s\mu_{\pi_\theta}(s)\sum_a
q_{\pi_\theta}(s,a)∇_\theta\pi_\theta(s,a)
\]</span>
<img src="/images/AssetMarkdown/image-20230524120505284.png" alt="image-20230524120505284" style="zoom:80%;" /></p>
<h3 id="不考6.3.2-基于蒙特卡洛采样的策略梯度法">(不考)6.3.2
基于蒙特卡洛采样的策略梯度法</h3>
<p><img src="/images/AssetMarkdown/image-20230524120622673.png" alt="image-20230524120622673" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524121005545.png" alt="image-20230524121005545" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524121431806.png" alt="image-20230524121431806" style="zoom:80%;" /></p>
<figure>
<img src="/images/AssetMarkdown/image-20230524121443343.png"
alt="image-20230524121443343" />
<figcaption aria-hidden="true">image-20230524121443343</figcaption>
</figure>
<h3 id="不考6.3.3-基于时序差分的策略梯度法actor-critic">(不考)6.3.3
基于时序差分的策略梯度法：Actor-Critic</h3>
<p><img src="/images/AssetMarkdown/image-20230524121517685.png" alt="image-20230524121517685" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524121527223.png" alt="image-20230524121527223" style="zoom:80%;" /></p>
<h2 id="不考6.4-深度强化学习的应用">(不考)6.4 深度强化学习的应用</h2>
<h3 id="不考6.4.1-deep-q-learning围棋博弈">(不考)6.4.1 Deep
Q-Learning：围棋博弈</h3>
<blockquote>
<p>围棋游戏的一个片段的轨迹</p>
</blockquote>
<p><img src="/images/AssetMarkdown/image-20230524121625616.png" alt="image-20230524121625616" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524121642810.png" alt="image-20230524121642810" style="zoom:80%;" /></p>
<h3 id="不考6.4.2-deep-q-learning雅达利游戏">(不考)6.4.2 Deep
Q-Learning：雅达利游戏</h3>
<p><img src="/images/AssetMarkdown/image-20230524121731339.png" alt="image-20230524121731339" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230524121902690.png" alt="image-20230524121902690" style="zoom:80%;" /></p>
<h3 id="不考6.4.3-难以探索的例子">(不考)6.4.3 难以探索的例子</h3>
<p><img src="/images/AssetMarkdown/image-20230524121930690.png" alt="image-20230524121930690" style="zoom:80%;" /></p>
<h2 id="强化学习的分类">6.5 强化学习的分类</h2>
<p><img src="/images/AssetMarkdown/image-20230524122000911.png" alt="image-20230524122000911" style="zoom:80%;" /></p>
<p>图中从两个角度对强化学习算法做了分类 1.
其中依靠对环境（即马尔可夫随机过程）的先验知识或建模的算法称为<strong>基于模型（Model-based）的方法</strong>，反之称为<strong>无模型的方法（Model-free）</strong>
2.
只对价值函数建模并利用策略优化定理求解的方法称为<strong>基于价值（Value-based）的方法</strong>，对策略函数建模并利用策略梯度定理求解的方法称为<strong>基于策略（Policy-based）的方法</strong></p>
<h1 id="七人工智能博弈">七、人工智能博弈</h1>
<h2 id="博弈论的相关概念">7.1 博弈论的相关概念</h2>
<h3 id="博弈的要素">7.1.1 博弈的要素</h3>
<ol type="1">
<li><strong>参与者/玩家 player</strong>：参与博弈的决策主体</li>
<li><strong>策略
strategy</strong>：参与者可以采取的行动方案，是在采取行动之前就已经准备好的完整方案
<ol type="1">
<li>每个参与者可采纳策略的全体组合形成了<strong>策略集 strategy
set</strong></li>
<li>所有参与者各自采取行动后形成的状态被称为<strong>局势
outcome</strong></li>
<li>如果参与者可以通过一定概率分布来选择若干个不同的策略，这样的策略称为<strong>混合策略
mixed strategy</strong>。</li>
<li>若参与者每次行动都选择某个确定的策略，这样的策略称为<strong>纯策略
pure strategy</strong></li>
</ol></li>
<li><strong>收益 payoff</strong>：各个参与者在不同局势下得到的利益
<ol type="1">
<li>混合策略意义下的收益应为<strong>期望收益 expected
payoff</strong></li>
</ol></li>
<li><strong>规则
rule</strong>：对参与者行动的先后顺序、参与者获得信息多少等内容的规定</li>
</ol>
<h3 id="研究范式">7.1.2 研究范式</h3>
<p><strong>研究范式</strong>：建模者对参与者规定可采取的策略集和取得的收益，观察当参与者选择若干策略以最大化其收益时会产生什么结果</p>
<h3 id="囚徒困境-prisoners-dilemma">7.1.3 囚徒困境 Prisoner's
Dilemma</h3>
<p><img src="/images/AssetMarkdown/image-20230531105937279.png" alt="image-20230531105937279" style="zoom:80%;" /></p>
<ol type="1">
<li><strong>均衡解</strong>：在当前情况下，任意一个人改变策略，而其他人保持不变，改变策略的人得到的收益一定变低</li>
</ol>
<h3 id="博弈的分类">7.1.4 博弈的分类</h3>
<p>合作博弈与非合作博弈</p>
<ol type="1">
<li><strong>合作博弈 cooperative
game</strong>：部分参与者可以组成联盟以获得更大的收益</li>
<li><strong>非合作博弈 non-cooperative
game</strong>：参与者在决策中都彼此独立，不事先达成合作意向</li>
</ol>
<p>静态博弈与动态博弈</p>
<ol type="1">
<li><strong>静态博弈 static
game</strong>：所有参与者同时决策，或参与者互相不知道对方的决策</li>
<li><strong>动态博弈 dynamic
game</strong>：参与者所采取行为的先后顺序由规则决定，且后行动者知道先行动者所采取的行为</li>
</ol>
<p>完全信息博弈与不完全信息博弈</p>
<ol type="1">
<li><strong>完全信息 complete
information</strong>：所有参与者均了解其他参与者的策略集、收益等信息</li>
<li><strong>不完全信息 incomplete
information</strong>：并非所有参与者均掌握了所有信息</li>
</ol>
<p>囚徒困境是一种非合作、不完全信息的静态博弈</p>
<h3 id="纳什均衡">7.1.5 纳什均衡</h3>
<p>博弈的稳定局势即为<strong>纳什均衡 Nash equilibrium</strong>：</p>
<ol type="1">
<li>指的是参与者所作出的这样一种策略组合，在该策略组合上，任何参与者单独改变策略都不会得到好处</li>
<li>换句话说，如果在一个策略组合上，当所有其他人都不改变策略时，没有人会改变自己的策略，则该策略组合就是一个纳什均衡</li>
<li>纳什均衡的本质：不后悔</li>
</ol>
<p><strong>Nash定理</strong>：</p>
<ol type="1">
<li>若参与者有限，每位参与者的策略集有限，收益函数为实值函数，则博弈必<strong>存在</strong>混合策略意义下的纳什均衡</li>
</ol>
<p>囚徒困境中两人同时认罪就是这一问题的纳什均衡</p>
<p><strong>混合策略纳什均衡</strong>：</p>
<ol type="1">
<li>博弈过程中，博弈方通过概率形式随机从可选策略中选择一个策略而达到的纳什均衡</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230531111906949.png" alt="image-20230531111906949" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230531111834818.png" alt="image-20230531111834818" style="zoom:80%;" /></p>
<p>设雇主检查的概率为<span
class="math inline">\(\alpha\)</span>，雇员偷懒的概率为<span
class="math inline">\(\beta\)</span>，则雇主-雇员之间博弈的期望收益为：</p>
<p><img src="/images/AssetMarkdown/image-20230531112006083.png" alt="image-20230531112006083" style="zoom:80%;" /></p>
<p><strong>纳什均衡</strong>：其他参与者策略不变的情况下，某个参与者单独采取其他策略都不会使得收益增加⇔<strong>无论雇主是否检查，雇主的收益都一样；无论雇员是否偷懒，雇员的收益都一样</strong></p>
<ol type="1">
<li>于是有<span class="math inline">\(𝑻_𝟏=𝑻_𝟐\)</span> 以及<span
class="math inline">\(𝑻_𝟑=𝑻_𝟒\)</span></li>
<li>在纳什均衡下，由于<span
class="math inline">\(𝑻_𝟑=𝑻_𝟒\)</span>，可知雇主采取检查策略的概率（雇主趋向于用这个概率去检查）：<span
class="math inline">\(𝜶=\frac{𝑯}{𝑾+𝑭}\)</span></li>
<li>在纳什均衡下，由于<span
class="math inline">\(𝑻_𝟏=𝑻_𝟐\)</span>，可知雇员采取偷懒策略的概率（雇员趋向于用这个概率去偷懒）：<span
class="math inline">\(𝜷=\frac{𝑪}{𝑾+𝑭}\)</span></li>
<li>在检查概率为𝜶之下，雇主的收益：<span
class="math inline">\(𝑻_𝟏=𝑻_𝟐=𝑽−𝑾−\frac{𝑪𝑽}{𝑾+𝑭}\)</span></li>
<li>对上式中<span class="math inline">\(𝑾\)</span>求导，则当<span
class="math inline">\(𝑾=\sqrt{𝑪𝑽}−𝑭\)</span>时，雇主的收益最大，其值为<span
class="math inline">\(𝑻_{𝒎𝒂𝒙}=𝑽−𝟐\sqrt{𝑪𝑽}+𝑭\)</span></li>
</ol>
</blockquote>
<h3 id="策梅洛定理">7.1.6 策梅洛定理</h3>
<p><strong>策梅洛定理 Zermelo's theorem</strong>：</p>
<ol type="1">
<li>对于任意一个<strong>有限步</strong>的<strong>双人完全信息零和</strong>动态博弈，一定存在<strong>先手必胜</strong>策略或<strong>后手必胜</strong>策略或<strong>双方保平</strong>策略</li>
<li>策梅洛定理仅对<strong>两人博弈</strong>有效，如果博弈竞技者超过了2人，如对于三人博弈，策梅洛定理无法保证三方中一定有一方获胜、其他两方必败或者三方和局的策略</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230531113435683.png" alt="image-20230531113435683" style="zoom:80%;" /></p>
</blockquote>
<h2 id="不考7.2-博弈策略求解">(不考)7.2 博弈策略求解</h2>
<p>动机</p>
<ol type="1">
<li>博弈论提供了许多问题的数学模型</li>
<li>纳什定理确定了博弈过程问题存在解</li>
<li>人工智能的方法可用来求解均衡局面或者最优策略</li>
</ol>
<p>主要问题</p>
<ol type="1">
<li>如何高效求解博弈参与者的策略以及博弈的均衡局势？</li>
</ol>
<p>应用领域</p>
<ol type="1">
<li>大规模搜索空间的问题求解：围棋</li>
<li>非完全信息博弈问题求解：德州扑克</li>
<li>网络对战游戏智能：Dota、星球大战</li>
<li>动态博弈的均衡解：厂家竞争、信息安全</li>
</ol>
<h3 id="不考7.2.1-虚拟遗憾最小化算法-regret-minimization">(不考)7.2.1
虚拟遗憾最小化算法 Regret Minimization</h3>
<ol type="1">
<li><p>对于一个有<span
class="math inline">\(N\)</span>个玩家参加的博弈，玩家<span
class="math inline">\(i\)</span>在博弈中采取的策略为<span
class="math inline">\(\sigma_i\)</span></p></li>
<li><p>对于所有玩家来说，他们的所有策略构成了一个策略组合，记为<span
class="math inline">\(\sigma=\{\sigma_1,\sigma_2,...,\sigma_N\}\)</span></p></li>
<li><p>策略组中，除玩家<span
class="math inline">\(i\)</span>外，其它玩家的策略组合记为<span
class="math inline">\(\sigma_{-i}=\{\sigma_1,\sigma_2,...,\sigma_{i-1},\sigma_{i+1},...,\sigma_N\}\)</span></p></li>
<li><p>给定策略组合<span
class="math inline">\(\sigma\)</span>，玩家<span
class="math inline">\(i\)</span>在终结局势下的收益记为<span
class="math inline">\(u_i(\sigma)\)</span></p></li>
<li><p>在给定其它玩家的策略组合<span
class="math inline">\(\sigma_{-i}\)</span>的情况下，对玩家<span
class="math inline">\(i\)</span>而言的<strong>最优反应策略<span
class="math inline">\(\sigma_i^*\)</span></strong>满足如下条件： <span
class="math display">\[
u_i(\sigma_i^*,\sigma_{-i}) \ge \max_{\sigma_i&#39;\in
\sum_i}u_i(\sigma_i&#39;,\sigma_{-i})
\]</span></p>
<ol type="1">
<li><span class="math inline">\(\sum_i\)</span>是玩家<span
class="math inline">\(i\)</span>可以选择的所有策略</li>
<li>如上条件表示当玩家<span
class="math inline">\(i\)</span>采用最优反应策略时，玩家<span
class="math inline">\(i\)</span>能够获得最大收益</li>
</ol></li>
</ol>
<h4 id="不考7.2.1.1-纳什均衡策略的定义">(不考)7.2.1.1
纳什均衡策略的定义</h4>
<ol type="1">
<li><p>在策略组合<span
class="math inline">\(𝜎^∗\)</span>中，如果每个玩家的策略相对于其他玩家的策略而言都是最佳反应策略，那么策略组合<span
class="math inline">\(𝜎^∗\)</span>就是一个<strong>纳什均衡策略</strong></p>
<ol type="1">
<li>在有限对手、有限策略情况下，纳什均衡一定存在</li>
<li>策略组<span
class="math inline">\(𝜎^∗=\{𝜎_1^∗,𝜎_2^∗,…,𝜎_𝑁^∗\}\)</span>对任意玩家<span
class="math inline">\(𝑖=1,..,𝑁\)</span>，满足如下条件：</li>
</ol></li>
</ol>
<p><span class="math display">\[
   𝑢_𝑖(𝜎^∗) ≥ \max_{𝜎_𝑖^′∈Σ_𝑖} 𝜇_𝑖 (𝜎_1^∗,𝜎_2^∗,…,𝜎_𝑖^′,…,𝜎_𝑁^∗)
\]</span></p>
<ol start="2" type="1">
<li><p>在博弈策略求解的过程中，希望求解得到<strong>每个玩家最优反应策略</strong>，若所有玩家都是理性的，则算法求解最优反应策略就是一个纳什均衡</p></li>
<li><p>考虑到计算资源有限这一前提，难以通过遍历博弈中所有策略组合来找到一个最优反应策略，因此需要找到一种能<strong>快速发现近似纳什均衡</strong>的方法</p></li>
</ol>
<h4 id="不考7.2.1.2-遗憾最小化算法">(不考)7.2.1.2 遗憾最小化算法</h4>
<p>遗憾最小化算法是一种<strong>根据以往博弈过程中所得遗憾程度来选择未来行为</strong>的方法</p>
<ol type="1">
<li><p>在过去<span class="math inline">\(𝑇\)</span>轮中，玩家<span
class="math inline">\(𝑖\)</span>采取策略<span
class="math inline">\(𝜎_𝑖\)</span>的<strong>累加遗憾值</strong>定义如下：
<span class="math display">\[
Regret_i^T (𝜎_i)=\sum_{𝑡=1}^𝑇 (u_i(𝜎_i,𝜎_{−i}^t) − u_i(𝜎^t))
\]</span></p>
<ol type="1">
<li><span class="math inline">\(𝜎^𝑡\)</span>表示：第<span
class="math inline">\(𝑡\)</span>轮中所有玩家的策略组合</li>
<li><span class="math inline">\(𝜎_{−𝑖}^𝑡\)</span>表示：第<span
class="math inline">\(𝑡\)</span>轮中除了玩家<span
class="math inline">\(𝑖\)</span>以外的策略组合</li>
</ol></li>
<li><p>简单地说，累加遗憾值代表：</p>
<ol type="1">
<li>在过去<span class="math inline">\(𝑇\)</span>轮中，玩家<span
class="math inline">\(𝑖\)</span>在每一轮中选择策略<span
class="math inline">\(𝜎_𝑖\)</span>所得收益与采取其他策略所得收益之差的累加</li>
</ol></li>
</ol>
<h4 id="不考7.2.1.3-有效遗憾值">(不考)7.2.1.3 有效遗憾值</h4>
<ol type="1">
<li><p>在得到玩家<span
class="math inline">\(i\)</span>的所有可选策略的遗憾值后，可以根据遗憾值的大小来选择后续第<span
class="math inline">\(𝑇+1\)</span>轮博弈的策略，这种选择方式被称为<strong>遗憾匹配</strong></p></li>
<li><p>当然，通常遗憾值为负数的策略被认为不能提升下一时刻收益，所以如下定义有效遗憾值：
<span class="math display">\[
𝑅𝑒𝑔𝑟𝑒𝑡_𝑖^{𝑇,+} 𝜎_𝑖=\max (𝑅𝑒𝑔𝑟𝑒𝑡_𝑖^𝑇(𝜎_𝑖),0)
\]</span></p></li>
<li><p>利用有效遗憾值的遗憾匹配，可得到玩家<span
class="math inline">\(𝑖\)</span>在第<span
class="math inline">\(T+1\)</span>轮选择策略<span
class="math inline">\(𝜎_𝑖\)</span>的概率<span
class="math inline">\(𝑃(𝜎_𝑖^{T+1})\)</span>为：</p>
<p><img src="/images/AssetMarkdown/image-20230531115940689.png" alt="image-20230531115940689" style="zoom:80%;" /></p>
<ol type="1">
<li><span class="math inline">\(|Σ_i|\)</span>表示玩家<span
class="math inline">\(𝑖\)</span>所有策略的总数</li>
<li>显然，如果在过往<span class="math inline">\(𝑇\)</span>轮中策略<span
class="math inline">\(𝜎_𝑖\)</span>所带来的遗憾值大、其他策略<span
class="math inline">\(𝜎_𝑖^′\)</span>所带来的遗憾值小，则在第<span
class="math inline">\(𝑇+1\)</span>轮选择策略<span
class="math inline">\(𝜎_𝑖\)</span>的概率值<span
class="math inline">\(𝑃(𝜎_𝑖^{𝑇+1})\)</span>就大</li>
<li>即：<strong>带来越大遗憾值的策略具有更高的价值，因此其在后续被选择的概率就应该越大</strong></li>
<li>如果没有一个能够提升前<span
class="math inline">\(T\)</span>轮收益的策略，则在后续轮次中随机选择一种策略</li>
</ol></li>
<li><p>可以依照一定的概率选择行动</p>
<ol type="1">
<li>为了防止对手发现自己所采取的策略，如采取遗憾值最大的策略</li>
</ol></li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230531120014258.png" alt="image-20230531120014258" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230531120224753.png" alt="image-20230531120224753" style="zoom:80%;" /></p>
</blockquote>
<h4 id="不考7.2.1.4-计算理论">(不考)7.2.1.4 计算理论</h4>
<ol type="1">
<li><p>对于任何序贯决策的博弈对抗，可将博弈过程表示成一棵博弈树，博弈树中的每一个中间节点都是一个<strong>信息集<span
class="math inline">\(I\)</span></strong>，信息集中包含了博弈中当前的状态</p>
<ol type="1">
<li>给定博弈树的每一个节点，玩家都可以从一系列的动作中选择一个，然后状态发生转换，如此周而复始，直到终局（博弈树的叶子节点）</li>
<li>玩家在当前状态下可采取的策略就是当前状态下所有可能动作的一个概率分布</li>
</ol></li>
<li><p>具体而言，在信息集<span
class="math inline">\(𝐼\)</span>下，玩家可以采取的<strong>行动集合记作<span
class="math inline">\(𝐴(𝐼)\)</span></strong></p>
<ol type="1">
<li>玩家<span class="math inline">\(𝑖\)</span>所采取的行动<span
class="math inline">\(𝑎_𝑖∈𝐴(𝐼)\)</span>可认为是其采取的策略<span
class="math inline">\(𝜎_𝑖\)</span>的一部分</li>
<li>在信息集<span class="math inline">\(𝐼\)</span>下采取行动<span
class="math inline">\(𝑎\)</span>所代表的策略记为<span
class="math inline">\(𝜎_{𝐼→𝑎}\)</span></li>
<li>这样，要计算虚拟遗憾值的对象，就是博弈树中每个中间节点在信息集下所采取的行动，并根据遗憾值匹配得到该节点在信息集下应该采取的策略<span
class="math inline">\(𝜎_{𝐼→𝑎}\)</span></li>
</ol></li>
<li><p>在一次博弈中，所有玩家交替采取的<strong>行动序列记为<span
class="math inline">\(ℎ\)</span></strong>（从根节点到当前节点的路径）</p>
<ol type="1">
<li>对于所有玩家的策略组合<span
class="math inline">\(𝜎\)</span>，行动序列ℎ出现的概率记为<span
class="math inline">\(𝜋^𝜎
(ℎ)\)</span>，不同的行动序列可以从根节点到达当前节点的信息集<span
class="math inline">\(𝐼\)</span>（即不同决策路径可到达博弈树中同一个中间节点）</li>
<li>在策略组合<span
class="math inline">\(𝜎\)</span>下，所有能够到达该信息集的行动序列的概率累加就是该信息集的出现概率，即<span
class="math inline">\(𝜋^𝜎(𝐼)=∑_{ℎ∈𝐼}𝜋^𝜎 (ℎ)\)</span></li>
</ol></li>
<li><p>博弈的<strong>终结局势集合</strong>也就是博弈树中叶子节点的集合，记为<span
class="math inline">\(𝑍\)</span></p>
<ol type="1">
<li>对于任意一个终结局势<span
class="math inline">\(𝑧∈𝑍\)</span>，玩家<span
class="math inline">\(𝑖\)</span>在此终点局势下的<strong>收益记作<span
class="math inline">\(𝑢_𝑖 (𝑧)\)</span></strong></li>
<li>给定行动序列<span
class="math inline">\(ℎ\)</span>，依照策略组合<span
class="math inline">\(𝜎\)</span>最终到达终结局势<span
class="math inline">\(𝑧\)</span>的<strong>概率记作<span
class="math inline">\(𝜋^𝜎 (ℎ,𝑧)\)</span></strong></li>
</ol></li>
<li><p>在策略组合<span class="math inline">\(𝜎\)</span>下，对玩家<span
class="math inline">\(𝑖\)</span>而言，如下计算从根节点到当前节点的<strong>行动序列路径<span
class="math inline">\(ℎ\)</span>的虚拟价值<span
class="math inline">\(v_i(σ,h)\)</span></strong>：</p>
<p><img src="/images/AssetMarkdown/image-20230607111149556.png" alt="image-20230607111149556" style="zoom:80%;" /></p>
<ol type="1">
<li>在上式中，<span
class="math inline">\(𝜋_{−𝑖}^𝜎(ℎ)\)</span>表示从根节点出发，不考虑玩家<span
class="math inline">\(𝑖\)</span>的策略，仅考虑其他玩家策略而经过路径<span
class="math inline">\(ℎ\)</span>到达当前节点的概率</li>
<li>也就是说，即使玩家𝑖有其他策略，总是要求玩家<span
class="math inline">\(𝑖\)</span>在每次选择时都选择路径ℎ中对应的动作，以保证从根节点出发能够到达当前节点</li>
<li>可见，行动序列路径ℎ的虚拟价值等于如下三项结果的乘积：不考虑玩家𝑖的策略（仅考虑其他玩家策略）经过路径ℎ到达当前节点的概率、从当前节点走到叶子结点（博弈结束）的概率、所到达叶子节点的收益。</li>
</ol></li>
<li><p>在定义了行动序列路径ℎ的虚拟价值之后，就可如下计算玩家<span
class="math inline">\(𝑖\)</span>在基于路径<span
class="math inline">\(ℎ\)</span>到达当前节点采取行动<span
class="math inline">\(𝑎\)</span>的<strong>遗憾值</strong>： <span
class="math display">\[
𝑟_𝑖 (ℎ,𝑎)=𝑣_𝑖(𝜎_{𝐼→𝑎},ℎ)−𝑣_𝑖 (𝜎,ℎ)
\]</span></p>
<ol type="1">
<li>该遗憾值是玩家<span
class="math inline">\(𝑖\)</span>通过行动序列<span
class="math inline">\(ℎ\)</span>到达当前节点采取行动<span
class="math inline">\(𝑎\)</span>所得虚拟价值减去采用策略<span
class="math inline">\(𝜎\)</span>所得路径<span
class="math inline">\(ℎ\)</span>的虚拟价值</li>
</ol></li>
<li><p>对能够到达同一个信息集<span
class="math inline">\(𝐼\)</span>（即博弈树中同一个中间节点）的所有行动序列的遗憾值进行累加，即可得到信息集<span
class="math inline">\(𝐼\)</span>的遗憾值： <span class="math display">\[
𝑟_𝑖(𝐼,𝑎)=\sum_{ℎ∈𝐼}𝑟_𝑖(ℎ,𝑎)
\]</span></p></li>
<li><p>类似于遗憾最小化算法，虚拟遗憾最小化的遗憾值是𝑇轮重复博弈后的累加值：
<span class="math display">\[
𝑅𝑒𝑔𝑟𝑒𝑡_𝑖^𝑇 (𝐼,𝑎)=∑_{𝑡=1}^𝑇 𝑟_𝑖^𝑡 (𝐼,𝑎)
\]</span></p>
<ol type="1">
<li><span class="math inline">\(𝑟_𝑖^𝑡 (𝐼,𝑎)\)</span>表示玩家<span
class="math inline">\(𝑖\)</span>在第<span
class="math inline">\(𝑡\)</span>轮中于当前节点选择行动<span
class="math inline">\(𝑎\)</span>的遗憾值</li>
</ol></li>
<li><p>进一步可以定义有效虚拟遗憾值： <span class="math display">\[
𝑅𝑒𝑔𝑟𝑒𝑡_𝑖^{𝑇,+}(𝐼,𝑎)=\max(𝑅_𝑖^𝑇 (𝐼,𝑎),0)
\]</span></p></li>
<li><p>根据有效虚拟遗憾值进行遗憾匹配以计算经过<span
class="math inline">\(𝑇\)</span>轮博弈后，玩家<span
class="math inline">\(𝑖\)</span>在信息集<span
class="math inline">\(𝐼\)</span>情况下于后续<span
class="math inline">\(𝑇+1\)</span>轮选择行动<span
class="math inline">\(𝑎\)</span>的概率：</p>
<p><img src="/images/AssetMarkdown/image-20230531121756780.png" alt="image-20230531121756780" style="zoom:80%;" /></p></li>
</ol>
<h4 id="不考7.2.1.5-虚拟最小化算法步骤">(不考)7.2.1.5
虚拟最小化算法步骤</h4>
<p>在虚拟最小化算法的求解过程中，同样需要反复模拟多轮博弈来拟合最佳反应策略，算法步骤如下</p>
<ol type="1">
<li>初始化遗憾值和累加策略表为0</li>
<li>采用随机选择的方法来决定策略</li>
<li>利用当前策略与对手进行博弈</li>
<li>计算每个玩家采取每次行为后的遗憾值</li>
<li>根据博弈结果计算每个行动的累加遗憾值大小来更新策略</li>
<li>重复3)到5)步若干次，不断的优化策略</li>
<li>根据重复博弈最终的策略，完成最终的动作选择</li>
</ol>
<h4 id="不考7.2.1.6-示例">(不考)7.2.1.6 示例</h4>
<p>双人库恩扑克游戏</p>
<ol type="1">
<li><p>规则：</p>
<p><img src="/images/AssetMarkdown/image-20230607104906067.png" alt="image-20230607104906067" style="zoom:80%;" /></p></li>
<li><p>先手玩家A对应的博弈树</p>
<p><img src="/images/AssetMarkdown/image-20230607110005718.png" alt="image-20230607110005718" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230607105608061.png" alt="image-20230607105608061" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230607105558855.png" alt="image-20230607105558855" style="zoom:80%;" /></p></li>
<li><p>计算节点<span
class="math inline">\(\{1PB\}\)</span>的虚拟价值：</p>
<p><img src="/images/AssetMarkdown/image-20230607105808959.png" alt="image-20230607105808959" style="zoom:80%;" /></p></li>
<li><p>计算玩家A在节点<span
class="math inline">\(\{1PB\}\)</span>选择过牌行动后的虚拟价值：</p>
<p><img src="/images/AssetMarkdown/image-20230607110243336.png" alt="image-20230607110243336" style="zoom:80%;" /></p></li>
<li><p>计算玩家A在节点<span
class="math inline">\(\{1PB\}\)</span>选择过牌行动后的虚拟遗憾值：</p>
<p><img src="/images/AssetMarkdown/image-20230607110537655.png" alt="image-20230607110537655" style="zoom:80%;" /></p></li>
</ol>
<h2 id="博弈规则设计">7.3 博弈规则设计</h2>
<h3 id="研究的问题">7.3.1 研究的问题</h3>
<ol type="1">
<li>在现实生活中，如果所有博弈者都追求自己利益最大化，很可能会导致两败俱伤的下场</li>
<li>那么应该如何设计博弈的规则使得博弈的最终局势能尽可能达到整体利益的最大化呢？</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230607111305301.png" alt="image-20230607111305301" style="zoom:80%;" /></p>
</blockquote>
<h3 id="双边匹配算法">7.3.2 双边匹配算法</h3>
<ol type="1">
<li>在生活中，人们常常会碰到与资源匹配相关的决策问题(如求职就业、报考录取等)，这些需要双向选择的情况被称为是<strong>双边匹配问题</strong></li>
<li>在双边匹配问题中，需要双方互相满足对方的需求才会达成匹配</li>
</ol>
<h4 id="稳定婚姻问题-stable-marriage-problem">7.3.2.1 稳定婚姻问题
Stable Marriage Problem</h4>
<p><img src="/images/AssetMarkdown/image-20230607112146858.png" alt="image-20230607112146858" style="zoom:80%;" /></p>
<blockquote>
<p>假设有4名单身男性{𝟏,𝟐,𝟑,𝟒}和4名单身女性{𝑨,𝑩,𝑪,𝑫}，他（她）们的、爱慕序列如表8.9所示</p>
<p><img src="/images/AssetMarkdown/image-20230607111451720.png" alt="image-20230607111451720" style="zoom:80%;" /></p>
<p>按照“修补”策略，匹配和修补过程如下</p>
<p><img src="/images/AssetMarkdown/image-20230607111528939.png" alt="image-20230607111528939" style="zoom:80%;" /></p>
</blockquote>
<h4 id="双边匹配问题g-s算法">7.3.2.2 双边匹配问题：G-S算法</h4>
<p>算法过程：</p>
<ol type="1">
<li>单身男性向最喜欢的女性表白</li>
<li>所有收到表白的女性从向其表白男性中选择最喜欢的男性，暂时匹配</li>
<li>未匹配的男性继续向没有拒绝过他的女性表白</li>
<li>收到表白的女性如果没有完成匹配，则从这一批表白者中选择最喜欢男性。即使收到表白的女性已经完成匹配，但是如果她认为有她更喜欢的男性，则可以拒绝之前的匹配者，重新匹配</li>
<li>如此循环迭代，直到所有人都成功匹配为止</li>
</ol>
<blockquote>
<p><img src="/images/AssetMarkdown/image-20230607111755649.png" alt="image-20230607111755649" style="zoom:80%;" /></p>
<ol type="1">
<li>在第一轮中，4名男性分别向自己最喜欢的女性告白，而收到3人告白的女性𝐴选择了自己最喜欢的男性3，另一个收到告白的女性𝐵选择了男性4</li>
<li>在第二轮中，尚未匹配的男性1和男性2继续向自己第二喜欢的对象告白，收到告白的女性𝐵选择了自己更喜欢的男性2而放弃了男性4</li>
<li>同理继续三轮告白和选择，所有人都找到了自己的伴侣，且所有匹配都是稳定的</li>
<li>可以看出，使用G-S算法得到了稳定匹配的结果。</li>
</ol>
</blockquote>
<h3 id="单边匹配问题最大交易圈算法-top-trading-cycle-ttc">7.3.3
单边匹配问题：最大交易圈算法 Top-trading cycle TTC</h3>
<p>算法流程：</p>
<ol type="1">
<li>首先记录每个物品的初始占有者，或者对物品进行随机分配</li>
<li>每个交易者连接一条指向他最喜欢的物品的边，并从每一个物品连接到其占有者或是具有高优先权的交易者</li>
<li>此时形成一张有向图，且必存在环，这种环被称为“交易圈”，对于交易圈中的交易者，将每人指向节点所代表的物品赋予交易者，同时交易者放弃原先占有的物品，占有者和匹配成功的物品离开匹配市场</li>
<li>接着从剩余的交易者和物品之间重复进行交易圈匹配，直到无法形成交易圈，算法停止</li>
</ol>
<blockquote>
<p><strong>稳定室友匹配</strong>问题就是一个典型的单边匹配问题</p>
<p>假设某寝室有A、B、C、D四位同学和1、2、3、4四个床位，当前给A、B、C、D四位同学随机分配4、3、2、1四个床位</p>
<p><img src="/images/AssetMarkdown/image-20230607112416470.png" alt="image-20230607112416470" style="zoom:80%;" /></p>
<p>第一轮单边匹配：A和D之间构成一个交易圈，可达成交易，所以A得到床位1，D得到床位4，之后将
A和D以及1和4从匹配图中移除</p>
<p><img src="/images/AssetMarkdown/image-20230607112509623.png" alt="image-20230607112509623" style="zoom:80%;" /></p>
<p>第二轮单边匹配：B和C都希望得到床位2，无法再构成交易圈，但是由于C是床位的本身拥有者，所以C仍然得到床位2，B只能选择床位3</p>
<p><img src="/images/AssetMarkdown/image-20230607112551559.png" alt="image-20230607112551559" style="zoom:80%;" /></p>
<p>最后交易结果A→1，B →3，C →2，D →4</p>
</blockquote>
<h2 id="非完全信息博弈的实际应用">7.4 非完全信息博弈的实际应用</h2>
<h3 id="博弈论研究内容">7.4.1 博弈论研究内容</h3>
<p><img src="/images/AssetMarkdown/image-20230607112747539.png" alt="image-20230607112747539" style="zoom:80%;" /></p>
<h3 id="求解非完全信息博弈纳什均衡的一般方法">7.4.2
求解非完全信息博弈纳什均衡的一般方法</h3>
<p><img src="/images/AssetMarkdown/image-20230607112801250.png" alt="image-20230607112801250" style="zoom:80%;" /></p>
<h3 id="与深度强化学习相结合">7.4.3 与深度强化学习相结合</h3>
<p><img src="/images/AssetMarkdown/image-20230607112837994.png" alt="image-20230607112837994" style="zoom:80%;" /></p>
<p><img src="/images/AssetMarkdown/image-20230607112848255.png" alt="image-20230607112848255" style="zoom:80%;" /></p>

    </div>

    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      </div>
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>华丰夏
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/03/01/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" title="人工智能">http://example.com/2023/03/01/人工智能/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%B8%93%E4%B8%9A%E8%AF%BE/" rel="tag"># 专业课</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/03/01/%E5%A4%9A%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF/" rel="prev" title="多媒体技术">
      <i class="fa fa-chevron-left"></i> 多媒体技术
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/01/GAMES104/" rel="next" title="GAMES104">
      GAMES104 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81ODcwNy8zNTE2OQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E9%80%BB%E8%BE%91%E4%B8%8E%E6%8E%A8%E7%90%86"><span class="nav-text">一、逻辑与推理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%91%BD%E9%A2%98%E9%80%BB%E8%BE%91-proposition-logic"><span class="nav-text">1.1 命题逻辑 proposition logic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-text">1.1.1 定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E7%AD%89%E4%BB%B7"><span class="nav-text">1.1.2 逻辑等价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E8%A7%84%E5%88%99"><span class="nav-text">1.1.3 推理规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%BD%E9%A2%98%E8%8C%83%E5%BC%8F"><span class="nav-text">1.1.4 命题范式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%93%E8%AF%8D%E9%80%BB%E8%BE%91"><span class="nav-text">1.2 谓词逻辑</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BD%93%E4%B8%8E%E8%B0%93%E8%AF%8D"><span class="nav-text">1.2.1 个体与谓词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8F%E8%AF%8D"><span class="nav-text">1.2.2 量词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E5%85%83"><span class="nav-text">1.2.3 变元</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E4%B8%8E%E5%8E%9F%E5%AD%90%E8%B0%93%E8%AF%8D%E5%85%AC%E5%BC%8F"><span class="nav-text">1.2.4 项与原子谓词公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%88%E5%BC%8F%E5%85%AC%E5%BC%8F"><span class="nav-text">1.2.5 合式公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E8%A7%84%E5%88%99-1"><span class="nav-text">1.2.6 推理规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%93%E5%AE%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%84%E6%88%90"><span class="nav-text">1.2.7 专家系统的构成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1"><span class="nav-text">1.3 知识图谱</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">1.3.1 基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A8%E7%90%86"><span class="nav-text">1.3.2 知识图谱推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%92%E7%BA%B3%E5%AD%A6%E4%B9%A0"><span class="nav-text">1.3.3 归纳学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E9%98%B6%E6%8E%A8%E5%AF%BC%E5%AD%A6%E4%B9%A0-foil"><span class="nav-text">1.3.4 一阶推导学习 FOIL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%AF%E5%BE%84%E6%8E%92%E5%BA%8F%E6%8E%A8%E7%90%86-pra"><span class="nav-text">1.3.5 路径排序推理 PRA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86"><span class="nav-text">1.3.6 基于分布式的知识推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%80%BB%E8%BE%91%E7%BD%91%E7%BB%9C"><span class="nav-text">1.3.7 马尔可夫逻辑网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.4-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E7%90%86"><span class="nav-text">(不考)1.4 因果推理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.4.1-%E4%B8%89%E7%A7%8D%E5%9B%A0%E6%9E%9C%E6%8E%A8%E7%90%86"><span class="nav-text">(不考)1.4.1 三种因果推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.4.2-%E5%9B%A0%E6%9E%9C%E6%8E%A8%E7%90%86%E7%9A%84%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9E%8B"><span class="nav-text">(不考)1.4.2
因果推理的主要模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.4.3-%E7%BB%93%E6%9E%84%E5%9B%A0%E6%9E%9C%E6%A8%A1%E5%9E%8B"><span class="nav-text">(不考)1.4.3 结构因果模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5-%E5%9B%A0%E6%9E%9C%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="nav-text">(不考)1.5 因果图模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.1-%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-text">(不考)1.5.1 联合概率分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.2-%E9%93%BE"><span class="nav-text">(不考)1.5.2 链</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.3-%E5%88%86%E8%BF%9E"><span class="nav-text">(不考)1.5.3 分连</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.4-%E6%B1%87%E8%BF%9E"><span class="nav-text">(不考)1.5.4 汇连</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.5-d-%E5%88%86%E7%A6%BB"><span class="nav-text">(不考)1.5.5 D-分离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.6-%E5%B9%B2%E9%A2%84%E7%9A%84%E5%9B%A0%E6%9E%9C%E6%95%88%E5%BA%94"><span class="nav-text">(不考)1.5.6 干预的因果效应</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.7-%E5%9B%A0%E6%9E%9C%E6%95%88%E5%BA%94%E5%B7%AE"><span class="nav-text">(不考)1.5.7 因果效应差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%831.5.8-%E5%8F%8D%E4%BA%8B%E5%AE%9E%E6%A8%A1%E5%9E%8B"><span class="nav-text">(不考)1.5.8 反事实模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="nav-text">二、搜索与求解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80"><span class="nav-text">2.1 搜索算法基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96%E6%8F%8F%E8%BF%B0"><span class="nav-text">2.1.1 搜索算法的形式化描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-text">2.1.2 评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%91%E6%90%9C%E7%B4%A2"><span class="nav-text">2.1.3 树搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%AA%E6%9E%9D%E6%90%9C%E7%B4%A2"><span class="nav-text">2.1.4 剪枝搜索</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E6%90%9C%E7%B4%A2"><span class="nav-text">2.2 启发式搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%AA%E5%A9%AA%E6%9C%80%E4%BD%B3%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2-greedy-best-first-search"><span class="nav-text">2.2.1
贪婪最佳优先搜索 Greedy best-first search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#a%E7%AE%97%E6%B3%95"><span class="nav-text">2.2.2 A*算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#a%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-text">2.2.2.1 A*算法的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%832.2.2.2-a%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90"><span class="nav-text">(不考)2.2.2.2 A*算法的性能分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%832.2.2.3-a%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%8C%E5%A4%87%E6%80%A7"><span class="nav-text">(不考)2.2.2.3 A*算法的完备性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%832.2.2.4-a%E7%AE%97%E6%B3%95%E7%9A%84%E6%9C%80%E4%BC%98%E6%80%A7"><span class="nav-text">(不考)2.2.2.4 A*算法的最优性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2%E5%8D%9A%E5%BC%88%E6%90%9C%E7%B4%A2"><span class="nav-text">2.3 对抗搜索&#x2F;博弈搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E6%9C%80%E5%A4%A7%E6%90%9C%E7%B4%A2-minimax"><span class="nav-text">2.3.1 最小最大搜索 Minimax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alpha-beta%E5%89%AA%E6%9E%9D%E6%90%9C%E7%B4%A2"><span class="nav-text">2.3.2 Alpha-Beta剪枝搜索</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2"><span class="nav-text">2.4 蒙特卡洛树搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="nav-text">2.4.1 问题定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="nav-text">2.4.2 相关概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E7%AD%96%E7%95%A5"><span class="nav-text">2.4.3 贪心算法策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%CE%B5-%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95"><span class="nav-text">2.4.4 ε-贪心算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8A%E9%99%90%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4%E7%AE%97%E6%B3%95ucd1upper-confidence-bounds"><span class="nav-text">2.4.5
上限置信区间算法UCD1：Upper Confidence Bounds</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2"><span class="nav-text">2.4.6 对抗搜索：蒙特卡洛树搜索</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">三、监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">3.1 机器学习的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-text">3.1.1 机器学习的分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%87%8D%E8%A6%81%E5%85%83%E7%B4%A0"><span class="nav-text">3.1.2 监督学习的重要元素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">3.1.3 监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">3.1.3.1 损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="nav-text">3.1.3.2 训练数据与测试数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%9F%E6%9C%9B%E9%A3%8E%E9%99%A9"><span class="nav-text">3.1.3.3 经验风险、期望风险</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E5%AD%A6%E4%B9%A0-%E6%AC%A0%E5%AD%A6%E4%B9%A0"><span class="nav-text">3.1.3.4 过学习 &amp; 欠学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F"><span class="nav-text">3.1.3.5 结构风险最小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.1.3.6 判别模型 &amp; 生成模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90"><span class="nav-text">3.2 回归分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">3.2.1 线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">3.2.1.1 一元线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">3.2.1.2 多元线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#logistics-%E5%9B%9E%E5%BD%92%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92"><span class="nav-text">3.2.1.3 logistics
回归&#x2F;对数几率回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-text">3.2.1.4 二分类问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="nav-text">3.2.1.5 基于似然函数的参数优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mle%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-map%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1"><span class="nav-text">3.2.1.6 MLE最大似然估计
&amp; MAP最大后验概率估计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-text">3.3 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5-ed"><span class="nav-text">3.3.1 信息熵 E(D)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A-gainda"><span class="nav-text">3.3.2 信息增益 Gain(D,A)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5%E5%92%8C%E7%89%A9%E7%90%86%E7%86%B5%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">3.3.3 信息熵和物理熵的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%8C%BA%E5%88%AB%E5%88%86%E6%9E%90-lda"><span class="nav-text">3.4 线性区别分析 LDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"><span class="nav-text">3.4.1 符号定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98-1"><span class="nav-text">3.4.2 二分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-text">3.4.3 多分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E7%9A%84%E9%99%8D%E7%BB%B4%E6%AD%A5%E9%AA%A4"><span class="nav-text">3.4.4 线性判别分析的降维步骤</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ada-boosting"><span class="nav-text">3.5 Ada Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E9%9C%8D%E5%A4%AB%E4%B8%81%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="nav-text">3.5.1 计算学习理论：霍夫丁不等式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E6%A6%82%E7%8E%87%E8%BF%91%E4%BC%BC%E6%AD%A3%E7%A1%AE-pac"><span class="nav-text">3.5.2 计算学习理论：概率近似正确
PAC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ada-boosting%E6%80%9D%E8%B7%AF%E6%8F%8F%E8%BF%B0"><span class="nav-text">3.5.3 Ada Boosting：思路描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ada-boosting%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="nav-text">3.5.4 Ada Boosting：算法描述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%A0%B7%E6%9C%AC%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">3.5.4.1 数据样本权重初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%ACm%E4%B8%AA%E5%BC%B1%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="nav-text">3.5.4.2 第m个弱分类器的训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%B1%E5%88%86%E7%B1%BB%E5%99%A8%E7%BB%84%E5%90%88%E6%88%90%E5%BC%BA%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-text">3.5.4.3 弱分类器组合成强分类器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ada-boosting%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A"><span class="nav-text">3.5.5 Ada Boosting：算法解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ada-boosting%E5%9B%9E%E7%9C%8B%E9%9C%8D%E5%A4%AB%E4%B8%81%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="nav-text">3.5.6 Ada
Boosting：回看霍夫丁不等式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ada-boosting%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="nav-text">3.5.7 Ada Boosting：优化目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">3.5.8 回归和分类的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%833.6-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-text">(不考)3.6 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%833.6.1-vc%E7%BB%B4%E4%B8%8E%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96"><span class="nav-text">(不考)3.6.1
VC维与结构风险最小化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%833.6.2-%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-text">(不考)3.6.2
线性可分支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%833.6.3-%E6%9D%BE%E5%BC%9B%E5%8F%98%E9%87%8F%E8%BD%AF%E9%97%B4%E9%9A%94%E4%B8%8Ehinge%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">(不考)3.6.3
松弛变量，软间隔与hinge损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%833.7-%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="nav-text">(不考)3.7 生成学习模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">四、无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="nav-text">4.1 K均值聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="nav-text">4.1.1 算法描述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E8%81%9A%E7%B1%BB%E8%B4%A8%E5%BF%83"><span class="nav-text">4.1.1.1 初始化聚类质心</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%86%E6%AF%8F%E4%B8%AA%E5%BE%85%E8%81%9A%E7%B1%BB%E6%95%B0%E6%8D%AE%E6%94%BE%E5%85%A5%E5%94%AF%E4%B8%80%E4%B8%80%E4%B8%AA%E8%81%9A%E7%B1%BB%E9%9B%86%E5%90%88%E4%B8%AD"><span class="nav-text">4.1.1.2
将每个待聚类数据放入唯一一个聚类集合中</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C%E6%9B%B4%E6%96%B0%E8%81%9A%E7%B1%BB%E8%B4%A8%E5%BF%83"><span class="nav-text">4.1.1.3
根据聚类结果，更新聚类质心</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%BE%AA%E7%8E%AF%E8%BF%AD%E4%BB%A3%E7%9B%B4%E5%88%B0%E6%BB%A1%E8%B6%B3%E6%9D%A1%E4%BB%B6"><span class="nav-text">4.1.1.4
算法循环迭代，直到满足条件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A6%E4%B8%80%E4%B8%AA%E8%A7%86%E8%A7%92%E6%9C%80%E5%B0%8F%E5%8C%96%E6%AF%8F%E4%B8%AA%E7%B1%BB%E7%B0%87%E7%9A%84%E6%96%B9%E5%B7%AE"><span class="nav-text">4.1.2
另一个视角：最小化每个类簇的方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="nav-text">4.1.3 K均值聚类算法的不足</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca"><span class="nav-text">4.2 主成分分析 PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5-1"><span class="nav-text">4.2.1 相关概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%8A%A8%E6%9C%BA"><span class="nav-text">4.2.2 算法动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0-1"><span class="nav-text">4.2.3 算法描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95"><span class="nav-text">4.2.4 其他常用降维方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E4%BA%BA%E8%84%B8%E6%96%B9%E6%B3%95"><span class="nav-text">4.3 特征人脸方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-text">4.3.1 动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0-2"><span class="nav-text">4.3.2 算法描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E4%BA%BA%E8%84%B8%E8%A1%A8%E8%BE%BE%E7%9A%84%E6%96%B9%E6%B3%95%E8%81%9A%E7%B1%BBpca%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="nav-text">4.3.3
其他人脸表达的方法：聚类、PCA、非负矩阵分解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90"><span class="nav-text">4.4 潜在语义分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E6%80%9D%E6%83%B3"><span class="nav-text">4.4.1 潜在语义分析思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%9E%90%E8%BF%87%E7%A8%8B"><span class="nav-text">4.4.2 分析过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96%E7%AE%97%E6%B3%95-em"><span class="nav-text">4.5 期望最大化算法 EM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="nav-text">4.5.1 模型参数估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-text">4.5.2 期望最大化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#em%E7%A4%BA%E4%BE%8B%E4%BA%8C%E7%A1%AC%E5%B8%81%E6%8A%95%E6%8E%B7%E4%BE%8B%E5%AD%90"><span class="nav-text">4.5.3 EM示例：二硬币投掷例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#em%E7%A4%BA%E4%BE%8B%E4%B8%89%E7%A1%AC%E5%B8%81%E6%8A%95%E6%8E%B7%E4%BE%8B%E5%AD%90"><span class="nav-text">4.5.4 EM示例：三硬币投掷例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#em%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%80%E8%88%AC%E5%BD%A2%E5%BC%8F"><span class="nav-text">4.5.5 EM算法的一般形式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-text">五、深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8E%86%E5%8F%B2%E5%8F%91%E5%B1%95"><span class="nav-text">5.1 深度学习的历史发展</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-fnn"><span class="nav-text">5.2 前馈神经网络 FNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%85%E5%B1%82%E5%AD%A6%E4%B9%A0-vs-%E6%B7%B1%E5%B1%82%E5%AD%A6%E4%B9%A0%E5%88%86%E6%AE%B5%E5%AD%A6%E4%B9%A0%E9%80%90%E5%B1%82%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0"><span class="nav-text">5.2.1 浅层学习 vs
深层学习：分段学习&#x3D;&gt;逐层端到端学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A5%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%96%B9%E5%BC%8F%E9%80%90%E5%B1%82%E6%8A%BD%E8%B1%A1%E9%80%90%E5%B1%82%E5%AD%A6%E4%B9%A0"><span class="nav-text">5.2.2
深度学习：以端到端的方式逐层抽象、逐层学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mcp%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="nav-text">5.2.3 MCP神经元</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%AF%B9%E8%BE%93%E5%85%A5%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="nav-text">5.2.3
激活函数：对输入信息进行非线性变换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0sigmoidtanhrelu"><span class="nav-text">5.2.3.1
常用激活函数：sigmoid、tanh、ReLU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax%E5%87%BD%E6%95%B0%E5%B0%86%E8%BE%93%E5%87%BA%E6%98%A0%E5%B0%84%E5%88%B001%E6%A6%82%E7%8E%87%E7%A9%BA%E9%97%B4"><span class="nav-text">5.2.3.2
softmax函数：将输出映射到[0,1]概率空间</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E5%8A%9F%E8%83%BD%E5%8A%A0%E6%9D%83%E7%B4%AF%E5%8A%A0-%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="nav-text">5.2.4
单个神经元的功能：加权累加 + 非线性变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-loss-function"><span class="nav-text">5.2.5 损失函数 Loss Function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E6%8D%9F%E5%A4%B1-mse%E9%A2%84%E6%B5%8B%E5%80%BC%E5%92%8C%E5%AE%9E%E9%99%85%E5%80%BC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE"><span class="nav-text">5.2.5.1 均方误差损失
MSE：预测值和实际值之间的差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%A4%E4%B8%AA%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB"><span class="nav-text">5.2.5.2
交叉熵损失函数：两个概率分布之间的距离</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B"><span class="nav-text">5.2.6 感知机模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-text">5.2.6.1 单层感知机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cfnn"><span class="nav-text">5.2.6.2
多层感知机&#x2F;前馈神经网络(FNN)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="nav-text">5.2.7 参数优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-gradient-descent"><span class="nav-text">5.2.7.1 梯度下降 Gradient
Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-bp"><span class="nav-text">5.2.7.2 误差反向传播 BP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99"><span class="nav-text">5.2.7.3 链式求导法则</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%83%BD%E5%8A%9B%E5%9C%A8%E4%BA%8E%E6%8B%9F%E5%90%88%E5%92%8C%E4%BC%98%E5%8C%96"><span class="nav-text">5.2.8
机器学习的能力在于拟合和优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-cnn"><span class="nav-text">5.3 卷积神经网络 CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E7%BA%BF%E6%80%A7%E6%93%8D%E4%BD%9C"><span class="nav-text">5.3.1 卷积操作：线性操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E6%9D%83%E9%87%8D%E6%98%AF%E9%80%9A%E8%BF%87%E5%AD%A6%E4%B9%A0%E5%BE%97%E5%87%BA%E7%9A%84"><span class="nav-text">5.3.1.1
卷积核的权重是通过学习得出的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="nav-text">5.3.1.2 对图像进行卷积操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%89%B9%E5%BE%81%E5%9B%BE%E6%84%9F%E5%8F%97%E9%87%8E"><span class="nav-text">5.3.1.3 卷积核、特征图、感受野</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E6%93%8D%E4%BD%9C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%93%8D%E4%BD%9C"><span class="nav-text">5.3.2 池化操作：非线性操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="nav-text">5.3.3
神经网络正则化：解决过拟合问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-rnn"><span class="nav-text">5.4 循环神经网络 RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn%E7%9A%84%E7%BB%93%E6%9E%84"><span class="nav-text">5.4.1 RNN的结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B2%BF%E6%97%B6%E9%97%B4%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-bptt"><span class="nav-text">5.4.2 沿时间反向传播算法 BPTT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E6%83%B3"><span class="nav-text">5.4.2.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-text">5.4.2.2 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-text">5.4.2.3 应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E6%A8%A1%E5%9E%8B-lstm%E8%A7%A3%E5%86%B3rnn%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-text">5.4.3 长短时记忆模型
LSTM：解决RNN梯度消失的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E6%83%B3-1"><span class="nav-text">5.4.3.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89-1"><span class="nav-text">5.4.3.2 符号定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm%E7%BB%93%E6%9E%84"><span class="nav-text">5.4.3.3 LSTM结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm%E5%A6%82%E4%BD%95%E5%85%8B%E6%9C%8D%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-text">5.4.3.4 LSTM如何克服梯度消失</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gru%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">5.4.4 GRU门控循环单元神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%835.5-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-text">(不考)5.5 深度生成学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8"><span class="nav-text">5.6 深度学习应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%B8%AD%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%9F%E6%88%90"><span class="nav-text">5.6.1 自然语言中词向量生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word2vec%E6%A8%A1%E5%9E%8B"><span class="nav-text">5.6.2 Word2Vec模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cbow%E6%A8%A1%E5%9E%8B%E9%80%9A%E8%BF%87%E4%B8%8A%E4%B8%8B%E6%96%87%E9%A2%84%E6%B5%8B%E5%8D%95%E8%AF%8D"><span class="nav-text">5.6.3
CBOW模型：通过上下文预测单词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E7%9B%AE%E6%A0%87%E5%AE%9A%E4%BD%8D"><span class="nav-text">5.6.4 图像分类和目标定位</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AD%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">六、强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="nav-text">6.1 强化学习问题定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-text">6.1.1 强化学习中的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-text">6.1.2 强化学习的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="nav-text">6.1.3 马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B-discrete-markov-process"><span class="nav-text">6.1.3.1
离散马尔可夫过程 Discrete Markov Process</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B-markov-reward-process%E5%BC%95%E5%85%A5%E5%A5%96%E5%8A%B1"><span class="nav-text">6.1.3.2
马尔可夫奖励过程 Markov Reward Process：引入奖励</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A5%96%E5%8A%B1%E6%9C%BA%E5%88%B6"><span class="nav-text">6.1.3.2.1 奖励机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%8D%E9%A6%88%E6%8A%98%E6%89%A3%E7%B3%BB%E6%95%B0"><span class="nav-text">6.1.3.2.2 反馈、折扣系数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B-markov-decision-process%E5%BC%95%E5%85%A5%E5%8A%A8%E4%BD%9C"><span class="nav-text">6.1.3.3
马尔可夫决策过程 Markov Decision Process：引入动作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A6%BB%E6%95%A3%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E6%8F%8F%E8%BF%B0%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%A7%BB%E5%8A%A8%E9%97%AE%E9%A2%98"><span class="nav-text">6.1.3.4
使用离散马尔可夫过程描述机器人移动问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0"><span class="nav-text">6.1.4 强化学习中的策略学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89-1"><span class="nav-text">6.1.5 强化学习问题定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E7%A8%8B"><span class="nav-text">6.1.6 贝尔曼方程 &lt;&#x3D;&gt;
动态规划方程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">6.2 基于价值的强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-text">6.2.1 策略迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96"><span class="nav-text">6.2.2 强化学习中的策略优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="nav-text">6.2.3 强化学习中的策略评估方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-text">6.2.3.1 动态规划</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7"><span class="nav-text">6.2.3.2 蒙特卡洛采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86-temporal-difference"><span class="nav-text">6.2.3.3 时序差分 Temporal
Difference</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#q-learning%E7%9B%B4%E6%8E%A5%E8%AE%A1%E7%AE%97q_pi"><span class="nav-text">6.2.3.4 Q-learning：直接计算\(q_\pi\)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%CE%B5-greedy%E7%AD%96%E7%95%A5%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8%E7%9A%84%E5%B9%B3%E8%A1%A1"><span class="nav-text">6.2.3.5
ε-greedy策略：策略学习中探索与利用的平衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#deep-q-learning%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88q_pi"><span class="nav-text">6.2.3.6 Deep
Q-learning：用神经网络拟合\(q_\pi\)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.3-%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">(不考)6.3 基于策略的强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.3.1-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86"><span class="nav-text">(不考)6.3.1 策略梯度定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.3.2-%E5%9F%BA%E4%BA%8E%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E7%9A%84%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-text">(不考)6.3.2
基于蒙特卡洛采样的策略梯度法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.3.3-%E5%9F%BA%E4%BA%8E%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E7%9A%84%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%B3%95actor-critic"><span class="nav-text">(不考)6.3.3
基于时序差分的策略梯度法：Actor-Critic</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.4-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-text">(不考)6.4 深度强化学习的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.4.1-deep-q-learning%E5%9B%B4%E6%A3%8B%E5%8D%9A%E5%BC%88"><span class="nav-text">(不考)6.4.1 Deep
Q-Learning：围棋博弈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.4.2-deep-q-learning%E9%9B%85%E8%BE%BE%E5%88%A9%E6%B8%B8%E6%88%8F"><span class="nav-text">(不考)6.4.2 Deep
Q-Learning：雅达利游戏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%836.4.3-%E9%9A%BE%E4%BB%A5%E6%8E%A2%E7%B4%A2%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-text">(不考)6.4.3 难以探索的例子</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-text">6.5 强化学习的分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%83%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8D%9A%E5%BC%88"><span class="nav-text">七、人工智能博弈</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E8%AE%BA%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="nav-text">7.1 博弈论的相关概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E7%9A%84%E8%A6%81%E7%B4%A0"><span class="nav-text">7.1.1 博弈的要素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%8C%83%E5%BC%8F"><span class="nav-text">7.1.2 研究范式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9A%E5%BE%92%E5%9B%B0%E5%A2%83-prisoners-dilemma"><span class="nav-text">7.1.3 囚徒困境 Prisoner&#39;s
Dilemma</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-text">7.1.4 博弈的分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">7.1.5 纳什均衡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E6%A2%85%E6%B4%9B%E5%AE%9A%E7%90%86"><span class="nav-text">7.1.6 策梅洛定理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2-%E5%8D%9A%E5%BC%88%E7%AD%96%E7%95%A5%E6%B1%82%E8%A7%A3"><span class="nav-text">(不考)7.2 博弈策略求解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2.1-%E8%99%9A%E6%8B%9F%E9%81%97%E6%86%BE%E6%9C%80%E5%B0%8F%E5%8C%96%E7%AE%97%E6%B3%95-regret-minimization"><span class="nav-text">(不考)7.2.1
虚拟遗憾最小化算法 Regret Minimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2.1.1-%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-text">(不考)7.2.1.1
纳什均衡策略的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2.1.2-%E9%81%97%E6%86%BE%E6%9C%80%E5%B0%8F%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-text">(不考)7.2.1.2 遗憾最小化算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2.1.3-%E6%9C%89%E6%95%88%E9%81%97%E6%86%BE%E5%80%BC"><span class="nav-text">(不考)7.2.1.3 有效遗憾值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2.1.4-%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA"><span class="nav-text">(不考)7.2.1.4 计算理论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2.1.5-%E8%99%9A%E6%8B%9F%E6%9C%80%E5%B0%8F%E5%8C%96%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="nav-text">(不考)7.2.1.5
虚拟最小化算法步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E8%80%837.2.1.6-%E7%A4%BA%E4%BE%8B"><span class="nav-text">(不考)7.2.1.6 示例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E8%A7%84%E5%88%99%E8%AE%BE%E8%AE%A1"><span class="nav-text">7.3 博弈规则设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-text">7.3.1 研究的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8C%E8%BE%B9%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95"><span class="nav-text">7.3.2 双边匹配算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A8%B3%E5%AE%9A%E5%A9%9A%E5%A7%BB%E9%97%AE%E9%A2%98-stable-marriage-problem"><span class="nav-text">7.3.2.1 稳定婚姻问题
Stable Marriage Problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8C%E8%BE%B9%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98g-s%E7%AE%97%E6%B3%95"><span class="nav-text">7.3.2.2 双边匹配问题：G-S算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E8%BE%B9%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98%E6%9C%80%E5%A4%A7%E4%BA%A4%E6%98%93%E5%9C%88%E7%AE%97%E6%B3%95-top-trading-cycle-ttc"><span class="nav-text">7.3.3
单边匹配问题：最大交易圈算法 Top-trading cycle TTC</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E5%AE%8C%E5%85%A8%E4%BF%A1%E6%81%AF%E5%8D%9A%E5%BC%88%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="nav-text">7.4 非完全信息博弈的实际应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E8%AE%BA%E7%A0%94%E7%A9%B6%E5%86%85%E5%AE%B9"><span class="nav-text">7.4.1 博弈论研究内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E9%9D%9E%E5%AE%8C%E5%85%A8%E4%BF%A1%E6%81%AF%E5%8D%9A%E5%BC%88%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1%E7%9A%84%E4%B8%80%E8%88%AC%E6%96%B9%E6%B3%95"><span class="nav-text">7.4.2
求解非完全信息博弈纳什均衡的一般方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9B%B8%E7%BB%93%E5%90%88"><span class="nav-text">7.4.3 与深度强化学习相结合</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="华丰夏"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">华丰夏</p>
  <div class="site-description" itemprop="description">一切都是上天最好的安排</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/unicorn2022" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;unicorn2022" rel="noopener" target="_blank"><i class="fa fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-07 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">华丰夏</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">328k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">4:58</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共204.2k字</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
